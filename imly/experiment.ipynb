{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments using IMLY ###\n",
    "\n",
    "This notebook contains experimental runs of IMLY with different datasets.  \n",
    "The readings of these experiments can be referred to in this [sheet](https://docs.google.com/spreadsheets/d/1E5jcq2w42gN8bMIaeaRJpAdhgSVN-2XDJ_YTHe4qfwY/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #1\n",
    "\n",
    "#### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "  round_epochs             val_loss                 loss  lr units batch_size  \\\n",
      "0          100  0.12337211660056743  0.14693315284765832  30     1         10   \n",
      "\n",
      "  epochs weight_regulizer emb_output_dims optimizer losses activation  \\\n",
      "0    100             None            None     nadam    mse     linear   \n",
      "\n",
      "         model_name  \n",
      "0  LinearRegression  \n",
      "266/266 [==============================] - ETA:  - 0s 109us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# X = preprocessing.scale(X)\n",
    "# Y = preprocessing.normalize(Y)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #2\n",
    "\n",
    "#### UCI Abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "794/794 [==============================] - ETA:  - 0s 77us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_abalone_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"uci_abalone\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\" if path.exists(\"../data/abalone.data.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #3\n",
    "\n",
    "#### UCI Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "60/60 [==============================] - ETA:  - 0s 6ms/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_iris_logistic_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "params = {\n",
    "    'epochs': 170\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 22]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "a, b = np.unique(y_train, return_counts=True)\n",
    "# len(y_test)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.509 - 0s 12ms/step - loss: 5.1482\n",
      "Epoch 2/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.240 - 0s 0us/step - loss: 5.1335\n",
      "Epoch 3/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.517 - 0s 391us/step - loss: 5.1188\n",
      "Epoch 4/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.011 - 0s 390us/step - loss: 5.1038\n",
      "Epoch 5/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.508 - 0s 391us/step - loss: 5.0873\n",
      "Epoch 6/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.215 - 0s 0us/step - loss: 5.0720\n",
      "Epoch 7/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.087 - 0s 390us/step - loss: 5.0557\n",
      "Epoch 8/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.836 - 0s 390us/step - loss: 5.0393\n",
      "Epoch 9/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.396 - 0s 391us/step - loss: 5.0238\n",
      "Epoch 10/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.395 - 0s 391us/step - loss: 5.0078\n",
      "Epoch 11/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.055 - 0s 391us/step - loss: 4.9915\n",
      "Epoch 12/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.953 - 0s 0us/step - loss: 4.9744\n",
      "Epoch 13/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.635 - 0s 391us/step - loss: 4.9605\n",
      "Epoch 14/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.623 - 0s 390us/step - loss: 4.9432\n",
      "Epoch 15/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.790 - 0s 391us/step - loss: 4.9275\n",
      "Epoch 16/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.494 - 0s 391us/step - loss: 4.9111\n",
      "Epoch 17/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.714 - 0s 390us/step - loss: 4.8952\n",
      "Epoch 18/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.580 - 0s 0us/step - loss: 4.8786\n",
      "Epoch 19/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.689 - 0s 391us/step - loss: 4.8623\n",
      "Epoch 20/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.878 - 0s 391us/step - loss: 4.8461\n",
      "Epoch 21/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.709 - 0s 0us/step - loss: 4.8294\n",
      "Epoch 22/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.673 - 0s 391us/step - loss: 4.8130\n",
      "Epoch 23/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.621 - 0s 390us/step - loss: 4.7966\n",
      "Epoch 24/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.869 - 0s 391us/step - loss: 4.7806\n",
      "Epoch 25/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.858 - 0s 0us/step - loss: 4.7642\n",
      "Epoch 26/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.628 - 0s 391us/step - loss: 4.7477\n",
      "Epoch 27/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.529 - 0s 391us/step - loss: 4.7315\n",
      "Epoch 28/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.308 - 0s 390us/step - loss: 4.7150\n",
      "Epoch 29/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.863 - 0s 0us/step - loss: 4.6993\n",
      "Epoch 30/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.463 - 0s 391us/step - loss: 4.6824\n",
      "Epoch 31/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.488 - 0s 391us/step - loss: 4.6661\n",
      "Epoch 32/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.270 - 0s 390us/step - loss: 4.6494\n",
      "Epoch 33/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.786 - 0s 391us/step - loss: 4.6336\n",
      "Epoch 34/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.702 - 0s 390us/step - loss: 4.6170\n",
      "Epoch 35/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.659 - 0s 781us/step - loss: 4.6007\n",
      "Epoch 36/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.110 - 0s 390us/step - loss: 4.5838\n",
      "Epoch 37/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.053 - 0s 0us/step - loss: 4.5675\n",
      "Epoch 38/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.404 - 0s 0us/step - loss: 4.5514\n",
      "Epoch 39/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.908 - 0s 391us/step - loss: 4.5354\n",
      "Epoch 40/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.359 - 0s 391us/step - loss: 4.5182\n",
      "Epoch 41/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.530 - 0s 391us/step - loss: 4.5022\n",
      "Epoch 42/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.781 - 0s 391us/step - loss: 4.4864\n",
      "Epoch 43/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.048 - 0s 390us/step - loss: 4.4707\n",
      "Epoch 44/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.526 - 0s 390us/step - loss: 4.4544\n",
      "Epoch 45/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.533 - 0s 0us/step - loss: 4.4391\n",
      "Epoch 46/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.277 - 0s 390us/step - loss: 4.4234\n",
      "Epoch 47/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.263 - 0s 391us/step - loss: 4.4079\n",
      "Epoch 48/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.745 - 0s 0us/step - loss: 4.3915\n",
      "Epoch 49/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.943 - 0s 390us/step - loss: 4.3771\n",
      "Epoch 50/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.038 - 0s 0us/step - loss: 4.3597\n",
      "Epoch 51/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.719 - 0s 391us/step - loss: 4.3433\n",
      "Epoch 52/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.439 - 0s 781us/step - loss: 4.3280\n",
      "Epoch 53/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.210 - 0s 391us/step - loss: 4.3111\n",
      "Epoch 54/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.314 - 0s 391us/step - loss: 4.2949\n",
      "Epoch 55/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.834 - 0s 391us/step - loss: 4.2779\n",
      "Epoch 56/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.171 - 0s 523us/step - loss: 4.2619\n",
      "Epoch 57/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.022 - 0s 0us/step - loss: 4.2452\n",
      "Epoch 58/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.090 - 0s 391us/step - loss: 4.2303\n",
      "Epoch 59/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.372 - 0s 390us/step - loss: 4.2130\n",
      "Epoch 60/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.004 - 0s 391us/step - loss: 4.1970\n",
      "Epoch 61/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.203 - 0s 392us/step - loss: 4.1817\n",
      "Epoch 62/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.513 - 0s 390us/step - loss: 4.1664\n",
      "Epoch 63/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.094 - 0s 390us/step - loss: 4.1501\n",
      "Epoch 64/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.275 - 0s 391us/step - loss: 4.1349\n",
      "Epoch 65/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.851 - 0s 0us/step - loss: 4.1188\n",
      "Epoch 66/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.861 - 0s 781us/step - loss: 4.1047\n",
      "Epoch 67/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.231 - 0s 391us/step - loss: 4.0882\n",
      "Epoch 68/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.723 - 0s 391us/step - loss: 4.0724\n",
      "Epoch 69/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.415 - 0s 0us/step - loss: 4.0566\n",
      "Epoch 70/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.104 - 0s 0us/step - loss: 4.0417\n",
      "Epoch 71/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.804 - 0s 391us/step - loss: 4.0250\n",
      "Epoch 72/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.847 - 0s 391us/step - loss: 4.0088\n",
      "Epoch 73/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.039 - 0s 390us/step - loss: 3.9928\n",
      "Epoch 74/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.277 - 0s 390us/step - loss: 3.9769\n",
      "Epoch 75/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.317 - 0s 0us/step - loss: 3.9608\n",
      "Epoch 76/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.784 - 0s 390us/step - loss: 3.9443\n",
      "Epoch 77/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.753 - 0s 390us/step - loss: 3.9287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.743 - 0s 391us/step - loss: 3.9128\n",
      "Epoch 79/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.752 - 0s 390us/step - loss: 3.8968\n",
      "Epoch 80/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.988 - 0s 391us/step - loss: 3.8810\n",
      "Epoch 81/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.013 - 0s 391us/step - loss: 3.8649\n",
      "Epoch 82/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.802 - 0s 390us/step - loss: 3.8486\n",
      "Epoch 83/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.492 - 0s 390us/step - loss: 3.8322\n",
      "Epoch 84/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.399 - 0s 391us/step - loss: 3.8160\n",
      "Epoch 85/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.101 - 0s 391us/step - loss: 3.8007\n",
      "Epoch 86/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.168 - 0s 391us/step - loss: 3.7843\n",
      "Epoch 87/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.587 - 0s 391us/step - loss: 3.7674\n",
      "Epoch 88/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.915 - 0s 0us/step - loss: 3.7522\n",
      "Epoch 89/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.781 - 0s 0us/step - loss: 3.7361\n",
      "Epoch 90/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.762 - 0s 0us/step - loss: 3.7204\n",
      "Epoch 91/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.485 - 0s 0us/step - loss: 3.7043\n",
      "Epoch 92/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.315 - 0s 391us/step - loss: 3.6881\n",
      "Epoch 93/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.937 - 0s 390us/step - loss: 3.6731\n",
      "Epoch 94/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.400 - 0s 0us/step - loss: 3.6560\n",
      "Epoch 95/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.745 - 0s 0us/step - loss: 3.6405\n",
      "Epoch 96/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.762 - 0s 390us/step - loss: 3.6244\n",
      "Epoch 97/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.713 - 0s 390us/step - loss: 3.6084\n",
      "Epoch 98/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.460 - 0s 391us/step - loss: 3.5922\n",
      "Epoch 99/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.474 - 0s 391us/step - loss: 3.5764\n",
      "Epoch 100/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.676 - 0s 391us/step - loss: 3.5608\n",
      "Epoch 101/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.412 - 0s 391us/step - loss: 3.5444\n",
      "Epoch 102/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.191 - 0s 391us/step - loss: 3.5281\n",
      "Epoch 103/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.802 - 0s 391us/step - loss: 3.5130\n",
      "Epoch 104/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.355 - 0s 0us/step - loss: 3.4960\n",
      "Epoch 105/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.545 - 0s 0us/step - loss: 3.4804\n",
      "Epoch 106/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.301 - 0s 0us/step - loss: 3.4639\n",
      "Epoch 107/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.530 - 0s 390us/step - loss: 3.4483\n",
      "Epoch 108/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.769 - 0s 391us/step - loss: 3.4327\n",
      "Epoch 109/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.130 - 0s 391us/step - loss: 3.4158\n",
      "Epoch 110/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.464 - 0s 391us/step - loss: 3.4007\n",
      "Epoch 111/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.222 - 0s 0us/step - loss: 3.3844\n",
      "Epoch 112/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.709 - 0s 0us/step - loss: 3.3694\n",
      "Epoch 113/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.640 - 0s 391us/step - loss: 3.3534\n",
      "Epoch 114/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.449 - 0s 391us/step - loss: 3.3377\n",
      "Epoch 115/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.202 - 0s 390us/step - loss: 3.3220\n",
      "Epoch 116/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.037 - 0s 0us/step - loss: 3.3064\n",
      "Epoch 117/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.792 - 0s 0us/step - loss: 3.2902\n",
      "Epoch 118/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.338 - 0s 0us/step - loss: 3.2751\n",
      "Epoch 119/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.517 - 0s 391us/step - loss: 3.2591\n",
      "Epoch 120/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.152 - 0s 391us/step - loss: 3.2422\n",
      "Epoch 121/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.443 - 0s 0us/step - loss: 3.2270\n",
      "Epoch 122/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.158 - 0s 0us/step - loss: 3.2106\n",
      "Epoch 123/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.128 - 0s 391us/step - loss: 3.1949\n",
      "Epoch 124/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.026 - 0s 391us/step - loss: 3.1790\n",
      "Epoch 125/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.434 - 0s 0us/step - loss: 3.1639\n",
      "Epoch 126/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.406 - 0s 0us/step - loss: 3.1480\n",
      "Epoch 127/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.360 - 0s 391us/step - loss: 3.1325\n",
      "Epoch 128/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.351 - 0s 0us/step - loss: 3.1174\n",
      "Epoch 129/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.308 - 0s 0us/step - loss: 3.1024\n",
      "Epoch 130/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.160 - 0s 391us/step - loss: 3.0874\n",
      "Epoch 131/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.276 - 0s 391us/step - loss: 3.0729\n",
      "Epoch 132/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.797 - 0s 0us/step - loss: 3.0573\n",
      "Epoch 133/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.941 - 0s 0us/step - loss: 3.0426\n",
      "Epoch 134/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.097 - 0s 391us/step - loss: 3.0276\n",
      "Epoch 135/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.317 - 0s 391us/step - loss: 3.0126\n",
      "Epoch 136/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.871 - 0s 0us/step - loss: 2.9965\n",
      "Epoch 137/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.615 - 0s 0us/step - loss: 2.9808\n",
      "Epoch 138/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.624 - 0s 391us/step - loss: 2.9652\n",
      "Epoch 139/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.206 - 0s 391us/step - loss: 2.9504\n",
      "Epoch 140/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.321 - 0s 0us/step - loss: 2.9345\n",
      "Epoch 141/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.950 - 0s 0us/step - loss: 2.9182\n",
      "Epoch 142/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.817 - 0s 0us/step - loss: 2.9027\n",
      "Epoch 143/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.303 - 0s 0us/step - loss: 2.8864\n",
      "Epoch 144/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.756 - 0s 0us/step - loss: 2.8715\n",
      "Epoch 145/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.132 - 0s 391us/step - loss: 2.8561\n",
      "Epoch 146/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.777 - 0s 390us/step - loss: 2.8392\n",
      "Epoch 147/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.689 - 0s 391us/step - loss: 2.8233\n",
      "Epoch 148/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.030 - 0s 0us/step - loss: 2.8082\n",
      "Epoch 149/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.840 - 0s 391us/step - loss: 2.7921\n",
      "Epoch 150/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.820 - 0s 391us/step - loss: 2.7766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imly import dope\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1,\n",
    "                input_dim=4,\n",
    "                activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy')\n",
    "\n",
    "# m = dope(LogisticRegression())\n",
    "# m.fit(x_train.values, y_train.values)\n",
    "# m.predict(x_test)\n",
    "model.fit(x_train.values, y_train.values, epochs=150)\n",
    "test = model.predict(x_test)\n",
    "test.argmax(axis=-1)\n",
    "# value, count = np.unique(test, return_counts=True)\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../data/uci_iris_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://mlsquare-pdf.s3.ap-south-1.amazonaws.com:443/uci_iris_logistic_regression.pdf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto\n",
    "import sys\n",
    "from boto.s3.key import Key\n",
    "# from boto.s3.key import Key\n",
    "fig_path = '../data/uci_iris_logistic_regression.pdf'\n",
    "fig_name = 'uci_iris_logistic_regression.pdf'\n",
    "bucket_name = 'mlsquare-datasets'\n",
    "credentials_json = json.load(open('../data/aws_credentials.json'))\n",
    "AWS_ACCESS_KEY_ID = credentials_json['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = credentials_json['AWS_SECRET_ACCESS_KEY']\n",
    "REGION_HOST = 's3.ap-south-1.amazonaws.com'\n",
    "\n",
    "# bucket_name = AWS_ACCESS_KEY_ID.lower() + '-dump'\n",
    "conn = boto.connect_s3(AWS_ACCESS_KEY_ID,\n",
    "                       AWS_SECRET_ACCESS_KEY, host=REGION_HOST)\n",
    "bucket = conn.get_bucket('mlsquare-pdf', validate=False)\n",
    "\n",
    "# bucket = conn.create_bucket(bucket_name,\n",
    "#     location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "print('Uploading %s to Amazon S3 bucket %s' % (fig_path, bucket_name))\n",
    "\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "k = Key(bucket)\n",
    "k.key = fig_name\n",
    "k.set_contents_from_filename(fig_path,\n",
    "                             cb=percent_cb, num_cb=10)  # upload file\n",
    "url = k.generate_url(expires_in=0, query_auth=False)\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #4\n",
    "\n",
    "#### UCI Adult salary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:29<00:00, 29.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "27134/27134 [==============================] - ETA: 26 - ETA: 1 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 25us/step\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_adult_salary_logistic_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_adult_salary\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\" \", header=None, names=names)\n",
    "\n",
    "\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)\n",
    "\n",
    "# Split the dataset into test and train datasets\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #5\n",
    "\n",
    "#### UCI Ad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "1416/1416 [==============================] - ETA:  - ETA:  - 0s 67us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_ad_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_name = \"uci_ad\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/ad.data.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "data = data.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Label encoding #\n",
    "\n",
    "lb = LabelEncoder()\n",
    "Y = lb.fit_transform(data.iloc[:, -1])\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #6\n",
    "\n",
    "#### UCI Mushroom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info #\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_mushroom\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields with missing values\n",
      "stalk-root\n",
      "2480\n",
      "30.53%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = ['classes', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n",
    "        'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "        'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "        'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color',\n",
    "        'population', 'habitat']\n",
    "url = \"../data/mushroom.data.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "print(\"Fields with missing values\")\n",
    "col_names = data.columns\n",
    "num_data = data.shape[0]\n",
    "for c in col_names:\n",
    "    num_non = data[c].isin([\"?\"]).sum()\n",
    "    if num_non > 0:\n",
    "        print (c)\n",
    "        print (num_non)\n",
    "        print (\"{0:.2f}%\".format(float(num_non) / num_data * 100))\n",
    "        print (\"\\n\")\n",
    "\n",
    "data = data[data[\"stalk-root\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "\n",
    "for col in names:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "# Split the dataset into test and train datasets #\n",
    "feature_list = names[1:23]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['classes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "3387/3387 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 63us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_mushroom_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #7\n",
    "\n",
    "#### Covertype dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"covertype\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/covtype.data.csv\", delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "data = data[data[54].isin([1,2])]\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [10:42<00:00, 642.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "297085/297085 [==============================] - ETA: 3: - ETA: 13s - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 8s 28us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/covertype_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #8\n",
    "\n",
    "#### TestData1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"test_data_1\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData1.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "600/600 [==============================] - ETA:  - 0s 105us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/test_data_1_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\":10\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #9\n",
    "\n",
    "#### TestData2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"test_data_2\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData2.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique,count = np.unique(Y,return_counts=True)\n",
    "class1=count[0]/X.shape[0]*100\n",
    "class2=count[1]/X.shape[0]*100\n",
    "class_distribution = round(class1, 2)\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "480/480 [==============================] - ETA:  - 0s 98us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/test_data_2_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #10\n",
    "\n",
    "#### UCI Airfoil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"uci_airfoil\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_airfoil_self_noise.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "902/902 [==============================] - ETA:  - 0s 44us/step\n",
      "Uploading ../data/uci_airfoil_linear_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "....."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #11\n",
    "\n",
    "#### UCI Auto-mpg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"uci_auto_mpg\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_auto_mpg.csv\", delimiter=\",\", header=0, index_col='car name')\n",
    "data = data[data.horsepower != '?']\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:,1]\n",
    "X = data.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "236/236 [==============================] - ETA:  - 0s 101us/step\n",
      "Uploading ../data/uci_auto_mpg_linear_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2f5bc21b2f8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((int(1e12),int(1e12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #12\n",
    "\n",
    "#### Testdata 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"test_data_3\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData3.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "600/600 [==============================] - ETA:  - 0s 100us/step\n",
      "Uploading ../data/test_data_3_linear_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "...."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #13\n",
    "\n",
    "#### Testdata 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"test_data_4\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData4.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:27<00:00, 27.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "600/600 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 843us/step\n",
      "Uploading ../data/test_data_4_linear_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "...."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJztnXucXGV9/z/fmUyS2YDZRNIKKyERMWkxZGNWiE2rBiloubhyMUWwai9RWy+JuDYUComFH7GphrZaW6qttiAu4bIGAj9QA/IryCVhd4mR4E/lEgb8GSTLJTtJZne/vz/mnMmZM8/znOecOWfOzM73/XrllZ2ZM+c8c3b2+T7P9/L5EjNDEARBEDJpD0AQBEFoDsQgCIIgCADEIAiCIAgOYhAEQRAEAGIQBEEQBAcxCIIgCAIAMQiCUAMRPU1Ep0V87x8Q0ZNxj8niuuuI6PpGX1eYXIhBEJoOIvoQEW0noteI6AUiuouIfj/tcakgIiaiN7uPmfn/MPOCNMcURD0GT5jciEEQmgoi+hyAawH8LwC/DWAugH8B8P4I55pi85wgCGXEIAhNAxHNBPBFAH/FzLcy835mLjHz7czc5xwzjYiuJaLnnX/XEtE057V3E9FzRPTXRPQrAP+pes459iwiGiKiESJ6kIhO0ozpZCL6sXPcC0T0VSKa6rx2v3PYsLObWelez/P+3yGi+5z37yKiczyvfYuIvkZEW4noVSJ6mIiO14xjnrMbWeV87heI6BLDvTzHud6Ic/3fcZ7/b5SN7O3OmL9g+esR2gAxCEIz8Q4A0wHcZjjmMgDLAHQDWAzgZACXe15/A4DZAI4DsEr1HBG9DcB/APg4gNcD+DcAW1zD4mMcwBoARznjew+AvwQAZn6nc8xiZj6Cmfu9bySiHIDbAdwD4LcAfBrADUTkdSldCGA9gFkAfg7gasNnB4AVAE4AcDqAtSrXDxG9BcCNAFYDmAPgTpQNwFRm/jCAZwGc7Yz57wOuJ7QRYhCEZuL1AF5k5jHDMRcB+CIz/5qZ96I8mX7Y8/oEgCuZ+SAzFzXP/QWAf2Pmh5l5nJm/DeAgyoamCmbewcwPMfMYMz+NsvF4l+XnWQbgCAAbmPkQM28DcAfKRsDlVmZ+xPnMN6Bs6Eysd3ZOO1He7VyoOGYlgK3M/H1mLgH4BwB5AL9nOW6hTRGDIDQTvwFwVICf/xgAz3geP+M857KXmQ/43uN/7jgAlzjulBEiGgFwrO88AMqrbSK6g4h+RUSvoBzbOMry8xwDYA8zT/jG2+V5/CvPz6MoGxATe3znqhkzfPfIuf4e33UFoQYxCEIz8WMABwD0Go55HuUJ3WWu85yLSr7X/9weAFczc6fnXwcz36h479cB7AZwAjO/DsDfAKCAz+Ed67FE5P07mwugYPl+Fcf6zvW84piqe0RE5LzPva5IHAtKxCAITQMzvwzgCgBfI6JeIuogohwRvY+IXF/3jQAuJ6I5RHSUc3zY/Pt/B/AJIjqFyswgojOJ6EjFsUcCeAXAa0S0EMAnfa//PwBv0lznYQD7AXzB+RzvBnA2gO+GHK+Xv3Xuy4kAPgagX3HMTQDOJKL3OHGMS1B2iT1oMWahjRGDIDQVzPwVAJ9DOVC8F+XV/KcADDiHXAVgO4DHAewE8JjzXJhrbEc5jvBVAPtQDuZ+VHP45wF8CMCrKBsS/wS8DsC3HdfTB33XOQTgHADvA/Aiyumzf8LMu8OM18ePnPH+EMA/MPM9/gOY+UkAFwP4Z+e6Z6McRD7kHHINykZ1hIg+X8dYhEkGSYMcQWh+iGgegKcA5AKC7oIQGdkhCIIgCADEIAiCIAgO4jISBEEQAMgOQRAEQXBoKaGvo446iufNm5f2MARBEFqKHTt2vMjMc4KOaymDMG/ePGzfvj3tYQiCILQURPRM8FHiMhIEQRAcxCAIgiAIAMQgCIIgCA5iEARBEAQAYhAEQRAEBzEIgiAIAoAWSzsVBEFoJQYGC9h495N4fqSIYzrz6DtjAXqXNG+fIjEIgiAICTAwWMClt+5EsTQOACiMFHHprTsBoGmNgriMBEEQEmDj3U9WjIFLsTSOjXc/mdKIghGDIAiCkADPjxRDPd8MpG4QiChLRINEdEfaYxEEQYiLYzrzoZ5vBlI3CAA+C+CJtAchCEI0BgYLWL5hG+av3YrlG7ZhYLCQ9pCagr4zFiCfy1Y9l89l0XfGgpRGFEyqBoGI3gjgTADfSHMcgiBEww2cFkaKYBwOnIpRKAeOrzl3Ebo68yAAXZ15XHPuoqYNKAPpZxldC+ALAI5MeRyCIETAFDht5omvUfQu6Wqp+5DaDoGIzgLwa2beEXDcKiLaTkTb9+7d26DRCYJgQysGTgU9abqMlgM4h4ieBvBdAKcS0fX+g5j5OmbuYeaeOXMC+zsIgtBAWjFwKuhJzSAw86XM/EZmngfgjwFsY+aL0xqPIAjhacXAqaAn7RiCIAgtjOsfbyV5BkEPMXPaY7Cmp6eHpYWmIAhCOIhoBzP3BB3XDHUIgiAIQhMgBkEQBEEAIDEEQRDajFaTpG4kYhAEQdAy2SZPkyQ1IMFxMQiCIChpRT1/wGzEdJXV62/fhQOliZb7rHEjBkEQBCWtKEsRZMR0FdT7Rks1z7mGop12DRJUFgRBSSvKUgQ1pQlbQb1vtNRWwn1iEARBUNKKshRBRkxXWd2Zz1mdv9k7ntWLGARBEJS0oixFkBHTSVKvO+fEms+qo5l3SPUiMQRBEJS0oixF3xkLqmIIQK0RM0lSez/r/oNjGCnWxhaaeYdULyJdIQjCpCKuVFl/gBooG5dmb3Kjwla6QnYIgiBMKuJqStOKO6R6EYMgCIKgodU6ntWLBJUFQRAEAGIQBEEQBAcxCIIgCAIAMQiCIAiCQ2pBZSKaDuB+ANOccdzMzFemNR5BEJqTyaa42sykmWV0EMCpzPwaEeUA/A8R3cXMD6U4JkEQmohWVVxtVVJzGXGZ15yHOedf61TJCYKQOEFidUK8pFqHQERZADsAvBnA15j54TTHIwhCOJJ257Si4mork2pQmZnHmbkbwBsBnExEb/UfQ0SriGg7EW3fu3dv4wcpCIIS152TpDx0KyqutjJNkWXEzCMA7gPwXsVr1zFzDzP3zJkzp+FjEwRBTSPcOa2ouNrKpJllNAdAiZlHiCgP4DQAX0prPIIwmUnCtdMId0476gmlSZoxhKMBfNuJI2QA3MTMd6Q4HkGYlCSVqXNMZx4FxeQftzun3fSE0iQ1g8DMjwNYktb1BaFdSKo3sk3vAS/17FKkFqExiNqpIExyknLthHHn1LNLkVqExiEGQRAmOUm6dmzdOfXsUpLa4Qi1NEWWkSAIydEMmTr17FKkFqFxyA5BECY5aWTq+H3+M/O5yP2JTTsciS3EixgEQWgDGpmpo/L557KEXIZQmjisTmO7S9EFr1csnNO0sYVWNVTiMhIEIVZUPv/SOOOI6VPQ1ZkHAejqzAc2qx8YLGD5hm1Y0z+E6bkMOvO5qvfeu3tvU+ocNaKCOylkhyAIQqzofPsjoyUMXnG61Tn8u4x9oyXkc1lsWtldMSJr+odCXb9RtHIQXHYIgiDEShz6QzayGM2qc9TKQXAxCIIgxEocWU02k2pa2VOuK2v+2q1YvmFbjSsoyFAFvT/MteJGXEaCIACILxAaR1aTTe1EWtlTQYFsUwV3mCK7NAryiLl1etL09PTw9u3b0x6GIEw6/JMPUJ7EggK/7TIel+UbtikNVVdnHg+sPbXyWGdcbd8f5lo2ENEOZu4JOk52CIIgNF0gtBGr/yg7Itv4gC7NN0x8IY1YhBgEQRCaMhCaZO1EVHdMvTIgYd7fKDVZLxJUFoRJQL3Bx2bN2AmL7X2I2tyn3kB2mPenETSXHYIgtDhxBB9VgVAAmPf6+A1CUlW8Ye5D1B1Rva6sMO9PI2guQWVBaHGiBB9Vk/Lm7c/igV+8VHPsxcvm4qreRbGMVRUsJgAX+a4RxWjY3Af3vKrj/MdOJmyDyuIyEoQWJ+xqVyet8KDCGADAjQ/viWuoSlcNA7jhoWcr7p2o0g9B98F7XhXSqzndnsrHAvgvAG8AMAHgOmb+x7TGIwhJoFvpxuk2CRt81PnPdYwHeBFMn8X/mm4yZmdcvUu6tONb3T+EjXc/qb1XQfdBdV6XrhYSoEuSNGMIYwAuYebHiOhIADuI6PvM/NMUxyQIsaHzaW9/5iXcsqMQW8FR2FaWYTOHMqR/TfUZ1/QPYXX/EGZ15PDagbGKwmlhpAhCefI3jcs0PtO9WrFwDq5/6Nma96xYOMd4XgImpZsoCqm5jJj5BWZ+zPn5VQBPAGhv8yxMKnQr3Rsf3hOrSmfvki5cc+4iayVR3c5hxtSs8nkwQmXruBP+vtFSldy19zXNZXD8pXcajwH09+re3XuVx299/AUs37BNe95Wy6RKksAdAhHNAFBk5gkieguAhQDuYubabhcRIaJ5AJYAeDiucwpC2uhWpDoXTGGkiPlrt0ZyIYXJ2dftKK7+wCJceuvjKJYmqo6fALQFalHrFHQ7hSD3lOm6urHsGy1h36h6upK4QTU2O4T7AUwnoi4APwTwMQDfimsARHQEgFsArGbmVxSvryKi7US0fe9e9QpAEOIgbiEx3cozS3ofjCqIGve4VDuK85a6vvsJ5Xt0k22U1XVnPodNK7vRVcfKPENkLSqnw6YnQ7sRmHZKRI8x89uI6NMA8sz890Q0yMxL6r44UQ7AHQDuZuavBB0vaadCUqjSIXNZwoypU/BysRRp1a7T4zlvaVdVDEGHG+js2zxc5XrJZQgbL1gc20SmGqdqLP7UzedHipiZz2H/oTGUxu3T13NZwsbzy+Ofv3ZroItIh1/byOZzuBCApzacGfHKrUecWkZERO8AcBGAPwvxvsCTAvgmgCdsjIEgJImuy5fbBzhK4NdUWNRz3OzK86Yg67otu2r88KUJxrotu2IzCKbsG6DarXL5wE7c8NCzlTGPFEvIZQizOnLYN1oyBo0r4x9nXHLTMAB9ZpCL6Xx+rSXV/d5/cCywl3OrtrtMApuJ/bMALgVwGzPvIqI3Abg3hmsvB/BhADuJyG199DfMfGcM5xaEUNj4wqOIvel8+97ndQVVpslSNclFxfTZu3ypsqosntIEg7l8bGGkiCwRxpkrj1WMM+PSW3cad0vubure3Xu15wkSldPt0lwDl4bEdDMTGENg5vuZ+Rxm/pLz+JfM/Jl6L8zM/8PMxMwnMXO380+MgZAKtv7nJMTe0mr04qL77K6byJ0Y123ZpT3HSLFUmbTHmSvjn9WR076nWBrH9Q89W+mXDByOr7j+/at6F+GBtadq4w1Bv7egDKyomkaTFZsso7cA+DyAed7jmVkSd4VJg07Lx08SKYom19JqTd9gAJEzkvyoPnsuS9h/cKzqGmF2Je6kapM05PZLvtbTL9lmjLZGU7VLC5KwaIV2l0lg4zLaDOBfAXwDQHC0RhBaEP+k3OkrqgLSSVHszOe0E7E3IwmI7uLQfXZV/CQMYSbVIHdcnEJvNsHndq1NsMky2sHMSxs0HiOSZSTUS5gAYqOCjabuYABqsoxUZInw5Q/Gk3mki2mExXXz2J6rUZk/QZ+vGTqzxU2cWUa3E9FfArgNwEH3SWZWK2EJQpMSNoCYZIMWLyY/tpvqGZSR5AZpgfqDoXG5S1YsnIOe42Zbp4I2alVuG0QH2i8DycYgfMT5v8/zHAN4U/zDEYTk0E28l9w0jDX9Q6n9wesmKHcVa5ORBMTX8jIoFdSWe3fvrUhaeyfVFQvn1GQWhXXH1SMaqPt8qr7I7ZaBFGgQmHl+IwYiCEkTJCUR1x982FWlboIi51ze9/adsQBr+ocCBeJM4/AGVL0pou7rqgCuTX2Bn8JIEcs3bKtcf5MnaOytwwhriOsVDVQG0TOE0UPVQfRm6zPdCGxiCDkAnwTwTuep+wD8W5xaRrZIDEGoB1vfeD1NUkzxAFOsQjfJq8Yyb+3WwLFHqZL2jtNvTFSr+iD8RiQu37zu9+gaNz+qexhUbZ3PZbWftRWrnOOMIXwdQA7AvziPP+w89+fRhycIjcc2tdTGh65bfUdZVZrSS1VjMWUeuVLPJqVVnYCcv+eAfxJ1V/U2RlW1o4i6urbtqaD7XKp76HfD+e9nsTSuNTCTOQPJxiC8nZkXex5vI6LhpAYkCEnhT13MRPyDN/mWo/bq1VX1+scyMFjAKwf0m/PrH3rWWNlroyaqc7W4k6hpRwPoPwugvg9BDXb891rnvoo6gZtciapdTlCso5UD0TYGYZyIjmfmXwCAI10h9QhCS+JdGQbJGugw7QLCdi9zMRVeeScYINiXb1rB6yZNP7pgu3vPVGfwuoRMchwuA4MFrL99V5U0td8Y6fotqCZqlTtMdQ/dzwOUf5e6u+G/BgE4b6k586zVA9E2BqEPwL1E9EuU78lxKEtgC0JLE7XYyZQVpJJqMBkZ70TV2ZHDtCmZKnVVANZpm0GEUVoF1MF2nRBelqgiob2mfwidHTnkMqQt7DMVh3ldS7p7zSjvRFSigV4jM21KRhls7rt5GGAY6zv8rzD0TXhcWj0QbZNl9EMiOgHAApQNwm5mPhjwNkFoCaLUGpiygvyNWEyrSv+k6Eo4uNk4A4MFXHLTsHXTGBNdnbVKq94soyDcSc3kXvFOuvtGS8hmqLLKdg2G7edyr2ObIurlgKenw0ixVKXO6hJGrls1rrCvt4oUhtYgENGpzLyNiM71vXQ8EYGZb014bILQlIRJy2QAt+wooOe42TVGIUhY7dJbd8ZiDPw9g20UQVWYgrrkjN3LuGf17RoMoHw/gj6X61oKq2FkaukZB0Guv6guw2bBpHb6Luf/sxX/zkp4XILQtPgVNPO5jHHS0alnmlaTQT0KvGQzhFxG34UtrCKorqObm36qwmbS1fWT9uOd8MP2i05yJW4TX0pbubZebOoQ5jPzU0HPNQKpQxCajcsHdip7BKjwyyLoAq+ub1z3l5nLEI6YPgUjo9WxBn+AFtDn/ofJ7PGexzbtNCqd+RzOWnw07t29N1KWju6e+ndwuSzVxBBMxXezOnJghrF7XlDBX5rY1iHY9FS+RfHczeGHJAiTjxsf3mN9rL9Xsmk1qVvVZwiYMa3aGLguoMErTse1Tq9i02ranfALjtHxj8u0Kq93Ba7bx2SJcO3Kbqw750TcsqOgHVsQunt60bK5VZ9n4/mLsfGCxVXPXbRsrvK9Fy+biwOlCYwUS9oxee8pUN0TIm1jEAZTDGEhgBMBzPTFEV4HYHrSAxOEuEgyLzysj9+bcRKU5aTqUQBGlSx13+bhys7AuyrdZOgtYJMJowu2B+kcdeZzeNkZn+rOTJ2SwcQE12T3XHjKsZV01XqydMJmjvmfV0lq2NyverKLmqluwZRltADlWEEnynEDl1cB/EUcFyei/3Cu8Wtmfmsc5xQEL0nnhdtm6XjxrrJNLTaB4P7ApQmuuIlsNZnqaQrTd8YCoxz3jGlTMHTl6Vp5jYNjE8gq4h39j+xBz3GzQ2fp6CbTenpD+N+7xqKKPGp2UbPVLWhdRsz8PWb+GICzmPljnn+fYeYHY7r+twC8N6ZzCW3AwGAByzdsw/y1W7F8w7ZAV0LSLRIvPOVY5fMXOy4KFZ2GtpJeepd04YG1p+KpDWfigbWnVlbeNug+48BgQeu2yRAF3s/eJV3YeMFi7euuoJ2JcYUxKU1wpbBPher5gcEC+jYPV7mX+jYPW7uXbLEZU5hxe2m2Fp42MYRPEFGn+4CIZjkr+7ph5vsBSF8FwQqT71tnKJLOC7+qdxEuXja3kpmTJcLFy+biqt5F6DtjQdnN4+O1A2ORJq2wqYuqz2iqzB1nRt/Nw+hef4/R4PYu6ar0QFYRNej8/EhRec9yWVJm6azbsqtmp1KaYGPv5yjYZA5FzS5qtroFm0rlk5h5xH3AzPuIaEmCYxIEJbrV1Prbd+FAaUK57W5EXvhVvYsquv9eepd0Yd2WXUo3T5TKVVtxPhfVZwyaaErjXBWjWNM/hNX9Q1XZMgODBew/NBZq7DYwgMtu21lbNKaxYDqBvzC9n22wiUtErXpvtroFm7TTYQDvZuZ9zuPZAH7EzLV/AVEGQDQPwB26GAIRrQKwCgDmzp279JlnnonjskILMn/t1lBFRu4kZppEO/M5rDvnxMRaY+pUTKNeP0i22cVNobRNdbUln8vg4NgEAjp6VnCF7qL0U/CfJ4wM+NMtIk8dRS49CnHKX38ZwINE5KaaXgDg6noGFwZmvg7AdUC5DqFR1xWaj7CdvJ4fKVb+qFQ5+kB5Ndm3uSzeG9cf4MBgQbkzUGG6vk3AVJX7DhyefAsjRazuH8L623fhyrNPxIqFc6zrJlQUPbIQQbiTeBw9mlU7m1kdOeXvVKUnFZakM3/8hn16LlOTSpwGNlpG/0VE2wGcivLC41xm/mniIxMEHzoZg2lTMsrJ17vtPmCYyKK6cFTYykAEXV+VfdJ38zDWbdmFl4slzMznQITKJHLtym4A0O5I9o2WsLp/CIaC5ljx+s/j8Ie7v0v/RJrNUE2QemS0hMsHdirdeDYknfnjP/9IsVrDKk1MdQivY+ZXHBfRrwB8x/PabGauOxhMRDcCeDeAo4joOQBXMvM36z2vMDnR+WmB2px974RkIwPhn7SirhDDSE6Yrq86j9e/7zWA7oQ1PRecI2Lr6qkX8jiI6u3RTEBFwto/keYyhClTym4sFwYqu6AoRiFpxdJmVkQ17RC+g3KNwA7UyoIzgDfVe3FmvrDecwjthSnHXDeB26xQ/Tr9UVaIA4OFyBOfP4gYdlVdLI3HIpEdF6OlCazuH8K6Lbtw1uKjtZLbuQwBpFcfJQAXLZurLVorTbDWyt348B6tQTAZ/KQzf5ots8iL1iAw81nO//MbNxxBiIbJUAStUHOZ6rTGKCs414gEoQqu+q9vM+ZWYaRYwi07CjhvaVelk5u3onrFwjm4Y/iFyo5nxtQsctmMUjMo7ISpKxgMMvhJZ/40W2aRF5PL6G2mNzLzY/EPRxDix5RppMryibKCs3UVTckQpk7JYP+h8rGEwzEE4PAOJGyKaTNTLI3j3t17lY3u/Z9xgqHNugprJHWqrUEGP6zkdliSPn89mFxGX3b+nw6gB8Awyt/fkwA8DOD3kx2aIJSpN+MjbI54lBWc7eq1NMH4rY6puPoDC4yrVHdsQWmrUcjnsg03NLoiuTA7Md1E+ra5M/HAL2pDmroq8iCDH7WmwJakz18PJpfRCgAgou8CWMXMO53HbwXw+cYMT2h34sr4CKNvE2UF16lJgVRRGCli3ZZdVgJz9chN5zKEjRcsxvZnXsJ3Hn624mpPY9fR2ZGrMexhNZVME+nlAztx48N7MM6MLBEuPOVYbfzAxuDbfF/qWajUo7eUJDaFaUPM3B30XCOQfgitTZQ/IFPPAF0LxTjwpzd6UzxV4+5ef09sFbL+quB6XEf5XAZjExy5ZWRcZKjswrHpP5Alwpc/uFhZb1HvilpXIxK2GMy2oKxZlEzjLEx7goi+AeB6lH9/FwN4os7xCW1G1JV+WhkZ7grOdtxhhOeCUF3D3SmErfi1LSSLotoahnIyUPX5TZpK3s8f1y5RZ1xndeRw5dnhqtVt3F3NpmRqg4243ccA7ALwWQCrAfzUeU4QrImq6hhVRTIuTK6dJMfjvYaretrVmY+1P7ALoXayThvv549LEVQX+O+YOiX0BG2zUGk2JVMbbCqVDxDRvwK4k5mb95MITU3UlX6aGRkDgwWtG8g/7iSyggojRcxfu7XSyzhKLCFDwcVorjHTnb9eHaKouPc4rl1iXOcZGCwgo9lReRcGzVxvoCNwh0BE5wAYAvC/ncfdRLQl6YEJrYlOhjrqSj9sk/U4Ma3k/ON2xxk3rsx3FP2hXJbwoVPmKiW4vRRGisZJigGj3HVSuPdY306UrPtimM6j67Wg+h67biCVMfAvVNLe3UbBxmV0JYCTAYwAADMPAZiX4JiEFsXUryCqXjxQ2yimUf5X0ySpGnfvki5tU5xG4ebeu32Dr+pdhI3nLw4UfDPtALJEWHfOibGIxtlqKXm/G6rvDlCONYTpu2z7HTQ13tG5nbJENQuVer7zaWETVB5j5pdJU+QhCC4mn6mbERR3xkVQFkc9WR6m1Eh/IZlL3xkLEqkdsEXV3F0XIA9zzrjcYbZaSuct7aoaP3D4u6Ny1+jqF1RqsF4XmEr/ydR4R5c8MMFcc+1mrjfQYWMQfkJEHwKQJaITAHwGQFwtNIVJhE3BT9wSwqYsjnqzPExxAdO5iABVjHZWRw4jo6XE/fGqyXFgsIA1/UORrp0lanjtwr2791Y99n535mv6IKgECr2/P780OFBWgfX/Hk2Nd7o0iwSdG6hZ6w102BiETwO4DMBBlAXv7gZwVZKDElqTpDVa/Kv90UNjxtS/MJWwqpVkV2e+SoPHjz8TxpQWms9lceXZJ2L7My/hhoeeTdwoeCdH1wUS5Zr1VDWrpKltMbnrbL9ntnIiYZRG+85YgL6bh6vqOnQtPlsRYwyBiLIA1jPzZcz8duff5cx8oEHjE1qIJH2mqviErjI4bHaK99zA4ZVkYaSIW3YUjON3dwrue3WFVq5/+areRdi0srsSa7BxxLp9msMEdmfmc5XA6Or+oRoXiC3F0rhWE8jF/2o+l8W1K7vx5QsWB75Xh7f/gT+4G/Q9c98TtpmSiy5WUnnefyubK2O3LowGgZnHASxt0FiEFifJjKAwfQaCslPCrCTdns26ac3GneL3L4etK5hgxlW9izBjms2GvsyhsfEqQ1UPQQVrbqtO/++8d0lXpGI3QtnQdq+/B303D9ckKQDlGINrbLJElZiD37jb4v1OXHn2iTWZWbks4cqzT8TGu59UxheaubYgDDbfsEEnzXQzgP3uk8x8a2KjElqWpHymtrnb7mSyfMM2rFg4p0aH311JhmlzadIospnw/L0WbK/rQhReGmM0RKvLuPsduyv0qPn27lhUn9c10AdKE5V7P86MW3YU0HPc7EgNivy7WFMweI0mYaCZawvCYGMQZgP4DcotNF0YgBgEoWHo/Mb5XAaHxlgYD8lOAAAgAElEQVTZS9irw+/vsNa3eTiyG8VLkORDFAPkZ4L1gc56yecyeOLv3ldX3+MVC+dUfo6ayWRTQOeiMtBuHMA0Mbu/q04LbSrdwqaZexnEgU2lcmIyFUT0XgD/CCAL4BvMvCGpawmtjSrjJ5chjE2wdkLW6fAv37DN2hiYejYDwTsEt1itWXsbZIgqxiDqTsGbERS1hWgcbniTimpcYoh9ZyyoWUyoGhy1KoEGgYjehPKkvQzl39uPAaxm5qfqubATsP4agD8E8ByAR4loCzP/tJ7zCpMHf1bReUu7qrprjTNjIsAzoloxBm3vvVlG7h+6LmVTl4bosrp/KHHhuHrYf2gc+w8dDohHMQre+xnZTWR5UZOBdlf7iUud+ANKk6hEy6ZS+TsAbgJwNIBjUI4lfDeGa58M4OfM/EtmPuSc8/0xnFeYBKiyivof3YP9B8cqx9gs8lVbedP2vqszj19c80d42lcV3TG1tlIWKLtLgjJpmtUYqGDoO43p8N7PqK4T0zVndeSqAtbrzjlRm2WUtNTJxrufrJESL423V1CZmPm/PY+vJ6JPxXDtLgB7PI+fA3BKzcWJVgFYBQBz586N4bJCK6ByPYTV9NetDFXbfkCdTx7kE+9/ZA+WvWmWsmNXqzLOjFyGrNxq/nusWqEH7TryuSzOW9pVkwBAAC5aNlfb6EZXAZxkMVhYwbpm6Ydgi41BuJeI1qK8gmcAKwFsJaLZAMDMUf8SVEuCmu8NM18H4Dqg3CAn4rWEFiOq68GdfLoMf3zuc94gr04TP8gnXppg7Hr+1UhjDYP7uRrlfipNMDpyGRRLE5iZz+HVg2M1RWaqftT+DJ3OjhwOlsZrsp5Uvyc3S8hm8kyrAjhMULkV+yHYGISVzv8f9z3/pyj/Tt8U8drPAfA2PX0jgOcjnktoMupdGYVpqJ4lwgRzqOvYTig2himpDCAvjMMr6e889Czsk0qjM1qawMXL5uLe3XuVn3HGNHUfgVrtpOrRqgyJ931hGRgsYP3tuyrZR7rzu8fqvpc239kwMYqwPaObAZsso/kJXftRACcQ0XwABQB/DOBDCV1LaCBxrIyUWUVZAhhVrgyb1odJCdw1mmJpHHcMv4BsljDhc58tP342Hnv25ZqJqt7MJpPMhlvvobufut2VzpBEYWCwUCMlMVIsoW/zMIDq75vpewnA6jsbRrBuUvZDSApmHgPwKZS1kZ4AcBMz70prPEJ8xNEpShUc3Hj+Ymy8YHGogKFJktuGvjMWGPsJ5DKEGZqAcxKMFEvKWMrTvynW3K/zltY/6TLMktWFkSLW9A/h8oGdNa81YkJUBXkBdfWw6XsZ5jtrK8feiv0Q7GvhE4CZ7wRwZ5pjEOKnnonAZjUfR+/b9bfvsj+PZoncmc/hrMVHo/+RPeoDGoi7i/Hm2i/fsM3qvUEB5KDYMqO8k+g5bnbVPW1EEZfpO+V/Lcr3sh7jlWa3v6iktkMQJi+mDlemlXm9q3kVuj/ofaOlwLGYhOG6OvMYuvJ03Lt7r3EyzWXIurHMlIDuMUHn6rt5GN3r76kIwdm4uro689h4wWJcvKy+DD5GbYe5RjSIMRkX/2umFXsSq/k0u/1FxaaF5nIimuH8fDERfYWIjkt+aEKrYupwZZrgdav5S24ajmwUTH/QOheWjUBakKKqS1D7Si+//brpuNajhFoDAWeedLTy3gLltNyRYqliTIOunCXC8yNFbLz7SfQcNxsXL5tb856cbYsz1N6LeidEXRtLLzqXnqp62GSgkjJeaXX7i4qNy+jrABYT0WIAXwDwTQD/BeBdSQ5MaF3cL/0lNw1bd7YC9JOra0i857bF1MHM3zOgooWjaXDjxauoajIco6UJa6G5IONSGmfcu3svrjl3kVVXtqDKY6/M96W37sQ15y6qSf0cPTRmFPfzojK+9WQOhQny2mQZ2QSEW6lmIAmIA775RPQYM7+NiK4AUGDmb7rPNWaIh+np6eHt27c3+rJCROav3aqcjAjAUxvOrHl+yRfvMU4+UfVoTEqhXZ15pSqqCbdgque42bjstp3Yfyg+jSIb6YhrV3ZXGvLYMKsjZzWpq+6v7nfohwBsWtld1wTqNcqqNpm6MQrBENEOZu4JOs4mhvAqEV0K4GKUC9KyAOrvti1MesL6ZYNW5YWRotF94OJ3NZy1WO9mKYwUccNDz4ZKz2QA/Y/uQd/m4ViNgXvuIPpuHsaKhXO0n8nLrI4cXimOBR4HqHcotj703zu+vLOw+f2o8MePdMV3zZyyORmwMQgrUW6f+WfM/CuUJSc2JjoqYVIQ1i+ra2DuRRdsdo3AvLVbsaZ/qCow7cpg63zzUep+S+Mci3x2FLyuI9c/P6sjV+Pvz2UJrx0Ys65sVk3+uniQlwyAR57eV1cygK1K6vRcY/NgbOIYkwmbwrRfAfiK5/GzKMcQBMFImCIeIFwRmDcW4fc3+6c/rwy2rQuk2Xl+pFjjn/en7O4/OGZdRU0oC/W5jW38v6s1Nw1pd3ATQE2hXLE0jnVb7FN7bVf+xdIEBgYLDfHtt6L0RL3YyF8vA/DPAH4HwFSUexe8xswzEx6bMAkIE1RU5W2bcI2HzerSnXB0RqfermF+ZnXkMDJawkynGYttYNYWmwDu/LVbrc/35t+aURVH8VfxTslQaHHBkWLJevKemc9ZG69GST+0ovREvdhkGX0VZVmJzQB6APwJgBOSHJTQnnh3FDY7BUJ5FWezuuzsyGkbwbgaQW5nNaB+4zB4xelVjwcGC7F1afOqspoK+cLsuH65d1SbEQaEV5p1sZk8BwYL2H/ILs4BNC6O0IrSE/Vi5ZBj5p8DyDLzODP/J4B3JzqqmGk3P2Ar421AH4RbDBUU+HR96QXPZO96293c+Kt6F+GBtadi08ruwAIxoJza2JlX51aoxr5uyy6lMcgS4eJlc6ty9U3M6shh4/mL0bukC5cP7KyJl3h99zb+fxdTELeeCdDmvetv3xXK4DRK+qEVpSfqxcYgjBLRVABDRPT3RLQGwIyExxUbSVS/CvFgMtS2k9nzI0Xlsd4Jf8bUKTWTsSu97C8W2nj3k1ar+NL4BM5afHRNUZSqp8LlAzu17pAJ5ooxcouXdEahqzOPwStOr8RNVMJz3lW9qjBMZ8R0DWo6O3J1TYBB7x0YLIRypxHQMOmHRlRaNxs2BuHDznGfArAfZcnq85IcVJzEIbQmxE+QoXYns6DmXcd05pUT36aV3ZWuZ7rsJX9hWphG8/sPjePGR/Zg3L+y9T28fGAnrn/oWeP4/egMnKsu6rqJTCqkrpEFUGVszlp8dE01cj6XxYWnHKus+H3twJh1iqsfm8kz7N8ho3EB3VaUnqgXYwzBqTm4mpkvBnAAwPqGjCpG2tEP2ArYBOx6l3RVVaD68U44puB1kMhaUFc0Hf6GMcBhlU13FW8yBoA5s8eNpXhjHq7hDBqr18gCqIznlh2FKkNCAM5b2oWrehdV9av2fh43xTWoaKwzn8OMaVNCVfqG/Tu0cSXGSVqNeNLCaBCYeZyI5hDRVKfvccvRCMVFITy2hnrE4E6wXa0FqU7a5sDb4n6G9beb1dxnTM1qM3vciUi1awkzVq+RVX1OBnDv7r0A9HUg/hRXlQHN57LapjQmdH+f+VwGALWUUuhkwMZl9DSAB4job4noc+6/hMcVG+3oB2wFbAN2uuO6HFeRDUFb/7h3i+6YTb5xApDLZpS7pNX9Q5i3divmrd0aS3OeICE+b0quCv/z/vvZmc9hei6DNf1DoZM2Viyco3RhXXPuSW3nrmkGbAzC8wDucI490vOvJWhHP2ArYGuo05ZQDovt2C5aNrchrTeBaiE+0+th7rWbDbZpZTcOjk1g32gpdNKGyYXl7khaSSl0MmBTqbweAIhoBjPvT35I8dNufsBWwKaK2Q2eFkvjlebyXZa+aS9BFadhC+JMuJMZUF45qyb9XAboOW62sT1lFHKZ8j3yhzZGD41hYLCAFQvn1FzTG6zuO2NBVazAJg5gW7ylqpcIcmEJjcdG7fQdKEteH8HMcx0Z7I8z819GvijRBQDWoVz9fDIzW0mYitpp+6DzU0fZ3emyh7zKmQODBazbsks5gWcA66b2/nN+rn+o5r25LOGIaVNir16+eNlc9D+6R5nTr+pH7cd7f237UNso2up+lzoDrFPDFaITp9rptQDOAPAbAGDmYQDvrG94+AmAcwHcX+d5hElKXOnCA4MFrR/e61PvXdKFGdPUG+aZlh3PVOdUvbc0zrEbg67OfLl7m6bAy0aMz72/YWp3bOIOut+lLqVYEj7Sw6qnMjPvoerfXl17a2Z+AgAoKMlcmFTYrjqBeNKF3YlNhzft1CSXEWby9k9mpiypuHD9/GssmuYE4XZQC3IDBd2zea/PV9JpdWZI5ZxQdToTGoeNQdhDRL8HgJ2K5c8AeCLZYR2GiFYBWAUAc+fW1/dVqCbMBB3HtcIoR8aRLmxKJ81lCfsPjmHe2q2xCdupArBh9ISikKHD6bdhmuboOKYzH2iMbeo2HvzFS5Hu6RHTp0i8L0VsXEafAPBXKPdBeA5AN4DA+AER/YCIfqL49/4wA2Tm65i5h5l75syZE+atgoFGS3qEdQGpMl5yGcLI6KFKSmb3+nuM4zXuJhiVeEGUiaszn8Py42dXJB+yRFUBZdPniJOvfPBwl7J6V9auQQtyA9nUbUQ1sI3YUQl6tEFlInojMz+nee1sZr697osT3Qfg8xJUbjwmmYYomTxBmPoQ6K7n3cHMzOfw6sExZXWw/xxB7oysptI2DLM6cjhQmlBOjO6Oo9MjfR3HNf3XuGjZXFzVu6jq+fmXbg3sPOclS4QJ5qodYlBAv96eEqZ7kUSLzEbuhJsV26CyyWX0QyI6g5mf9p34YwAuB1C3QRDSw7R6TqIRiMl1Ymqg7j5evmGbMW/fPcf2Z14y9kc2Zbd4cSUSosQV3KnOO95xZuRzWUzPZeoOKJsMdlibM8GMpzacWZk01/QP4ZjOfJUceFhZbZ0LzmREgWQKRtuxyU09mFxGawB8n4gqvQ+c3sqfA/Cuei5KRB8goucAvAPlPs1313M+ITxBvvi4BQCDXCdB17MJJhdL47jx4T3aCcctSrTRwymMFLH/oL1Gvw3F0jiYUZcLSaXQ6n89DMd05pXuwxseehYrFs7BUxvOrNQMuIJ5KrE7r7rsRcvmKgvcmPWyG0kVjIq4ZTi0OwRmvpOIDgK4i4h6Afw5gLcDeCcz76vnosx8G4Db6jmHUB82xVhuwVIcW22b5jem69kGZ3WuCAKqXBE2hWgjxZKyBqE82TGKJdvqhMO8XCxh08ruyK0ugwyj6veayxByWcKob7zuilxXIHaDI8zn11tye1TrdhBAufDO76bRZUH5fzdxIuKW4QgSt/shEX0UwH0AHgTwHmY+0IBxCQlj253Mfa0wUkTf5uGq90a95iU3DSsnbrdq1r2ev5q47+bhwEYqOv+0d0dk0yfYZcIZV6fTEvOYzjxWLJyD/kf3mN+owZXr9lfx+idxndslaGfnrwB3W3iOjJYwqyMH5rJR8k7iuomaAdz48B5lJzW3R7VpHP7vie67lmTdgYhbhkPrMiKiV4noFQB3AXgdgPcA+LXneaHFcbVirl3ZbeXGKE0w1m0xK3iacCc+nTEIavay8fzFmGUoEstlCNNz6q90YaSI4y+9E5cPePzHlv52BnCgNIFNK7vxwNpTjQVgJky6QH69LZXbBQD2HyzLUJiaC+l0hvaNlnBw7PDn8O6+dJg6qYUlDaFJEbcMR6B0RTMhWUbJ4c/EMO0ano4oK6DLbDJlnZhkDPxZSPsPjVlN1MuPn42nf1MMnbPv+u+Dsmz8WUbuziKMy80kpZHLEEDVfY7da3oDzjaSHe611vQPKT+T7ncTNRsojYwfyTKKJ8tIaCP8W/x5a7fGfg3dqnLCEa0Lu7UPk4Xk5YFfvGR1nB+vTLRqrJ35HIauPD3Sub0MDBbQt3lYKzWhet7fQMc7Xj/+53uXdGH7My/VCN/lc1mct7SrJmurnhV2GkKTIm5pj01hmtCGmFwzUYvXTAVPQVt7k4sEaEyQ0CsTncvUyq7sd1RF62Xdll1WfZ11uK423f3u7MjV3Murehdh08ruGpn4q3oXiXx8GyEuI0HJwGABqzXBxnrcBarsnlkdOVx59okA1HLYNsqnYfohR8F/vSVfvEdZTxBHYVUcuzMCsGlld23GkUL1NKqKrNA6xKl2KrQhpskh6mrcDZ525qt3H/tGSxU3h6ohSlAu+cBgAaOH4q0Z8OOfMHUSC43YqbhppCYyRFjTP4RpUzKY1ZGrrO5nTJ1Ss/uQvHzBRQyCoEVX5FRPyp5OZto0KZl84e7uQVf9m81QpWgqS4Tlx88OXRjmus+8bpa8JptpZt5eKjvoeiq6OvPYeMFibDx/ceX3ozIN48xglGspvBlSpr7JgiBBZUFLUHN6HUFZHWGKhQYGC8gYaguChNbGJ1iZVbP+9l1WEhK5LOHMk46uCvIaZRvILqvFdMyVZ59YU3ORyxI2nr+4RtrDfy7VvfJKV8/UdHGLw5AJrY8YBEGLTZtLPzbaMbbFQqa6hTA9ALyGxp083UKtA6VxbcUxAVj59mNxx/AL1kFe1/1l+vxB98h033WGxH3PfE38wb0HuhYk0ppEAMQgCBr8E8+mld1WQUeb5iq2Ow/d6j9LFKoHgLcRjve6+0ZLSneLi9vf1zad1R1b0Oe3uUeqVMk4jK0u9lGP7LTk+U8eJIYg1FBPrwQbd5CqMleV5WKqW/AaF1NMwGtodJo9UT6P7lpBlb22LT1V2Ai1BaXv2rS8DEOj+2oIySIGQaihHoVI2wnHlVfwZxTZnGtm/nAe/ca7n8R5S7sqxmVWR65cIYxaQxMlcMoodyXT4c3gUWVQeccc1NIzQ6StswDsje15S7u0jXvilnIQNdHJhbiMhBrqUYiMGoi2PVcuQ9h/6LA6qKu+qdph+DX+dQHVIEzhAzeDx732+tvVWk9EwZ3G3N2FTrPfJvYyMFjALTsKlXONM+OWHQX0HDc7MD4RBVETnVzIDqFNMVX+hnEr+M8DILbKVpVr6YjpU2r0ilQrUpUrY/+hsZoK43rbW/qvbfLRh5kkVZ/JZnVvs2K32Z3ZErcLSkgX2SG0IUHBSdtVvu4815y7KDZ9e39wNSiLxkU1MZbGGbM6cuiYOqVqdVxvc3rvtYNW8WGuo9IcAsyr+0av2OPcEdaLBLfrRwxCGxKU5WLrVrDJlokb25RV3QS4b7SEjqlTarKmbBrm2PRa0DUe2n9wDGctPlopFDdtSkbpylKtsoOE2hqt/x+3Cyoq0iozHlJxGRHRRiLaTUSPE9FtRNSZxjgmI0EicIB9cDLIrZCG/9g2KGqaAP2ZMH7X1KyOnNK1dOEpxypdTKMeUTv3XP5q45FiqdJpzO9OW3fOibEFehut/98sq3IJbsdDWjGE7wN4KzOfBOBnAC5NaRyTCtsUQN1kGZTl4icN/7FtymrYHs5eAzh4xenYeMFirfKnTovJaxQ6pqrlOdxOY5tWdgMA1vQP1WRKxR13SUq4rplSTiW4HQ+pq50S0QcAnM/MFwUdK2qnZsI0RAlykdgoYNqokIbF3/QmaoMZ77nibvZjc591TXR0KqStqDhq+31rt7E0I62kdvqnKLfpVEJEq4hoOxFt37t3bwOH1XqEaYjiXUVmFboFNtvtuFej/hXnSLFUaf8YZfXprvpVnw9Qf24bbO6zafc0WdwbuvtQGCmG2mnGgbTKjIfEgspE9AMAb1C8dBkzf8855jIAYwBu0J2Hma8DcB1Q3iEkMNRJQ5iAYhj9GxNxdqMKytOPGrDWVQ/rng/C5j6bsm90+kut5t4wtVr1GnEg+cBuswS3W53EdgjMfBozv1XxzzUGHwFwFoCLOG2/1SQh6iqpUyO33OhccpsJMcqkGfcOweY+m3ZPkyV3PyhOAzR25xNnfUW7kkraKRG9F8BfA3gXM4+mMYbJSFR10tcO1DaXyWWp4dtt04rTe0xY4twhuHGJYmm8kobapbnPut1TM+Xu14P/+6a7m62282ln0qpD+CqAaQC+T+VV2kPM/ImUxjKpCOvC2Xj3k0pp5xlTpzR8haXL4XeJOml2aQyNrgGQDn8QfZy5MqYw9ypO90baaZ/e75susNtqO592JhWDwMxvTuO6Qi261Zuus1YUbCct/0RZb5aRS9gVuW68cRbixRF7abZirMmy82lnpFK5zUm6sjXspOWfKL2Ts+uLjjL5AnYrctN4TVk1A4MFZQezJFftaVSKm5DAbuuTeh1CGKQOIX6SqCXwUk9+uM3Y4p58TeMF9FpE7riAWhmMpGoMTLUOT0WorxAmL61UhyCkSNKVrfVUkAbl60etlDXJe5jGa8qqccfVyBqDOLKVbKROhPZBXEZCrLUEfupxSQV1FoviMglyYZnG655zdYQ6giQyber12TdbDEJIH9khCIkStTZiYLCg7XfsGpMou4+gFXzQeHuXdGmzkzJEDa3pqHd3N1kqpoX4kB2CkCi2gUZ/LGD/wTGtf9zbHzjs7iPIiNiMV5ceO86M1w6MIZelqiY+SWba1LO7E0E4wY8YBCFxgiYtletCB+PwpB3FZaJroznTo2AaNF73tUtuGq4pbitNMDrzOcyYNqXpM20a3TtBaH7EIAipE6Rh5MXrromS5qhTqwirYtG7pEurSfRysYShK08Pd8IUkLoBwY8YBCF1bF0UqskqrMvE1PM4LK2+wpa6AcGPGAQhdXQTa4aA103P4eVifZXKNteKMolPhhV2khlmQushWUZC6ujy+ycYODg2gU0ru2NTr4xTN7+R3ckEoRHIDkFIHVOQNm4phrjdJLLCFiYTYhCExAgjK2EK0sadBmmaxNNWDxWENBGDICRClCrYtIO0UrkrtDsSQxASIUoVbFJ9cW31eqRyV2h3ZIcgJEKUKtgk0iDDrPqlcldod8QgCImgTyUlzF+7VTvZxx2kDSOAl7bLShDSJhWXERH9HRE9TkRDRHQPER2TxjiE5NClko4zh5Kqrpcwq/6kXFaC0CqkFUPYyMwnMXM3gDsAXJHSOISE8OfoZxXaEDr/fJwa/WF6BkhdgdDupNVT+RXPwxmAUthSaHG87p/5a7cqj/Gv1OPO9AlbTSx1BbVIKm77kFqWERFdTUR7AFwEww6BiFYR0XYi2r53797GDVCIFduVetyZPrLqr4+oXemE1iSxnspE9AMAb1C8dBkzf89z3KUApjPzlUHnlJ7KrYtt72bpE9xc1NMTW2gebHsqJ+YyYubTLA/9DoCtAAINgtC62KaUSqZPcyGpuO1FKjEEIjqBmf+v8/AcALvTGIfQWGz885NBQXQyIQa6vUirDmEDES0AMAHgGQCfSGkcQpMRtjhNAp7JIga6vUgshpAEEkMQvNjGJYT6EKPb+qQeQxCEpAlThSxER1Jx2wcRtxNaFgl4CkK8yA5BaFniCHiKO0QQDiM7BKFlqVd7SIquBKEaMQhCy1JvFbL0PxCEasRlJLQ09QQ8JQYhCNXIDkFoW8IooQpCOyAGQWhbpP+BIFQjLiOhbUmiZacgtDJiEIS2RoquBOEw4jISBEEQAIhBEARBEBzEIAiCIAgAxCAIgiAIDmIQBEEQBAAt1g+BiPai3FBnMnAUgBfTHkQTI/fHjNwfM3J/qjmOmecEHdRSBmEyQUTbbRpWtCtyf8zI/TEj9yca4jISBEEQAIhBEARBEBzEIKTHdWkPoMmR+2NG7o8ZuT8RkBiCIAiCAEB2CIIgCIKDGARBEAQBgBiEVCGijUS0m4geJ6LbiKgz7TE1E0R0ARHtIqIJIpIUQgBE9F4iepKIfk5Ea9MeT7NBRP9BRL8mop+kPZZWRAxCunwfwFuZ+SQAPwNwacrjaTZ+AuBcAPenPZBmgIiyAL4G4H0AfhfAhUT0u+mOqun4FoD3pj2IVkUMQoow8z3MPOY8fAjAG9McT7PBzE8ws3S8P8zJAH7OzL9k5kMAvgvg/SmPqalg5vsBvJT2OFoVMQjNw58CuCvtQQhNTReAPZ7HzznPCUIsSMe0hCGiHwB4g+Kly5j5e84xlwEYA3BDI8fWDNjcH6ECKZ6TvHEhNsQgJAwzn2Z6nYg+AuAsAO/hNiwKCbo/QhXPATjW8/iNAJ5PaSzCJERcRilCRO8F8NcAzmHm0bTHIzQ9jwI4gYjmE9FUAH8MYEvKYxImEWIQ0uWrAI4E8H0iGiKif017QM0EEX2AiJ4D8A4AW4no7rTHlCZOAsKnANwN4AkANzHzrnRH1VwQ0Y0AfgxgARE9R0R/lvaYWgmRrhAEQRAAyA5BEARBcBCDIAiCIAAQgyAIgiA4iEEQBEEQAIhBEARBEBzEIAhNDRFd5iiePu6k5p5iOLaHiP7J+XkdEX1eccwXieg05+fVRNSR3Oirrnufq9hKRHealG2JqNcrWucdsyAkiVQqC00LEb0D5SrutzHzQSI6CsBU3fHMvB3AdtM5mfkKz8PVAK4HEKkokIimeMQJrWHmPwo4pBfAHQB+6hx/hflwQYgH2SEIzczRAF5k5oMAwMwvMvPzAEBEbyeiB4lomIgeIaIjiejdRHSH/yRE9BdEdBcR5YnoW0R0PhF9BsAxAO4lonsV73maiL7knPsRInqz8/y3iOgrznu+REQzHA3+R4lokIje7xyXJ6LvOjubfgB537mPcn7+E+eYYSL6byL6PQDnANjo7IiOd8fsHP8e5zo7netO85xzPRE95ry20Hn+Xc55hpz3HRnbb0eYdIhBEJqZewAcS0Q/I6J/IaJ3AYAj29AP4LPMvBjAaQCKqhMQ0acAnA2gl5krxzDzP6GsA7SCmVdorv8KM5+MckX5tZ7n3wLgNGa+BMBlALYx89sBrEB5Ip8B4JMARp1eF1cDWKoY24nO+091PsdnmYsJk0oAAAJRSURBVPlBlOUo+pi5m5l/4Tl+Osp6/yuZeRHKO/xPek75IjO/DcDXAbjuss8D+Ctm7gbwB7r7JAiAGAShiWHm11CeSFcB2Augn4g+CmABgBeY+VHnuFc0rpsPo9xM5jx3lxGSGz3/v8Pz/GZmHnd+Ph3AWiIaAnAfgOkA5gJ4J8ruKDDz4wAeV5z/VAA3M/OLznFBOv4LADzFzD9zHn/buY7Lrc7/OwDMc35+AMBXnB1RZxQXl9A+SAxBaGqcifc+APcR0U4AHwHwGOxkn38CoBtlVdCnolxe8/N+z8+EssGpauRDRP73qCCLY/zHm3CN3jicv21m3kBEWwH8EYCHiOg0Zt4d4ppCGyE7BKFpIaIFRHSC56luAM8A2A3gGCJ6u3PckUSkWtwMAvg4gC1EdIzi9VdRFhfUsdLz/481x9wN4NPkWAAiWuI8fz+Ai5zn3grgJMV7fwjgg0T0eue42QHj2g1gnhvPQHkH9CPD+EFExzPzTmb+EsoB94Wm44X2RnYIQjNzBIB/dlI0xwD8HMAqZj5ERCud1/Io+8WVaZnM/D9O+ulWIvpD38vXAbiLiF7QxBGmEdHDKC+cLtSM8e9Qji887hiFp1HOjPo6gP8koscBDAF4RDG2XUR0NYAfEdE4ygbsoyi3xvx3x81zvuf4A0T0MQCbHQP4KIAghdzVRLQC5V3DTyFd+QQDonYqCAqI6GkAPa5/XxDaAXEZCYIgCABkhyAIgiA4yA5BEARBACAGQRAEQXAQgyAIgiAAEIMgCIIgOIhBEARBEAAA/x9lePBKFBWzhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #14\n",
    "\n",
    "#### Iris LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "Epoch 1/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0149 - acc: 0.55 - 0s 125us/step - loss: -0.0099 - acc: 0.5500\n",
      "Epoch 2/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0126 - acc: 0.45 - 0s 125us/step - loss: -0.0085 - acc: 0.5500\n",
      "Epoch 3/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0152 - acc: 0.55 - 0s 125us/step - loss: -0.0105 - acc: 0.5500\n",
      "Epoch 4/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0086 - acc: 0.60 - 0s 349us/step - loss: -0.0103 - acc: 0.5500\n",
      "Epoch 5/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0074 - acc: 0.50 - 0s 274us/step - loss: -0.0108 - acc: 0.5500\n",
      "Epoch 6/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0156 - acc: 0.45 - 0s 125us/step - loss: -0.0111 - acc: 0.5500\n",
      "Epoch 7/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0075 - acc: 0.60 - 0s 149us/step - loss: -0.0107 - acc: 0.5500\n",
      "Epoch 8/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0188 - acc: 0.45 - 0s 299us/step - loss: -0.0138 - acc: 0.5500\n",
      "Epoch 9/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -9.6864e-05 - acc: 0.75 - 0s 324us/step - loss: -0.0106 - acc: 0.5500\n",
      "Epoch 10/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0182 - acc: 0.55 - 0s 150us/step - loss: -0.0148 - acc: 0.5500\n",
      "Epoch 11/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0142 - acc: 0.45 - 0s 150us/step - loss: -0.0133 - acc: 0.5500\n",
      "Epoch 12/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0167 - acc: 0.50 - 0s 150us/step - loss: -0.0158 - acc: 0.5500\n",
      "Epoch 13/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0081 - acc: 0.55 - 0s 150us/step - loss: -0.0183 - acc: 0.5500\n",
      "Epoch 14/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0053 - acc: 0.60 - 0s 100us/step - loss: -0.0174 - acc: 0.5500\n",
      "Epoch 15/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0178 - acc: 0.55 - 0s 199us/step - loss: -0.0187 - acc: 0.5500\n",
      "Epoch 16/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0234 - acc: 0.60 - 0s 224us/step - loss: -0.0191 - acc: 0.5500\n",
      "Epoch 17/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0282 - acc: 0.55 - 0s 274us/step - loss: -0.0213 - acc: 0.5500\n",
      "Epoch 18/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0157 - acc: 0.50 - 0s 175us/step - loss: -0.0208 - acc: 0.5500\n",
      "Epoch 19/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0178 - acc: 0.60 - 0s 276us/step - loss: -0.0239 - acc: 0.5500\n",
      "Epoch 20/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0209 - acc: 0.50 - 0s 125us/step - loss: -0.0272 - acc: 0.5500\n",
      "Epoch 21/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0090 - acc: 0.60 - 0s 199us/step - loss: -0.0289 - acc: 0.5500\n",
      "Epoch 22/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0633 - acc: 0.50 - 0s 150us/step - loss: -0.0347 - acc: 0.5500\n",
      "Epoch 23/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0079 - acc: 0.65 - 0s 200us/step - loss: -0.0342 - acc: 0.5500\n",
      "Epoch 24/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0137 - acc: 0.60 - 0s 224us/step - loss: -0.0324 - acc: 0.5500\n",
      "Epoch 25/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0372 - acc: 0.60 - 0s 374us/step - loss: -0.0311 - acc: 0.5500\n",
      "Epoch 26/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0237 - acc: 0.55 - 0s 175us/step - loss: -0.0355 - acc: 0.5500\n",
      "Epoch 27/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0426 - acc: 0.65 - 0s 274us/step - loss: -0.0380 - acc: 0.5500\n",
      "Epoch 28/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0439 - acc: 0.50 - 0s 249us/step - loss: -0.0382 - acc: 0.5500\n",
      "Epoch 29/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0279 - acc: 0.55 - 0s 150us/step - loss: -0.0422 - acc: 0.5500\n",
      "Epoch 30/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0388 - acc: 0.50 - 0s 150us/step - loss: -0.0463 - acc: 0.5500\n",
      "Epoch 31/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0534 - acc: 0.45 - 0s 299us/step - loss: -0.0475 - acc: 0.5500\n",
      "Epoch 32/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0247 - acc: 0.60 - 0s 249us/step - loss: -0.0486 - acc: 0.5500\n",
      "Epoch 33/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0398 - acc: 0.50 - 0s 150us/step - loss: -0.0509 - acc: 0.5500\n",
      "Epoch 34/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0632 - acc: 0.55 - 0s 199us/step - loss: -0.0551 - acc: 0.5500\n",
      "Epoch 35/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0261 - acc: 0.45 - 0s 199us/step - loss: -0.0669 - acc: 0.5500\n",
      "Epoch 36/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0374 - acc: 0.60 - 0s 349us/step - loss: -0.0635 - acc: 0.5500\n",
      "Epoch 37/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0716 - acc: 0.60 - 0s 225us/step - loss: -0.0648 - acc: 0.5500\n",
      "Epoch 38/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.0905 - acc: 0.50 - 0s 125us/step - loss: -0.0662 - acc: 0.5500\n",
      "Epoch 39/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.1209 - acc: 0.65 - 0s 150us/step - loss: -0.0839 - acc: 0.5500\n",
      "Epoch 40/40\n",
      "40/40 [==============================] - ETA: 0s - loss: -0.1034 - acc: 0.40 - 0s 150us/step - loss: -0.0633 - acc: 0.5500\n",
      "60/60 [==============================] - ETA:  - 0s 33us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_iris_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 170\n",
    "# }\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #15\n",
    "\n",
    "#### Adult salary LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "Epoch 1/100\n",
      "13024/13024 [==============================] - 0s 0us/step - loss: 1.7349e-05 - acc: 0.7631\n",
      "Epoch 2/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.7289e-05 - acc: 0.7631\n",
      "Epoch 3/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.7224e-05 - acc: 0.7631\n",
      "Epoch 4/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.7157e-05 - acc: 0.7631\n",
      "Epoch 5/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.7088e-05 - acc: 0.7631\n",
      "Epoch 6/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.7018e-05 - acc: 0.7631\n",
      "Epoch 7/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6947e-05 - acc: 0.7631\n",
      "Epoch 8/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6875e-05 - acc: 0.7631\n",
      "Epoch 9/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6803e-05 - acc: 0.7631\n",
      "Epoch 10/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6731e-05 - acc: 0.7631\n",
      "Epoch 11/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6658e-05 - acc: 0.7631\n",
      "Epoch 12/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6585e-05 - acc: 0.7631\n",
      "Epoch 13/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6512e-05 - acc: 0.7631\n",
      "Epoch 14/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6439e-05 - acc: 0.7631\n",
      "Epoch 15/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6366e-05 - acc: 0.7631\n",
      "Epoch 16/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6293e-05 - acc: 0.7631\n",
      "Epoch 17/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6220e-05 - acc: 0.7631\n",
      "Epoch 18/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6147e-05 - acc: 0.7631\n",
      "Epoch 19/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6074e-05 - acc: 0.7631\n",
      "Epoch 20/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.6001e-05 - acc: 0.7631\n",
      "Epoch 21/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5928e-05 - acc: 0.7631\n",
      "Epoch 22/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5855e-05 - acc: 0.7631\n",
      "Epoch 23/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5783e-05 - acc: 0.7631\n",
      "Epoch 24/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5710e-05 - acc: 0.7631\n",
      "Epoch 25/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5638e-05 - acc: 0.7631\n",
      "Epoch 26/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5565e-05 - acc: 0.7631\n",
      "Epoch 27/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5493e-05 - acc: 0.7631\n",
      "Epoch 28/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5421e-05 - acc: 0.7631\n",
      "Epoch 29/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5350e-05 - acc: 0.7631\n",
      "Epoch 30/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5278e-05 - acc: 0.7631\n",
      "Epoch 31/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5207e-05 - acc: 0.7631\n",
      "Epoch 32/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5136e-05 - acc: 0.7631\n",
      "Epoch 33/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.5065e-05 - acc: 0.7631\n",
      "Epoch 34/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4994e-05 - acc: 0.7631\n",
      "Epoch 35/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4923e-05 - acc: 0.7631\n",
      "Epoch 36/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4853e-05 - acc: 0.7631\n",
      "Epoch 37/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4783e-05 - acc: 0.7631\n",
      "Epoch 38/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4713e-05 - acc: 0.7631\n",
      "Epoch 39/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4643e-05 - acc: 0.7631\n",
      "Epoch 40/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4573e-05 - acc: 0.7631\n",
      "Epoch 41/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4504e-05 - acc: 0.7631\n",
      "Epoch 42/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4435e-05 - acc: 0.7631\n",
      "Epoch 43/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4366e-05 - acc: 0.7631\n",
      "Epoch 44/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4298e-05 - acc: 0.7631\n",
      "Epoch 45/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4229e-05 - acc: 0.7631\n",
      "Epoch 46/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4161e-05 - acc: 0.7631\n",
      "Epoch 47/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4093e-05 - acc: 0.7631\n",
      "Epoch 48/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.4026e-05 - acc: 0.7631\n",
      "Epoch 49/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3958e-05 - acc: 0.7631\n",
      "Epoch 50/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3891e-05 - acc: 0.7631\n",
      "Epoch 51/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3824e-05 - acc: 0.7631\n",
      "Epoch 52/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3758e-05 - acc: 0.7631\n",
      "Epoch 53/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3691e-05 - acc: 0.7631\n",
      "Epoch 54/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3625e-05 - acc: 0.7631\n",
      "Epoch 55/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3559e-05 - acc: 0.7631\n",
      "Epoch 56/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3493e-05 - acc: 0.7631\n",
      "Epoch 57/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3428e-05 - acc: 0.7631\n",
      "Epoch 58/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3363e-05 - acc: 0.7631\n",
      "Epoch 59/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3298e-05 - acc: 0.7631\n",
      "Epoch 60/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3233e-05 - acc: 0.7631\n",
      "Epoch 61/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3169e-05 - acc: 0.7631\n",
      "Epoch 62/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3104e-05 - acc: 0.7631\n",
      "Epoch 63/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.3040e-05 - acc: 0.7631\n",
      "Epoch 64/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2977e-05 - acc: 0.7631\n",
      "Epoch 65/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2913e-05 - acc: 0.7631\n",
      "Epoch 66/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2850e-05 - acc: 0.7631\n",
      "Epoch 67/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2787e-05 - acc: 0.7631\n",
      "Epoch 68/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2724e-05 - acc: 0.7631\n",
      "Epoch 69/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2662e-05 - acc: 0.7631\n",
      "Epoch 70/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2599e-05 - acc: 0.7631\n",
      "Epoch 71/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2537e-05 - acc: 0.7631\n",
      "Epoch 72/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2476e-05 - acc: 0.7631\n",
      "Epoch 73/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2414e-05 - acc: 0.7631\n",
      "Epoch 74/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2353e-05 - acc: 0.7631\n",
      "Epoch 75/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2292e-05 - acc: 0.7631\n",
      "Epoch 76/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2231e-05 - acc: 0.7631\n",
      "Epoch 77/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2171e-05 - acc: 0.7631\n",
      "Epoch 78/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2110e-05 - acc: 0.7631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.2050e-05 - acc: 0.7631\n",
      "Epoch 80/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1990e-05 - acc: 0.7631\n",
      "Epoch 81/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1931e-05 - acc: 0.7631\n",
      "Epoch 82/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1872e-05 - acc: 0.7631\n",
      "Epoch 83/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1813e-05 - acc: 0.7631\n",
      "Epoch 84/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1754e-05 - acc: 0.7631\n",
      "Epoch 85/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1695e-05 - acc: 0.7631\n",
      "Epoch 86/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1637e-05 - acc: 0.7631\n",
      "Epoch 87/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1579e-05 - acc: 0.7631\n",
      "Epoch 88/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1521e-05 - acc: 0.7631\n",
      "Epoch 89/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1463e-05 - acc: 0.7631\n",
      "Epoch 90/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1406e-05 - acc: 0.7631\n",
      "Epoch 91/100\n",
      "13024/13024 [==============================] - 0s 0us/step - loss: 1.1349e-05 - acc: 0.7631\n",
      "Epoch 92/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1292e-05 - acc: 0.7631\n",
      "Epoch 93/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1235e-05 - acc: 0.7631\n",
      "Epoch 94/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1178e-05 - acc: 0.7631\n",
      "Epoch 95/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1122e-05 - acc: 0.7631\n",
      "Epoch 96/100\n",
      "13024/13024 [==============================] - 0s 0us/step - loss: 1.1066e-05 - acc: 0.7631\n",
      "Epoch 97/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.1010e-05 - acc: 0.7631\n",
      "Epoch 98/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.0955e-05 - acc: 0.7631\n",
      "Epoch 99/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.0900e-05 - acc: 0.7631\n",
      "Epoch 100/100\n",
      "13024/13024 [==============================] - 0s 1us/step - loss: 1.0845e-05 - acc: 0.7631\n",
      "   32/19537 [..............................] - ETA: 0s"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs\nApply node that caused the error: Eigvalsh{lower=True}(Elemwise{Composite{((i0 / i1) - i2)}}[(0, 0)].0, Elemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)].0)\nToposort index: 43\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[nan]], dtype=float32), array([[nan]], dtype=float32)]\nOutputs clients: [[Subtensor{int64::}(Eigvalsh{lower=True}.0, Constant{-1})]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"<ipython-input-1-c3c07585f780>\", line 41, in <module>\n    automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60, params=params)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\automation_script.py\", line 153, in run_imly\n    m.fit(x_train, y_train.values.ravel())\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\wrappers\\sklearn\\keras_classifier.py\", line 50, in fit\n    self.model = mapping_instance.__call__(x_train=x_train)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\architectures\\sklearn\\model.py\", line 24, in __call__\n    x_train=kwargs['x_train'])\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\architectures\\sklearn\\model.py\", line 82, in lda\n    metrics=['accuracy'])\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 404, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\utils\\losses.py\", line 58, in inner_lda_objective\n    evals_t = T.slinalg.eigvalsh(Sb_t, Sw_t)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\tensor\\slinalg.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, outputs)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvalsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigvalsh\u001b[1;34m(a, b, lower, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    860\u001b[0m                 \u001b[0mturbo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mturbo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meigvals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meigvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 check_finite=check_finite)\n\u001b[0m\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigh\u001b[1;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m--> 374\u001b[1;33m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\_lib\\_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[1;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    460\u001b[0m         raise ValueError(\n\u001b[1;32m--> 461\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    462\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c3c07585f780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mautomation_script\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_imly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'linear_discrimant_analysis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\automation_script.py\u001b[0m in \u001b[0;36mrun_imly\u001b[1;34m(dataset_info, model_name, X, Y, test_size, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     \u001b[0mkeras_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# Create plot and write to s3 bucket #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1111\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m                                          steps=steps)\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m     def predict(self, x,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    918\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\tensor\\slinalg.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, outputs)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvalsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvalsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigvalsh\u001b[1;34m(a, b, lower, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[0moverwrite_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverwrite_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m                 \u001b[0mturbo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mturbo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meigvals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meigvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 check_finite=check_finite)\n\u001b[0m\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigh\u001b[1;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m--> 374\u001b[1;33m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected square matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\_lib\\_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[1;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'masked arrays are not supported'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'O'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AllFloat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         raise ValueError(\n\u001b[1;32m--> 461\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    462\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs\nApply node that caused the error: Eigvalsh{lower=True}(Elemwise{Composite{((i0 / i1) - i2)}}[(0, 0)].0, Elemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)].0)\nToposort index: 43\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[nan]], dtype=float32), array([[nan]], dtype=float32)]\nOutputs clients: [[Subtensor{int64::}(Eigvalsh{lower=True}.0, Constant{-1})]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"<ipython-input-1-c3c07585f780>\", line 41, in <module>\n    automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60, params=params)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\automation_script.py\", line 153, in run_imly\n    m.fit(x_train, y_train.values.ravel())\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\wrappers\\sklearn\\keras_classifier.py\", line 50, in fit\n    self.model = mapping_instance.__call__(x_train=x_train)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\architectures\\sklearn\\model.py\", line 24, in __call__\n    x_train=kwargs['x_train'])\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\architectures\\sklearn\\model.py\", line 82, in lda\n    metrics=['accuracy'])\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 404, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\utils\\losses.py\", line 58, in inner_lda_objective\n    evals_t = T.slinalg.eigvalsh(Sb_t, Sw_t)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "\n",
    "dataset_name = \"uci_adult_salary_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "url = \"../data/adult.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names)\n",
    "\n",
    "\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "# Y = column_or_1d(Y, warn=True)\n",
    "\n",
    "params = {\n",
    "    'batch_size': 1000,\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               False\n",
       "workclass         False\n",
       "fnlwgt            False\n",
       "education         False\n",
       "education-num     False\n",
       "marital-status    False\n",
       "occupation        False\n",
       "relationship      False\n",
       "race              False\n",
       "sex               False\n",
       "capital-gain      False\n",
       "capital-loss      False\n",
       "hours-per-week    False\n",
       "native-country    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isinf(X).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #16\n",
    "\n",
    "#### Abalone LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "Epoch 1/100\n",
      "529/529 [==============================] - 1s 1ms/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 2/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 3/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 4/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 5/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 6/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 7/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 8/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 9/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 10/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 11/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 12/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 13/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 14/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 15/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 16/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 17/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 18/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 19/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 20/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 21/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 22/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 23/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 24/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 25/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 26/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 27/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 28/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 29/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 30/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 31/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 32/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 33/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 34/100\n",
      "529/529 [==============================] - 0s 15us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 35/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 36/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 37/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 38/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 39/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 40/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 41/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 42/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 43/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 44/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 45/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 46/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 47/100\n",
      "529/529 [==============================] - 0s 17us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 48/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 49/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 50/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 51/100\n",
      "529/529 [==============================] - 0s 15us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 52/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 53/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 54/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 55/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0041 - acc: 0.4726\n",
      "Epoch 56/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0041 - acc: 0.4726\n",
      "Epoch 57/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0041 - acc: 0.4726\n",
      "Epoch 58/100\n",
      "529/529 [==============================] - 0s 15us/step - loss: 0.0040 - acc: 0.4726\n",
      "Epoch 59/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0040 - acc: 0.4726\n",
      "Epoch 60/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0040 - acc: 0.4726\n",
      "Epoch 61/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 62/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 63/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 64/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 65/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0038 - acc: 0.4726\n",
      "Epoch 66/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0038 - acc: 0.4726\n",
      "Epoch 67/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0037 - acc: 0.4726\n",
      "Epoch 68/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0037 - acc: 0.4726\n",
      "Epoch 69/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0037 - acc: 0.4726\n",
      "Epoch 70/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0036 - acc: 0.4726\n",
      "Epoch 71/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0036 - acc: 0.4726\n",
      "Epoch 72/100\n",
      "529/529 [==============================] - 0s 17us/step - loss: 0.0036 - acc: 0.4726\n",
      "Epoch 73/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0035 - acc: 0.4726\n",
      "Epoch 74/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0035 - acc: 0.4726\n",
      "Epoch 75/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0034 - acc: 0.4726\n",
      "Epoch 76/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0034 - acc: 0.4726\n",
      "Epoch 77/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0034 - acc: 0.4726\n",
      "Epoch 78/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0033 - acc: 0.4726\n",
      "Epoch 79/100\n",
      "529/529 [==============================] - 0s 17us/step - loss: 0.0033 - acc: 0.4726\n",
      "Epoch 80/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0032 - acc: 0.4726\n",
      "Epoch 81/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0032 - acc: 0.4726\n",
      "Epoch 82/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0031 - acc: 0.4726\n",
      "Epoch 83/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0031 - acc: 0.4726\n",
      "Epoch 84/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0030 - acc: 0.4726\n",
      "Epoch 85/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0030 - acc: 0.4726\n",
      "Epoch 86/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0030 - acc: 0.4726\n",
      "Epoch 87/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0029 - acc: 0.4726\n",
      "Epoch 88/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0029 - acc: 0.4726\n",
      "Epoch 89/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0028 - acc: 0.4726\n",
      "Epoch 90/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0027 - acc: 0.4726\n",
      "Epoch 91/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0027 - acc: 0.4726\n",
      "Epoch 92/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0026 - acc: 0.4726\n",
      "Epoch 93/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0026 - acc: 0.4726\n",
      "Epoch 94/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0025 - acc: 0.4726\n",
      "Epoch 95/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0025 - acc: 0.4726\n",
      "Epoch 96/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0024 - acc: 0.4726\n",
      "Epoch 97/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0024 - acc: 0.4726\n",
      "Epoch 98/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0023 - acc: 0.4726\n",
      "Epoch 99/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0022 - acc: 0.4726\n",
      "Epoch 100/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0022 - acc: 0.4726\n",
      "794/794 [==============================] - ETA:  - 0s 14us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"uci_abalone_lda\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #17\n",
    "\n",
    "#### Mushroom LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields with missing values\n",
      "stalk-root\n",
      "2480\n",
      "30.53%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0564 - acc: 0.5432\n",
      "Epoch 2/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0572 - acc: 0.5445\n",
      "Epoch 3/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0581 - acc: 0.5467\n",
      "Epoch 4/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0590 - acc: 0.5476\n",
      "Epoch 5/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0599 - acc: 0.5498\n",
      "Epoch 6/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0608 - acc: 0.5516\n",
      "Epoch 7/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0617 - acc: 0.5516\n",
      "Epoch 8/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0627 - acc: 0.5521\n",
      "Epoch 9/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0636 - acc: 0.5529\n",
      "Epoch 10/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0646 - acc: 0.5525\n",
      "Epoch 11/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0656 - acc: 0.5543\n",
      "Epoch 12/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0666 - acc: 0.5552\n",
      "Epoch 13/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0676 - acc: 0.5556\n",
      "Epoch 14/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0687 - acc: 0.5565\n",
      "Epoch 15/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0697 - acc: 0.5569\n",
      "Epoch 16/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0708 - acc: 0.5583\n",
      "Epoch 17/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0719 - acc: 0.5583\n",
      "Epoch 18/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0730 - acc: 0.5591\n",
      "Epoch 19/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0741 - acc: 0.5605\n",
      "Epoch 20/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0752 - acc: 0.5614\n",
      "Epoch 21/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.0764 - acc: 0.5631\n",
      "Epoch 22/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0775 - acc: 0.5662\n",
      "Epoch 23/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0787 - acc: 0.5667\n",
      "Epoch 24/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0800 - acc: 0.5702\n",
      "Epoch 25/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0812 - acc: 0.5698\n",
      "Epoch 26/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.0825 - acc: 0.5711\n",
      "Epoch 27/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0837 - acc: 0.5742\n",
      "Epoch 28/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0850 - acc: 0.5742\n",
      "Epoch 29/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0864 - acc: 0.5755\n",
      "Epoch 30/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0877 - acc: 0.5778\n",
      "Epoch 31/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0891 - acc: 0.5778\n",
      "Epoch 32/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.0904 - acc: 0.5804\n",
      "Epoch 33/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0918 - acc: 0.5813\n",
      "Epoch 34/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0933 - acc: 0.5813\n",
      "Epoch 35/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0947 - acc: 0.5826\n",
      "Epoch 36/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0962 - acc: 0.5840\n",
      "Epoch 37/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0977 - acc: 0.5857\n",
      "Epoch 38/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0992 - acc: 0.5866\n",
      "Epoch 39/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1007 - acc: 0.5888\n",
      "Epoch 40/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1022 - acc: 0.5911\n",
      "Epoch 41/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1038 - acc: 0.5919\n",
      "Epoch 42/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.1054 - acc: 0.5942\n",
      "Epoch 43/100\n",
      "2257/2257 [==============================] - 0s 6us/step - loss: -0.1070 - acc: 0.5937\n",
      "Epoch 44/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1086 - acc: 0.5942\n",
      "Epoch 45/100\n",
      "2257/2257 [==============================] - 0s 6us/step - loss: -0.1103 - acc: 0.5955\n",
      "Epoch 46/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1119 - acc: 0.5959\n",
      "Epoch 47/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1136 - acc: 0.5981\n",
      "Epoch 48/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.1153 - acc: 0.5964\n",
      "Epoch 49/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.1170 - acc: 0.5977\n",
      "Epoch 50/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1187 - acc: 0.5995\n",
      "Epoch 51/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1205 - acc: 0.6008\n",
      "Epoch 52/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1223 - acc: 0.5999\n",
      "Epoch 53/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1241 - acc: 0.5990\n",
      "Epoch 54/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1259 - acc: 0.6008\n",
      "Epoch 55/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1277 - acc: 0.6008\n",
      "Epoch 56/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1295 - acc: 0.6008\n",
      "Epoch 57/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1314 - acc: 0.6026\n",
      "Epoch 58/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1333 - acc: 0.6043\n",
      "Epoch 59/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1352 - acc: 0.6061\n",
      "Epoch 60/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1371 - acc: 0.6061\n",
      "Epoch 61/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1390 - acc: 0.6079\n",
      "Epoch 62/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1409 - acc: 0.6079\n",
      "Epoch 63/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1429 - acc: 0.6088\n",
      "Epoch 64/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1449 - acc: 0.6097\n",
      "Epoch 65/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.1469 - acc: 0.6097\n",
      "Epoch 66/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1489 - acc: 0.6114\n",
      "Epoch 67/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1509 - acc: 0.6123\n",
      "Epoch 68/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1529 - acc: 0.6128\n",
      "Epoch 69/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1549 - acc: 0.6128\n",
      "Epoch 70/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1570 - acc: 0.6145\n",
      "Epoch 71/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1591 - acc: 0.6154\n",
      "Epoch 72/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1611 - acc: 0.6154\n",
      "Epoch 73/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1632 - acc: 0.6167\n",
      "Epoch 74/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1653 - acc: 0.6167\n",
      "Epoch 75/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1674 - acc: 0.6176\n",
      "Epoch 76/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1696 - acc: 0.6185\n",
      "Epoch 77/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1717 - acc: 0.6194\n",
      "Epoch 78/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.1738 - acc: 0.6207\n",
      "Epoch 79/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1760 - acc: 0.6221\n",
      "Epoch 80/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1781 - acc: 0.6234\n",
      "Epoch 81/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1803 - acc: 0.6238\n",
      "Epoch 82/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1825 - acc: 0.6243\n",
      "Epoch 83/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1847 - acc: 0.6256\n",
      "Epoch 84/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1868 - acc: 0.6269\n",
      "Epoch 85/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1890 - acc: 0.6269\n",
      "Epoch 86/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1912 - acc: 0.6287\n",
      "Epoch 87/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1934 - acc: 0.6296\n",
      "Epoch 88/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1957 - acc: 0.6296\n",
      "Epoch 89/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1979 - acc: 0.6305\n",
      "Epoch 90/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.2001 - acc: 0.6318\n",
      "Epoch 91/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.2023 - acc: 0.6323\n",
      "Epoch 92/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.2046 - acc: 0.6331\n",
      "Epoch 93/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2068 - acc: 0.6336\n",
      "Epoch 94/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2090 - acc: 0.6336\n",
      "Epoch 95/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2113 - acc: 0.6340\n",
      "Epoch 96/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.2135 - acc: 0.6336\n",
      "Epoch 97/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2158 - acc: 0.6349\n",
      "Epoch 98/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2180 - acc: 0.6358\n",
      "Epoch 99/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.2202 - acc: 0.6380\n",
      "Epoch 100/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.2225 - acc: 0.6389\n",
      "3387/3387 [==============================] - ETA:  - 0s 13us/step\n"
     ]
    }
   ],
   "source": [
    "# Load dataset info #\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_mushroom_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "names = ['classes', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n",
    "        'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "        'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "        'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color',\n",
    "        'population', 'habitat']\n",
    "url = \"../data/mushroom.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "print(\"Fields with missing values\")\n",
    "col_names = data.columns\n",
    "num_data = data.shape[0]\n",
    "for c in col_names:\n",
    "    num_non = data[c].isin([\"?\"]).sum()\n",
    "    if num_non > 0:\n",
    "        print (c)\n",
    "        print (num_non)\n",
    "        print (\"{0:.2f}%\".format(float(num_non) / num_data * 100))\n",
    "        print (\"\\n\")\n",
    "\n",
    "data = data[data[\"stalk-root\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "\n",
    "for col in names:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "# Split the dataset into test and train datasets #\n",
    "feature_list = names[1:23]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['classes']]\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #18\n",
    "\n",
    "#### Ad dataset LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.1441 - acc: 0.50 - 0s 45us/step - loss: -0.0478 - acc: 0.5334\n",
      "Epoch 2/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.1230 - acc: 0.59 - 0s 48us/step - loss: -0.3141 - acc: 0.5567\n",
      "Epoch 3/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.2547 - acc: 0.59 - 0s 50us/step - loss: -0.5058 - acc: 0.5610\n",
      "Epoch 4/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.4350 - acc: 0.53 - 0s 48us/step - loss: -0.6790 - acc: 0.5546\n",
      "Epoch 5/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.7477 - acc: 0.59 - 0s 45us/step - loss: -0.8734 - acc: 0.5610\n",
      "Epoch 6/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.9564 - acc: 0.59 - 0s 44us/step - loss: -1.0366 - acc: 0.5726\n",
      "Epoch 7/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.0470 - acc: 0.58 - 0s 55us/step - loss: -1.1899 - acc: 0.6182\n",
      "Epoch 8/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.0445 - acc: 0.58 - 0s 42us/step - loss: -1.3710 - acc: 0.6713\n",
      "Epoch 9/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.6196 - acc: 0.74 - 0s 56us/step - loss: -1.5998 - acc: 0.7116\n",
      "Epoch 10/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.8422 - acc: 0.80 - 0s 42us/step - loss: -1.8681 - acc: 0.7635\n",
      "Epoch 11/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.9889 - acc: 0.77 - 0s 48us/step - loss: -2.4451 - acc: 0.8176\n",
      "Epoch 12/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -3.3058 - acc: 0.81 - 0s 51us/step - loss: -3.0846 - acc: 0.8664\n",
      "Epoch 13/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -5.5495 - acc: 0.93 - 0s 49us/step - loss: -3.9062 - acc: 0.9279\n",
      "Epoch 14/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -3.1521 - acc: 0.94 - 0s 55us/step - loss: -4.7984 - acc: 0.9671\n",
      "Epoch 15/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -6.7583 - acc: 0.98 - 0s 47us/step - loss: -5.2037 - acc: 0.9799\n",
      "Epoch 16/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -5.1262 - acc: 0.98 - 0s 49us/step - loss: -6.2920 - acc: 0.9745\n",
      "Epoch 17/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -4.3709 - acc: 0.97 - 0s 48us/step - loss: -7.4895 - acc: 0.9820\n",
      "Epoch 18/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -4.5437 - acc: 0.98 - 0s 51us/step - loss: -7.1479 - acc: 0.9830\n",
      "Epoch 19/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.7565 - acc: 0.99 - 0s 51us/step - loss: -7.4532 - acc: 0.9883\n",
      "Epoch 20/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -5.5416 - acc: 0.99 - 0s 43us/step - loss: -10.9768 - acc: 0.9873\n",
      "Epoch 21/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -32.6865 - acc: 1.000 - 0s 47us/step - loss: -9.4512 - acc: 0.9873\n",
      "Epoch 22/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -8.2866 - acc: 0.99 - 0s 43us/step - loss: -6.3921 - acc: 0.9852\n",
      "Epoch 23/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -10.0966 - acc: 0.990 - 0s 49us/step - loss: -17.1462 - acc: 0.9862\n",
      "Epoch 24/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -10.6942 - acc: 0.990 - 0s 42us/step - loss: -9.3739 - acc: 0.9883\n",
      "Epoch 25/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -32.7998 - acc: 0.990 - 0s 45us/step - loss: -9.0847 - acc: 0.9873\n",
      "Epoch 26/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -5.6524 - acc: 0.99 - 0s 49us/step - loss: -14.3058 - acc: 0.9862\n",
      "Epoch 27/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -15.8641 - acc: 1.000 - 0s 45us/step - loss: -14.7565 - acc: 0.9873\n",
      "Epoch 28/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.9645 - acc: 0.96 - 0s 43us/step - loss: -14.5745 - acc: 0.9883\n",
      "Epoch 29/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -11.1780 - acc: 0.980 - 0s 51us/step - loss: -17.1727 - acc: 0.9862\n",
      "Epoch 30/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.8385 - acc: 0.97 - 0s 52us/step - loss: -12.3677 - acc: 0.9873\n",
      "Epoch 31/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -3.8501 - acc: 0.98 - 0s 48us/step - loss: -9.5774 - acc: 0.9883\n",
      "Epoch 32/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -32.5214 - acc: 1.000 - 0s 47us/step - loss: -15.9126 - acc: 0.9883\n",
      "Epoch 33/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -3.1477 - acc: 0.99 - 0s 50us/step - loss: -13.4767 - acc: 0.9830\n",
      "Epoch 34/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -3.7216 - acc: 0.98 - 0s 48us/step - loss: -28.5051 - acc: 0.9820\n",
      "Epoch 35/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -4.0484 - acc: 0.98 - 0s 44us/step - loss: -31.3113 - acc: 0.9830\n",
      "Epoch 36/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.5977 - acc: 0.98 - 0s 41us/step - loss: -21.8777 - acc: 0.9830\n",
      "Epoch 37/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -2.1772 - acc: 0.97 - 0s 47us/step - loss: -4.7645 - acc: 0.9830\n",
      "Epoch 38/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.4095 - acc: 0.98 - 0s 41us/step - loss: -45.8703 - acc: 0.9830\n",
      "Epoch 39/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.9249 - acc: 0.97 - 0s 40us/step - loss: -52.3314 - acc: 0.9799\n",
      "Epoch 40/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -3.5130 - acc: 0.98 - 0s 43us/step - loss: -4.4621 - acc: 0.9788\n",
      "Epoch 41/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.9015 - acc: 0.98 - 0s 41us/step - loss: -20.0717 - acc: 0.9799\n",
      "Epoch 42/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -4.0235 - acc: 0.99 - 0s 47us/step - loss: -28.9606 - acc: 0.9799\n",
      "Epoch 43/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -2.3820 - acc: 0.97 - 0s 48us/step - loss: -30.0203 - acc: 0.9799\n",
      "Epoch 44/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -5.8755 - acc: 0.99 - 0s 55us/step - loss: -10.1090 - acc: 0.9820\n",
      "Epoch 45/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -152.8481 - acc: 1.00 - 0s 50us/step - loss: -38.5160 - acc: 0.9820\n",
      "Epoch 46/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -0.9084 - acc: 0.99 - 0s 48us/step - loss: -38.6074 - acc: 0.9820\n",
      "Epoch 47/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -1.4041 - acc: 0.98 - 0s 48us/step - loss: -8.6671 - acc: 0.9820\n",
      "Epoch 48/500\n",
      "943/943 [==============================] - ETA: 0s - loss: -2.0921 - acc: 0.99 - 0s 55us/step - loss: -36.8464 - acc: 0.9799\n",
      "Epoch 49/500\n",
      "100/943 [==>...........................] - ETA: 0s - loss: -2.6252 - acc: 0.9800"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs\nApply node that caused the error: Eigvalsh{lower=True}(Elemwise{Composite{((i0 / i1) - i2)}}[(0, 0)].0, Elemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)].0)\nToposort index: 80\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[nan]], dtype=float32), array([[nan]], dtype=float32)]\nOutputs clients: [[Subtensor{int64::}(Eigvalsh{lower=True}.0, Constant{-1})]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"<ipython-input-1-adec3f214c98>\", line 33, in <module>\n    automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\automation_script.py\", line 153, in run_imly\n    m.fit(x_train, y_train.values.ravel())\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\wrappers\\sklearn\\keras_classifier.py\", line 50, in fit\n    self.model = mapping_instance.__call__(x_train=x_train)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\architectures\\sklearn\\model.py\", line 24, in __call__\n    x_train=kwargs['x_train'])\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\architectures\\sklearn\\model.py\", line 82, in lda\n    metrics=['accuracy'])\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 404, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\utils\\losses.py\", line 58, in inner_lda_objective\n    evals_t = T.slinalg.eigvalsh(Sb_t, Sw_t)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\tensor\\slinalg.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, outputs)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvalsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigvalsh\u001b[1;34m(a, b, lower, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    860\u001b[0m                 \u001b[0mturbo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mturbo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meigvals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meigvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 check_finite=check_finite)\n\u001b[0m\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigh\u001b[1;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m--> 374\u001b[1;33m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\_lib\\_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[1;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    460\u001b[0m         raise ValueError(\n\u001b[1;32m--> 461\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    462\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-adec3f214c98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mautomation_script\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_imly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'linear_discrimant_analysis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\automation_script.py\u001b[0m in \u001b[0;36mrun_imly\u001b[1;34m(dataset_info, model_name, X, Y, test_size, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m  \u001b[1;31m# Talos accepts only numpy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[0mkeras_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\wrappers\\sklearn\\keras_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mmapping_instance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmapping_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mfinal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    918\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\tensor\\slinalg.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, outputs)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvalsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvalsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigvalsh\u001b[1;34m(a, b, lower, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[0moverwrite_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverwrite_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m                 \u001b[0mturbo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mturbo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meigvals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meigvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 check_finite=check_finite)\n\u001b[0m\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigh\u001b[1;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m--> 374\u001b[1;33m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected square matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\_lib\\_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[1;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'masked arrays are not supported'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'O'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AllFloat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         raise ValueError(\n\u001b[1;32m--> 461\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    462\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs\nApply node that caused the error: Eigvalsh{lower=True}(Elemwise{Composite{((i0 / i1) - i2)}}[(0, 0)].0, Elemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)].0)\nToposort index: 80\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[nan]], dtype=float32), array([[nan]], dtype=float32)]\nOutputs clients: [[Subtensor{int64::}(Eigvalsh{lower=True}.0, Constant{-1})]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"<ipython-input-1-adec3f214c98>\", line 33, in <module>\n    automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\automation_script.py\", line 153, in run_imly\n    m.fit(x_train, y_train.values.ravel())\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\wrappers\\sklearn\\keras_classifier.py\", line 50, in fit\n    self.model = mapping_instance.__call__(x_train=x_train)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\architectures\\sklearn\\model.py\", line 24, in __call__\n    x_train=kwargs['x_train'])\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\architectures\\sklearn\\model.py\", line 82, in lda\n    metrics=['accuracy'])\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 404, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\utils\\losses.py\", line 58, in inner_lda_objective\n    evals_t = T.slinalg.eigvalsh(Sb_t, Sw_t)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_name = \"uci_ad_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/ad.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "data = data.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Label encoding #\n",
    "\n",
    "lb = LabelEncoder()\n",
    "Y = lb.fit_transform(data.iloc[:, -1])\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test bed ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hyperas(Logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils.validation import column_or_1d\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import copy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import datasets\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from automation_script import get_dataset_info\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.datasets import make_regression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano.tensor as T\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from theano.compile.ops import as_op\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [10, 30]),\n",
      "        'epochs': hp.choice('epochs', [100, 170]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: Make sure to have every relevant import statement included here and return data as\n",
      "  6: used in model function below. This function is separated from model() so that hyperopt\n",
      "  7: won't reload data for each evaluation run.\n",
      "  8: '''\n",
      "  9: url = \"../data/iris.csv\"\n",
      " 10: data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
      " 11: class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
      " 12: data.iloc[:,-1] = index\n",
      " 13: data = data.loc[data[4] != 2]\n",
      " 14: X = data.iloc[:,:-1]\n",
      " 15: Y = data.iloc[:,-1]\n",
      " 16: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
      " 17: \n",
      " 18: \n",
      " 19: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13: \n",
      "  14:     model = Sequential()\n",
      "  15:     model.add(Dense(1, input_dim=4, activation='sigmoid'))\n",
      "  16: \n",
      "  17:     model.compile(loss='binary_crossentropy', optimizer=space['optimizer'],\n",
      "  18:                  metrics=['accuracy'])\n",
      "  19: \n",
      "  20:     model.fit(x_train, y_train,\n",
      "  21:               batch_size=space['batch_size'],\n",
      "  22:               epochs=space['epochs'],\n",
      "  23:               verbose=2,\n",
      "  24:               validation_data=(x_test, y_test))\n",
      "  25:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  26:     print('Test accuracy:', acc)\n",
      "  27:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  28: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 0.8220 - acc: 0.5500 - val_loss: 0.8823 - val_acc: 0.4667\n",
      "Epoch 2/170\n",
      " - 0s - loss: 0.8120 - acc: 0.5500 - val_loss: 0.8704 - val_acc: 0.4667\n",
      "Epoch 3/170\n",
      " - 0s - loss: 0.8036 - acc: 0.5500 - val_loss: 0.8589 - val_acc: 0.4667\n",
      "Epoch 4/170\n",
      " - 0s - loss: 0.7932 - acc: 0.5500 - val_loss: 0.8484 - val_acc: 0.4667\n",
      "Epoch 5/170\n",
      " - 0s - loss: 0.7865 - acc: 0.5500 - val_loss: 0.8375 - val_acc: 0.4667\n",
      "Epoch 6/170\n",
      " - 0s - loss: 0.7778 - acc: 0.5500 - val_loss: 0.8276 - val_acc: 0.4667\n",
      "Epoch 7/170\n",
      " - 0s - loss: 0.7692 - acc: 0.5500 - val_loss: 0.8183 - val_acc: 0.4667\n",
      "Epoch 8/170\n",
      " - 0s - loss: 0.7628 - acc: 0.5500 - val_loss: 0.8088 - val_acc: 0.4667\n",
      "Epoch 9/170\n",
      " - 0s - loss: 0.7558 - acc: 0.5500 - val_loss: 0.7994 - val_acc: 0.4667\n",
      "Epoch 10/170\n",
      " - 0s - loss: 0.7486 - acc: 0.5500 - val_loss: 0.7904 - val_acc: 0.4667\n",
      "Epoch 11/170\n",
      " - 0s - loss: 0.7421 - acc: 0.5500 - val_loss: 0.7815 - val_acc: 0.4667\n",
      "Epoch 12/170\n",
      " - 0s - loss: 0.7358 - acc: 0.5500 - val_loss: 0.7729 - val_acc: 0.4667\n",
      "Epoch 13/170\n",
      " - 0s - loss: 0.7287 - acc: 0.5500 - val_loss: 0.7648 - val_acc: 0.4667\n",
      "Epoch 14/170\n",
      " - 0s - loss: 0.7218 - acc: 0.5500 - val_loss: 0.7575 - val_acc: 0.4667\n",
      "Epoch 15/170\n",
      " - 0s - loss: 0.7161 - acc: 0.5500 - val_loss: 0.7503 - val_acc: 0.4667\n",
      "Epoch 16/170\n",
      " - 0s - loss: 0.7110 - acc: 0.5500 - val_loss: 0.7427 - val_acc: 0.4667\n",
      "Epoch 17/170\n",
      " - 0s - loss: 0.7061 - acc: 0.5500 - val_loss: 0.7350 - val_acc: 0.4667\n",
      "Epoch 18/170\n",
      " - 0s - loss: 0.6991 - acc: 0.5500 - val_loss: 0.7284 - val_acc: 0.4667\n",
      "Epoch 19/170\n",
      " - 0s - loss: 0.6941 - acc: 0.5500 - val_loss: 0.7217 - val_acc: 0.4667\n",
      "Epoch 20/170\n",
      " - 0s - loss: 0.6882 - acc: 0.5500 - val_loss: 0.7157 - val_acc: 0.4667\n",
      "Epoch 21/170\n",
      " - 0s - loss: 0.6837 - acc: 0.5500 - val_loss: 0.7094 - val_acc: 0.4667\n",
      "Epoch 22/170\n",
      " - 0s - loss: 0.6786 - acc: 0.5500 - val_loss: 0.7031 - val_acc: 0.4667\n",
      "Epoch 23/170\n",
      " - 0s - loss: 0.6736 - acc: 0.5500 - val_loss: 0.6971 - val_acc: 0.4667\n",
      "Epoch 24/170\n",
      " - 0s - loss: 0.6687 - acc: 0.5500 - val_loss: 0.6913 - val_acc: 0.4667\n",
      "Epoch 25/170\n",
      " - 0s - loss: 0.6638 - acc: 0.5500 - val_loss: 0.6857 - val_acc: 0.4667\n",
      "Epoch 26/170\n",
      " - 0s - loss: 0.6591 - acc: 0.5500 - val_loss: 0.6802 - val_acc: 0.4667\n",
      "Epoch 27/170\n",
      " - 0s - loss: 0.6545 - acc: 0.5500 - val_loss: 0.6748 - val_acc: 0.4667\n",
      "Epoch 28/170\n",
      " - 0s - loss: 0.6508 - acc: 0.5500 - val_loss: 0.6690 - val_acc: 0.4667\n",
      "Epoch 29/170\n",
      " - 0s - loss: 0.6462 - acc: 0.5500 - val_loss: 0.6633 - val_acc: 0.4667\n",
      "Epoch 30/170\n",
      " - 0s - loss: 0.6415 - acc: 0.5500 - val_loss: 0.6581 - val_acc: 0.4667\n",
      "Epoch 31/170\n",
      " - 0s - loss: 0.6367 - acc: 0.5500 - val_loss: 0.6533 - val_acc: 0.4667\n",
      "Epoch 32/170\n",
      " - 0s - loss: 0.6325 - acc: 0.5500 - val_loss: 0.6484 - val_acc: 0.4667\n",
      "Epoch 33/170\n",
      " - 0s - loss: 0.6286 - acc: 0.5500 - val_loss: 0.6434 - val_acc: 0.4667\n",
      "Epoch 34/170\n",
      " - 0s - loss: 0.6244 - acc: 0.5500 - val_loss: 0.6385 - val_acc: 0.4667\n",
      "Epoch 35/170\n",
      " - 0s - loss: 0.6199 - acc: 0.5750 - val_loss: 0.6342 - val_acc: 0.4667\n",
      "Epoch 36/170\n",
      " - 0s - loss: 0.6162 - acc: 0.5750 - val_loss: 0.6295 - val_acc: 0.4667\n",
      "Epoch 37/170\n",
      " - 0s - loss: 0.6121 - acc: 0.5750 - val_loss: 0.6248 - val_acc: 0.4667\n",
      "Epoch 38/170\n",
      " - 0s - loss: 0.6080 - acc: 0.5750 - val_loss: 0.6205 - val_acc: 0.4667\n",
      "Epoch 39/170\n",
      " - 0s - loss: 0.6042 - acc: 0.5750 - val_loss: 0.6161 - val_acc: 0.4833\n",
      "Epoch 40/170\n",
      " - 0s - loss: 0.6004 - acc: 0.6000 - val_loss: 0.6116 - val_acc: 0.5000\n",
      "Epoch 41/170\n",
      " - 0s - loss: 0.5967 - acc: 0.6000 - val_loss: 0.6071 - val_acc: 0.5000\n",
      "Epoch 42/170\n",
      " - 0s - loss: 0.5926 - acc: 0.6000 - val_loss: 0.6029 - val_acc: 0.5000\n",
      "Epoch 43/170\n",
      " - 0s - loss: 0.5895 - acc: 0.6000 - val_loss: 0.5983 - val_acc: 0.5167\n",
      "Epoch 44/170\n",
      " - 0s - loss: 0.5852 - acc: 0.6250 - val_loss: 0.5942 - val_acc: 0.5167\n",
      "Epoch 45/170\n",
      " - 0s - loss: 0.5819 - acc: 0.6250 - val_loss: 0.5899 - val_acc: 0.5333\n",
      "Epoch 46/170\n",
      " - 0s - loss: 0.5778 - acc: 0.6500 - val_loss: 0.5861 - val_acc: 0.5333\n",
      "Epoch 47/170\n",
      " - 0s - loss: 0.5741 - acc: 0.6500 - val_loss: 0.5825 - val_acc: 0.5333\n",
      "Epoch 48/170\n",
      " - 0s - loss: 0.5710 - acc: 0.6500 - val_loss: 0.5784 - val_acc: 0.5333\n",
      "Epoch 49/170\n",
      " - 0s - loss: 0.5672 - acc: 0.6500 - val_loss: 0.5748 - val_acc: 0.5333\n",
      "Epoch 50/170\n",
      " - 0s - loss: 0.5636 - acc: 0.6500 - val_loss: 0.5713 - val_acc: 0.5667\n",
      "Epoch 51/170\n",
      " - 0s - loss: 0.5603 - acc: 0.6500 - val_loss: 0.5676 - val_acc: 0.5667\n",
      "Epoch 52/170\n",
      " - 0s - loss: 0.5569 - acc: 0.6500 - val_loss: 0.5639 - val_acc: 0.5667\n",
      "Epoch 53/170\n",
      " - 0s - loss: 0.5538 - acc: 0.6500 - val_loss: 0.5601 - val_acc: 0.5833\n",
      "Epoch 54/170\n",
      " - 0s - loss: 0.5503 - acc: 0.6750 - val_loss: 0.5564 - val_acc: 0.5833\n",
      "Epoch 55/170\n",
      " - 0s - loss: 0.5468 - acc: 0.6750 - val_loss: 0.5531 - val_acc: 0.6167\n",
      "Epoch 56/170\n",
      " - 0s - loss: 0.5438 - acc: 0.6750 - val_loss: 0.5494 - val_acc: 0.6167\n",
      "Epoch 57/170\n",
      " - 0s - loss: 0.5404 - acc: 0.6750 - val_loss: 0.5460 - val_acc: 0.6167\n",
      "Epoch 58/170\n",
      " - 0s - loss: 0.5371 - acc: 0.6750 - val_loss: 0.5427 - val_acc: 0.6167\n",
      "Epoch 59/170\n",
      " - 0s - loss: 0.5341 - acc: 0.6750 - val_loss: 0.5391 - val_acc: 0.6333\n",
      "Epoch 60/170\n",
      " - 0s - loss: 0.5308 - acc: 0.6750 - val_loss: 0.5357 - val_acc: 0.6333\n",
      "Epoch 61/170\n",
      " - 0s - loss: 0.5277 - acc: 0.6750 - val_loss: 0.5325 - val_acc: 0.6333\n",
      "Epoch 62/170\n",
      " - 0s - loss: 0.5246 - acc: 0.7000 - val_loss: 0.5293 - val_acc: 0.6333\n",
      "Epoch 63/170\n",
      " - 0s - loss: 0.5216 - acc: 0.7000 - val_loss: 0.5259 - val_acc: 0.6500\n",
      "Epoch 64/170\n",
      " - 0s - loss: 0.5184 - acc: 0.7000 - val_loss: 0.5229 - val_acc: 0.6667\n",
      "Epoch 65/170\n",
      " - 0s - loss: 0.5154 - acc: 0.7000 - val_loss: 0.5196 - val_acc: 0.7000\n",
      "Epoch 66/170\n",
      " - 0s - loss: 0.5123 - acc: 0.7000 - val_loss: 0.5164 - val_acc: 0.7000\n",
      "Epoch 67/170\n",
      " - 0s - loss: 0.5094 - acc: 0.7250 - val_loss: 0.5132 - val_acc: 0.7000\n",
      "Epoch 68/170\n",
      " - 0s - loss: 0.5066 - acc: 0.7250 - val_loss: 0.5100 - val_acc: 0.7167\n",
      "Epoch 69/170\n",
      " - 0s - loss: 0.5035 - acc: 0.7250 - val_loss: 0.5068 - val_acc: 0.7167\n",
      "Epoch 70/170\n",
      " - 0s - loss: 0.5006 - acc: 0.7250 - val_loss: 0.5039 - val_acc: 0.7333\n",
      "Epoch 71/170\n",
      " - 0s - loss: 0.4979 - acc: 0.7500 - val_loss: 0.5006 - val_acc: 0.8000\n",
      "Epoch 72/170\n",
      " - 0s - loss: 0.4949 - acc: 0.7500 - val_loss: 0.4977 - val_acc: 0.8167\n",
      "Epoch 73/170\n",
      " - 0s - loss: 0.4921 - acc: 0.7500 - val_loss: 0.4946 - val_acc: 0.8167\n",
      "Epoch 74/170\n",
      " - 0s - loss: 0.4892 - acc: 0.7750 - val_loss: 0.4918 - val_acc: 0.8167\n",
      "Epoch 75/170\n",
      " - 0s - loss: 0.4865 - acc: 0.7750 - val_loss: 0.4888 - val_acc: 0.8333\n",
      "Epoch 76/170\n",
      " - 0s - loss: 0.4837 - acc: 0.7750 - val_loss: 0.4859 - val_acc: 0.8333\n",
      "Epoch 77/170\n",
      " - 0s - loss: 0.4813 - acc: 0.7750 - val_loss: 0.4828 - val_acc: 0.8500\n",
      "Epoch 78/170\n",
      " - 0s - loss: 0.4783 - acc: 0.8000 - val_loss: 0.4800 - val_acc: 0.8500\n",
      "Epoch 79/170\n",
      " - 0s - loss: 0.4755 - acc: 0.8000 - val_loss: 0.4775 - val_acc: 0.8500\n",
      "Epoch 80/170\n",
      " - 0s - loss: 0.4729 - acc: 0.8000 - val_loss: 0.4748 - val_acc: 0.8500\n",
      "Epoch 81/170\n",
      " - 0s - loss: 0.4703 - acc: 0.8000 - val_loss: 0.4720 - val_acc: 0.8667\n",
      "Epoch 82/170\n",
      " - 0s - loss: 0.4676 - acc: 0.8250 - val_loss: 0.4694 - val_acc: 0.8667\n",
      "Epoch 83/170\n",
      " - 0s - loss: 0.4652 - acc: 0.8250 - val_loss: 0.4666 - val_acc: 0.8667\n",
      "Epoch 84/170\n",
      " - 0s - loss: 0.4625 - acc: 0.8250 - val_loss: 0.4643 - val_acc: 0.8667\n",
      "Epoch 85/170\n",
      " - 0s - loss: 0.4599 - acc: 0.8250 - val_loss: 0.4618 - val_acc: 0.8667\n",
      "Epoch 86/170\n",
      " - 0s - loss: 0.4574 - acc: 0.8250 - val_loss: 0.4590 - val_acc: 0.8667\n",
      "Epoch 87/170\n",
      " - 0s - loss: 0.4550 - acc: 0.8250 - val_loss: 0.4563 - val_acc: 0.9000\n",
      "Epoch 88/170\n",
      " - 0s - loss: 0.4524 - acc: 0.8250 - val_loss: 0.4537 - val_acc: 0.9000\n",
      "Epoch 89/170\n",
      " - 0s - loss: 0.4499 - acc: 0.8250 - val_loss: 0.4512 - val_acc: 0.9000\n",
      "Epoch 90/170\n",
      " - 0s - loss: 0.4475 - acc: 0.8250 - val_loss: 0.4488 - val_acc: 0.9000\n",
      "Epoch 91/170\n",
      " - 0s - loss: 0.4453 - acc: 0.8250 - val_loss: 0.4460 - val_acc: 0.9000\n",
      "Epoch 92/170\n",
      " - 0s - loss: 0.4428 - acc: 0.8250 - val_loss: 0.4434 - val_acc: 0.9000\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.4402 - acc: 0.8250 - val_loss: 0.4411 - val_acc: 0.9000\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.4379 - acc: 0.8500 - val_loss: 0.4386 - val_acc: 0.9333\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.4356 - acc: 0.8750 - val_loss: 0.4362 - val_acc: 0.9500\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.4332 - acc: 0.8750 - val_loss: 0.4340 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/170\n",
      " - 0s - loss: 0.4310 - acc: 0.8750 - val_loss: 0.4319 - val_acc: 0.9500\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.4288 - acc: 0.8750 - val_loss: 0.4295 - val_acc: 0.9500\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.4265 - acc: 0.9000 - val_loss: 0.4271 - val_acc: 0.9500\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.4243 - acc: 0.9250 - val_loss: 0.4247 - val_acc: 0.9500\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.4219 - acc: 0.9500 - val_loss: 0.4225 - val_acc: 0.9500\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.4198 - acc: 0.9500 - val_loss: 0.4202 - val_acc: 0.9500\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.4176 - acc: 0.9500 - val_loss: 0.4179 - val_acc: 0.9500\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.4154 - acc: 0.9500 - val_loss: 0.4157 - val_acc: 0.9500\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.4132 - acc: 0.9500 - val_loss: 0.4136 - val_acc: 0.9667\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.4111 - acc: 0.9500 - val_loss: 0.4115 - val_acc: 0.9667\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.4089 - acc: 0.9500 - val_loss: 0.4094 - val_acc: 0.9667\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.4069 - acc: 0.9500 - val_loss: 0.4074 - val_acc: 0.9667\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.4048 - acc: 0.9750 - val_loss: 0.4054 - val_acc: 0.9667\n",
      "Epoch 110/170\n",
      " - 0s - loss: 0.4027 - acc: 0.9750 - val_loss: 0.4032 - val_acc: 0.9667\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.4006 - acc: 0.9750 - val_loss: 0.4012 - val_acc: 0.9667\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.3986 - acc: 0.9750 - val_loss: 0.3991 - val_acc: 0.9833\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.3965 - acc: 0.9750 - val_loss: 0.3969 - val_acc: 0.9833\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.3945 - acc: 0.9750 - val_loss: 0.3949 - val_acc: 0.9833\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.3925 - acc: 0.9750 - val_loss: 0.3928 - val_acc: 0.9833\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.3905 - acc: 0.9750 - val_loss: 0.3907 - val_acc: 0.9833\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.3886 - acc: 0.9750 - val_loss: 0.3885 - val_acc: 0.9833\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.3865 - acc: 0.9750 - val_loss: 0.3865 - val_acc: 0.9833\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.3846 - acc: 0.9750 - val_loss: 0.3845 - val_acc: 0.9833\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.3826 - acc: 0.9750 - val_loss: 0.3827 - val_acc: 0.9833\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.3807 - acc: 0.9750 - val_loss: 0.3807 - val_acc: 0.9833\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.3788 - acc: 0.9750 - val_loss: 0.3787 - val_acc: 0.9833\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.3769 - acc: 1.0000 - val_loss: 0.3768 - val_acc: 0.9833\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.3752 - acc: 1.0000 - val_loss: 0.3747 - val_acc: 0.9833\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.3731 - acc: 1.0000 - val_loss: 0.3729 - val_acc: 0.9833\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.3713 - acc: 1.0000 - val_loss: 0.3709 - val_acc: 0.9833\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.3694 - acc: 1.0000 - val_loss: 0.3691 - val_acc: 0.9833\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.3676 - acc: 1.0000 - val_loss: 0.3672 - val_acc: 0.9833\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.3658 - acc: 1.0000 - val_loss: 0.3653 - val_acc: 0.9833\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.3639 - acc: 1.0000 - val_loss: 0.3637 - val_acc: 0.9833\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.3623 - acc: 1.0000 - val_loss: 0.3617 - val_acc: 0.9833\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.3604 - acc: 1.0000 - val_loss: 0.3602 - val_acc: 0.9833\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.3586 - acc: 1.0000 - val_loss: 0.3583 - val_acc: 0.9833\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.3570 - acc: 1.0000 - val_loss: 0.3563 - val_acc: 0.9833\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.3551 - acc: 1.0000 - val_loss: 0.3546 - val_acc: 0.9833\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.3533 - acc: 1.0000 - val_loss: 0.3529 - val_acc: 0.9833\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.3516 - acc: 1.0000 - val_loss: 0.3513 - val_acc: 0.9833\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.3499 - acc: 1.0000 - val_loss: 0.3497 - val_acc: 0.9833\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.3483 - acc: 1.0000 - val_loss: 0.3479 - val_acc: 0.9833\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.3465 - acc: 1.0000 - val_loss: 0.3461 - val_acc: 0.9833\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.3449 - acc: 1.0000 - val_loss: 0.3446 - val_acc: 0.9833\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.3432 - acc: 1.0000 - val_loss: 0.3430 - val_acc: 0.9833\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.3415 - acc: 1.0000 - val_loss: 0.3414 - val_acc: 0.9833\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.3399 - acc: 1.0000 - val_loss: 0.3397 - val_acc: 0.9833\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.3383 - acc: 1.0000 - val_loss: 0.3380 - val_acc: 0.9833\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.3366 - acc: 1.0000 - val_loss: 0.3364 - val_acc: 0.9833\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.3352 - acc: 1.0000 - val_loss: 0.3346 - val_acc: 0.9833\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.3335 - acc: 1.0000 - val_loss: 0.3331 - val_acc: 0.9833\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.3318 - acc: 1.0000 - val_loss: 0.3315 - val_acc: 0.9833\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.3303 - acc: 1.0000 - val_loss: 0.3298 - val_acc: 0.9833\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.3287 - acc: 1.0000 - val_loss: 0.3282 - val_acc: 0.9833\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.3272 - acc: 1.0000 - val_loss: 0.3266 - val_acc: 0.9833\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.3256 - acc: 1.0000 - val_loss: 0.3250 - val_acc: 0.9833\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.3240 - acc: 1.0000 - val_loss: 0.3235 - val_acc: 0.9833\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.3225 - acc: 1.0000 - val_loss: 0.3220 - val_acc: 0.9833\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.3210 - acc: 1.0000 - val_loss: 0.3207 - val_acc: 0.9833\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.3195 - acc: 1.0000 - val_loss: 0.3192 - val_acc: 0.9833\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.3180 - acc: 1.0000 - val_loss: 0.3176 - val_acc: 0.9833\n",
      "Epoch 159/170\n",
      " - 0s - loss: 0.3166 - acc: 1.0000 - val_loss: 0.3161 - val_acc: 0.9833\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.3151 - acc: 1.0000 - val_loss: 0.3145 - val_acc: 0.9833\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.3136 - acc: 1.0000 - val_loss: 0.3131 - val_acc: 0.9833\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.3121 - acc: 1.0000 - val_loss: 0.3117 - val_acc: 0.9833\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.3108 - acc: 1.0000 - val_loss: 0.3101 - val_acc: 0.9833\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.3093 - acc: 1.0000 - val_loss: 0.3088 - val_acc: 0.9833\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.3078 - acc: 1.0000 - val_loss: 0.3074 - val_acc: 0.9833\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.3064 - acc: 1.0000 - val_loss: 0.3059 - val_acc: 0.9833\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.3051 - acc: 1.0000 - val_loss: 0.3045 - val_acc: 0.9833\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.3036 - acc: 1.0000 - val_loss: 0.3031 - val_acc: 0.9833\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.3022 - acc: 1.0000 - val_loss: 0.3017 - val_acc: 0.9833\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.3008 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.9833\n",
      "Test accuracy: 0.9833333412806193\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.6025 - acc: 0.5500 - val_loss: 0.5472 - val_acc: 0.5333\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.5933 - acc: 0.5500 - val_loss: 0.5400 - val_acc: 0.5333\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.5852 - acc: 0.5500 - val_loss: 0.5326 - val_acc: 0.5333\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.5768 - acc: 0.5500 - val_loss: 0.5252 - val_acc: 0.5333\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.5678 - acc: 0.5500 - val_loss: 0.5178 - val_acc: 0.5333\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.5597 - acc: 0.5500 - val_loss: 0.5105 - val_acc: 0.5333\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5506 - acc: 0.5500 - val_loss: 0.5033 - val_acc: 0.5333\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5427 - acc: 0.5750 - val_loss: 0.4962 - val_acc: 0.5833\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5344 - acc: 0.5750 - val_loss: 0.4892 - val_acc: 0.5833\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5265 - acc: 0.5750 - val_loss: 0.4824 - val_acc: 0.6000\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5184 - acc: 0.5750 - val_loss: 0.4758 - val_acc: 0.6000\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5101 - acc: 0.5750 - val_loss: 0.4693 - val_acc: 0.6333\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5031 - acc: 0.5750 - val_loss: 0.4628 - val_acc: 0.6500\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.4960 - acc: 0.6000 - val_loss: 0.4566 - val_acc: 0.6500\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.4880 - acc: 0.6750 - val_loss: 0.4507 - val_acc: 0.6500\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.4813 - acc: 0.7000 - val_loss: 0.4449 - val_acc: 0.7000\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.4752 - acc: 0.7000 - val_loss: 0.4392 - val_acc: 0.7000\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.4677 - acc: 0.7000 - val_loss: 0.4339 - val_acc: 0.7000\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.4619 - acc: 0.7000 - val_loss: 0.4287 - val_acc: 0.7167\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.4552 - acc: 0.7250 - val_loss: 0.4236 - val_acc: 0.7167\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.4497 - acc: 0.7250 - val_loss: 0.4186 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      " - 0s - loss: 0.4437 - acc: 0.7250 - val_loss: 0.4138 - val_acc: 0.7500\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.4379 - acc: 0.7250 - val_loss: 0.4092 - val_acc: 0.7500\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.4325 - acc: 0.7250 - val_loss: 0.4047 - val_acc: 0.7500\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.4273 - acc: 0.7250 - val_loss: 0.4005 - val_acc: 0.8167\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.4222 - acc: 0.7250 - val_loss: 0.3964 - val_acc: 0.8167\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.4166 - acc: 0.7250 - val_loss: 0.3924 - val_acc: 0.8167\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.4127 - acc: 0.7500 - val_loss: 0.3885 - val_acc: 0.8167\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.4075 - acc: 0.7500 - val_loss: 0.3848 - val_acc: 0.8167\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.4030 - acc: 0.7500 - val_loss: 0.3812 - val_acc: 0.8500\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.3987 - acc: 0.8250 - val_loss: 0.3776 - val_acc: 0.8667\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.3944 - acc: 0.8750 - val_loss: 0.3742 - val_acc: 0.9000\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.3903 - acc: 0.8750 - val_loss: 0.3710 - val_acc: 0.9000\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.3863 - acc: 0.8750 - val_loss: 0.3679 - val_acc: 0.9333\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.3824 - acc: 0.8750 - val_loss: 0.3649 - val_acc: 0.9500\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.3784 - acc: 0.9250 - val_loss: 0.3620 - val_acc: 0.9833\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.3749 - acc: 0.9250 - val_loss: 0.3591 - val_acc: 0.9833\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.3716 - acc: 0.9250 - val_loss: 0.3563 - val_acc: 0.9833\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.3682 - acc: 0.9500 - val_loss: 0.3537 - val_acc: 0.9833\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.3646 - acc: 0.9500 - val_loss: 0.3512 - val_acc: 0.9833\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.3614 - acc: 0.9500 - val_loss: 0.3488 - val_acc: 0.9833\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.3586 - acc: 0.9500 - val_loss: 0.3465 - val_acc: 0.9833\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.3558 - acc: 0.9500 - val_loss: 0.3443 - val_acc: 0.9833\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.3527 - acc: 0.9500 - val_loss: 0.3423 - val_acc: 0.9833\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.3500 - acc: 0.9500 - val_loss: 0.3402 - val_acc: 0.9833\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.3473 - acc: 0.9500 - val_loss: 0.3383 - val_acc: 0.9833\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.3447 - acc: 0.9500 - val_loss: 0.3363 - val_acc: 0.9833\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.3427 - acc: 0.9500 - val_loss: 0.3344 - val_acc: 0.9833\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.3396 - acc: 0.9750 - val_loss: 0.3327 - val_acc: 0.9833\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.3376 - acc: 0.9750 - val_loss: 0.3310 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.3353 - acc: 0.9750 - val_loss: 0.3294 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.3330 - acc: 0.9750 - val_loss: 0.3278 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.3312 - acc: 0.9750 - val_loss: 0.3263 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.3289 - acc: 1.0000 - val_loss: 0.3249 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.3271 - acc: 1.0000 - val_loss: 0.3236 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.3251 - acc: 1.0000 - val_loss: 0.3223 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.3234 - acc: 1.0000 - val_loss: 0.3210 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.3216 - acc: 1.0000 - val_loss: 0.3198 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.3201 - acc: 1.0000 - val_loss: 0.3186 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.3184 - acc: 1.0000 - val_loss: 0.3175 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.3167 - acc: 1.0000 - val_loss: 0.3165 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.3155 - acc: 1.0000 - val_loss: 0.3155 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.3137 - acc: 1.0000 - val_loss: 0.3146 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.3125 - acc: 1.0000 - val_loss: 0.3136 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.3112 - acc: 1.0000 - val_loss: 0.3127 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.3098 - acc: 1.0000 - val_loss: 0.3119 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.3085 - acc: 1.0000 - val_loss: 0.3110 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.3073 - acc: 1.0000 - val_loss: 0.3102 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.3061 - acc: 1.0000 - val_loss: 0.3094 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.3048 - acc: 1.0000 - val_loss: 0.3086 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.3037 - acc: 1.0000 - val_loss: 0.3079 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.3025 - acc: 1.0000 - val_loss: 0.3072 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.3015 - acc: 1.0000 - val_loss: 0.3065 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.3003 - acc: 1.0000 - val_loss: 0.3059 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2994 - acc: 1.0000 - val_loss: 0.3052 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.2985 - acc: 1.0000 - val_loss: 0.3047 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.2976 - acc: 1.0000 - val_loss: 0.3041 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.2967 - acc: 1.0000 - val_loss: 0.3036 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.2957 - acc: 1.0000 - val_loss: 0.3031 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.2951 - acc: 1.0000 - val_loss: 0.3026 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.2944 - acc: 1.0000 - val_loss: 0.3021 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.2935 - acc: 1.0000 - val_loss: 0.3016 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.2929 - acc: 1.0000 - val_loss: 0.3012 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.2921 - acc: 1.0000 - val_loss: 0.3007 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.2915 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.2908 - acc: 1.0000 - val_loss: 0.2998 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.2901 - acc: 1.0000 - val_loss: 0.2994 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.2895 - acc: 1.0000 - val_loss: 0.2990 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.2889 - acc: 1.0000 - val_loss: 0.2986 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.2884 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.2877 - acc: 1.0000 - val_loss: 0.2978 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.2871 - acc: 1.0000 - val_loss: 0.2974 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.2866 - acc: 1.0000 - val_loss: 0.2970 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.2860 - acc: 1.0000 - val_loss: 0.2966 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.2855 - acc: 1.0000 - val_loss: 0.2962 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.2849 - acc: 1.0000 - val_loss: 0.2958 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.2844 - acc: 1.0000 - val_loss: 0.2954 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.2839 - acc: 1.0000 - val_loss: 0.2951 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2833 - acc: 1.0000 - val_loss: 0.2947 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2828 - acc: 1.0000 - val_loss: 0.2943 - val_acc: 1.0000\n",
      "Test accuracy: 1.0\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 1.3511 - acc: 0.0000e+00 - val_loss: 1.3797 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.3415 - acc: 0.0000e+00 - val_loss: 1.3728 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.3321 - acc: 0.0000e+00 - val_loss: 1.3651 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3223 - acc: 0.0250 - val_loss: 1.3571 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.3115 - acc: 0.0250 - val_loss: 1.3488 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.3020 - acc: 0.0250 - val_loss: 1.3407 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 1.2903 - acc: 0.0250 - val_loss: 1.3323 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 1.2808 - acc: 0.0250 - val_loss: 1.3238 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 1.2702 - acc: 0.0250 - val_loss: 1.3153 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 1.2614 - acc: 0.0250 - val_loss: 1.3070 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 1.2522 - acc: 0.0750 - val_loss: 1.2989 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 1.2427 - acc: 0.0500 - val_loss: 1.2902 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 1.2316 - acc: 0.0750 - val_loss: 1.2818 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 1.2253 - acc: 0.0750 - val_loss: 1.2739 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 1.2142 - acc: 0.0750 - val_loss: 1.2657 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.2061 - acc: 0.0750 - val_loss: 1.2573 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      " - 0s - loss: 1.1967 - acc: 0.0750 - val_loss: 1.2488 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.1883 - acc: 0.0750 - val_loss: 1.2404 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.1788 - acc: 0.1000 - val_loss: 1.2319 - val_acc: 0.0167\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.1717 - acc: 0.0750 - val_loss: 1.2234 - val_acc: 0.0167\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.1620 - acc: 0.1000 - val_loss: 1.2151 - val_acc: 0.0333\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.1535 - acc: 0.1000 - val_loss: 1.2066 - val_acc: 0.0500\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.1465 - acc: 0.1250 - val_loss: 1.1988 - val_acc: 0.0667\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.1362 - acc: 0.1250 - val_loss: 1.1905 - val_acc: 0.0833\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.1277 - acc: 0.1250 - val_loss: 1.1823 - val_acc: 0.1000\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.1202 - acc: 0.1250 - val_loss: 1.1739 - val_acc: 0.1333\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.1120 - acc: 0.1250 - val_loss: 1.1655 - val_acc: 0.1333\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.1048 - acc: 0.1250 - val_loss: 1.1569 - val_acc: 0.1333\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.0952 - acc: 0.1250 - val_loss: 1.1485 - val_acc: 0.1333\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.0867 - acc: 0.1250 - val_loss: 1.1402 - val_acc: 0.1333\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.0803 - acc: 0.1750 - val_loss: 1.1323 - val_acc: 0.1667\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.0716 - acc: 0.2000 - val_loss: 1.1243 - val_acc: 0.1667\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.0628 - acc: 0.2000 - val_loss: 1.1158 - val_acc: 0.1667\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.0558 - acc: 0.2000 - val_loss: 1.1073 - val_acc: 0.1833\n",
      "Epoch 35/100\n",
      " - 0s - loss: 1.0486 - acc: 0.2250 - val_loss: 1.0992 - val_acc: 0.2500\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.0402 - acc: 0.2250 - val_loss: 1.0909 - val_acc: 0.2667\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.0335 - acc: 0.2250 - val_loss: 1.0830 - val_acc: 0.2667\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.0239 - acc: 0.2750 - val_loss: 1.0751 - val_acc: 0.2667\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.0177 - acc: 0.2500 - val_loss: 1.0670 - val_acc: 0.2667\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.0093 - acc: 0.3250 - val_loss: 1.0593 - val_acc: 0.3167\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.0015 - acc: 0.3500 - val_loss: 1.0515 - val_acc: 0.3333\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.9939 - acc: 0.3500 - val_loss: 1.0437 - val_acc: 0.3500\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.9875 - acc: 0.3500 - val_loss: 1.0357 - val_acc: 0.3667\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.9783 - acc: 0.3750 - val_loss: 1.0281 - val_acc: 0.3833\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.9713 - acc: 0.3750 - val_loss: 1.0202 - val_acc: 0.4000\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.9650 - acc: 0.4250 - val_loss: 1.0127 - val_acc: 0.4000\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.9567 - acc: 0.4500 - val_loss: 1.0052 - val_acc: 0.4000\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.9504 - acc: 0.4500 - val_loss: 0.9977 - val_acc: 0.4000\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.9422 - acc: 0.4500 - val_loss: 0.9900 - val_acc: 0.4000\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.9351 - acc: 0.4750 - val_loss: 0.9824 - val_acc: 0.4000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.9279 - acc: 0.4750 - val_loss: 0.9749 - val_acc: 0.4167\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.9212 - acc: 0.4750 - val_loss: 0.9673 - val_acc: 0.4167\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.9142 - acc: 0.4750 - val_loss: 0.9598 - val_acc: 0.4167\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.9070 - acc: 0.4750 - val_loss: 0.9522 - val_acc: 0.4167\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.8993 - acc: 0.4750 - val_loss: 0.9450 - val_acc: 0.4167\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.8924 - acc: 0.5250 - val_loss: 0.9378 - val_acc: 0.4167\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.8865 - acc: 0.5250 - val_loss: 0.9305 - val_acc: 0.4333\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.8787 - acc: 0.5250 - val_loss: 0.9232 - val_acc: 0.4333\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.8721 - acc: 0.5250 - val_loss: 0.9158 - val_acc: 0.4333\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.8658 - acc: 0.5250 - val_loss: 0.9086 - val_acc: 0.4333\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.8598 - acc: 0.5250 - val_loss: 0.9016 - val_acc: 0.4500\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.8520 - acc: 0.5250 - val_loss: 0.8945 - val_acc: 0.4500\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.8464 - acc: 0.5250 - val_loss: 0.8878 - val_acc: 0.4500\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.8401 - acc: 0.5500 - val_loss: 0.8808 - val_acc: 0.4667\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.8324 - acc: 0.5500 - val_loss: 0.8737 - val_acc: 0.4667\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.8254 - acc: 0.5500 - val_loss: 0.8669 - val_acc: 0.4667\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.8198 - acc: 0.5500 - val_loss: 0.8599 - val_acc: 0.4667\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.8124 - acc: 0.5500 - val_loss: 0.8532 - val_acc: 0.4667\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.8065 - acc: 0.5500 - val_loss: 0.8464 - val_acc: 0.4667\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.7997 - acc: 0.5500 - val_loss: 0.8399 - val_acc: 0.4667\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.7936 - acc: 0.5500 - val_loss: 0.8334 - val_acc: 0.4667\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.7873 - acc: 0.5500 - val_loss: 0.8268 - val_acc: 0.4667\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.7822 - acc: 0.5500 - val_loss: 0.8204 - val_acc: 0.4667\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.7751 - acc: 0.5500 - val_loss: 0.8138 - val_acc: 0.4667\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.7686 - acc: 0.5500 - val_loss: 0.8072 - val_acc: 0.4667\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.7634 - acc: 0.5500 - val_loss: 0.8006 - val_acc: 0.4667\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.7574 - acc: 0.5500 - val_loss: 0.7942 - val_acc: 0.4667\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.7508 - acc: 0.5500 - val_loss: 0.7879 - val_acc: 0.4667\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.7449 - acc: 0.5500 - val_loss: 0.7816 - val_acc: 0.4667\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.7389 - acc: 0.5500 - val_loss: 0.7754 - val_acc: 0.4667\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.7343 - acc: 0.5500 - val_loss: 0.7695 - val_acc: 0.4667\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.7273 - acc: 0.5500 - val_loss: 0.7631 - val_acc: 0.4667\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.7229 - acc: 0.5500 - val_loss: 0.7572 - val_acc: 0.4667\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.7160 - acc: 0.5500 - val_loss: 0.7511 - val_acc: 0.4667\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7099 - acc: 0.5500 - val_loss: 0.7451 - val_acc: 0.4667\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.7055 - acc: 0.5500 - val_loss: 0.7389 - val_acc: 0.4667\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.6987 - acc: 0.5500 - val_loss: 0.7331 - val_acc: 0.4667\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.6940 - acc: 0.5500 - val_loss: 0.7272 - val_acc: 0.4667\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.6885 - acc: 0.5500 - val_loss: 0.7214 - val_acc: 0.4667\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.6831 - acc: 0.5500 - val_loss: 0.7157 - val_acc: 0.4667\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.6774 - acc: 0.5500 - val_loss: 0.7100 - val_acc: 0.4667\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.6723 - acc: 0.5500 - val_loss: 0.7044 - val_acc: 0.4667\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.6673 - acc: 0.5500 - val_loss: 0.6989 - val_acc: 0.4667\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.6610 - acc: 0.5500 - val_loss: 0.6936 - val_acc: 0.4667\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.6562 - acc: 0.5500 - val_loss: 0.6882 - val_acc: 0.4667\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6507 - acc: 0.5500 - val_loss: 0.6829 - val_acc: 0.4667\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6457 - acc: 0.5500 - val_loss: 0.6775 - val_acc: 0.4667\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6412 - acc: 0.5500 - val_loss: 0.6723 - val_acc: 0.4667\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6355 - acc: 0.5500 - val_loss: 0.6670 - val_acc: 0.4667\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6313 - acc: 0.5500 - val_loss: 0.6618 - val_acc: 0.4667\n",
      "Test accuracy: 0.4666666626930237\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 2.5387 - acc: 0.5500 - val_loss: 2.9085 - val_acc: 0.4667\n",
      "Epoch 2/170\n",
      " - 0s - loss: 2.5212 - acc: 0.5500 - val_loss: 2.8939 - val_acc: 0.4667\n",
      "Epoch 3/170\n",
      " - 0s - loss: 2.5081 - acc: 0.5500 - val_loss: 2.8771 - val_acc: 0.4667\n",
      "Epoch 4/170\n",
      " - 0s - loss: 2.4935 - acc: 0.5500 - val_loss: 2.8592 - val_acc: 0.4667\n",
      "Epoch 5/170\n",
      " - 0s - loss: 2.4774 - acc: 0.5500 - val_loss: 2.8392 - val_acc: 0.4667\n",
      "Epoch 6/170\n",
      " - 0s - loss: 2.4605 - acc: 0.5500 - val_loss: 2.8196 - val_acc: 0.4667\n",
      "Epoch 7/170\n",
      " - 0s - loss: 2.4444 - acc: 0.5500 - val_loss: 2.8016 - val_acc: 0.4667\n",
      "Epoch 8/170\n",
      " - 0s - loss: 2.4270 - acc: 0.5500 - val_loss: 2.7791 - val_acc: 0.4667\n",
      "Epoch 9/170\n",
      " - 0s - loss: 2.4093 - acc: 0.5500 - val_loss: 2.7602 - val_acc: 0.4667\n",
      "Epoch 10/170\n",
      " - 0s - loss: 2.3920 - acc: 0.5500 - val_loss: 2.7392 - val_acc: 0.4667\n",
      "Epoch 11/170\n",
      " - 0s - loss: 2.3736 - acc: 0.5500 - val_loss: 2.7176 - val_acc: 0.4667\n",
      "Epoch 12/170\n",
      " - 0s - loss: 2.3560 - acc: 0.5500 - val_loss: 2.6978 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/170\n",
      " - 0s - loss: 2.3368 - acc: 0.5500 - val_loss: 2.6734 - val_acc: 0.4667\n",
      "Epoch 14/170\n",
      " - 0s - loss: 2.3162 - acc: 0.5500 - val_loss: 2.6503 - val_acc: 0.4667\n",
      "Epoch 15/170\n",
      " - 0s - loss: 2.2964 - acc: 0.5500 - val_loss: 2.6275 - val_acc: 0.4667\n",
      "Epoch 16/170\n",
      " - 0s - loss: 2.2765 - acc: 0.5500 - val_loss: 2.6043 - val_acc: 0.4667\n",
      "Epoch 17/170\n",
      " - 0s - loss: 2.2569 - acc: 0.5500 - val_loss: 2.5820 - val_acc: 0.4667\n",
      "Epoch 18/170\n",
      " - 0s - loss: 2.2378 - acc: 0.5500 - val_loss: 2.5600 - val_acc: 0.4667\n",
      "Epoch 19/170\n",
      " - 0s - loss: 2.2197 - acc: 0.5500 - val_loss: 2.5398 - val_acc: 0.4667\n",
      "Epoch 20/170\n",
      " - 0s - loss: 2.2008 - acc: 0.5500 - val_loss: 2.5170 - val_acc: 0.4667\n",
      "Epoch 21/170\n",
      " - 0s - loss: 2.1810 - acc: 0.5500 - val_loss: 2.4939 - val_acc: 0.4667\n",
      "Epoch 22/170\n",
      " - 0s - loss: 2.1607 - acc: 0.5500 - val_loss: 2.4699 - val_acc: 0.4667\n",
      "Epoch 23/170\n",
      " - 0s - loss: 2.1403 - acc: 0.5500 - val_loss: 2.4468 - val_acc: 0.4667\n",
      "Epoch 24/170\n",
      " - 0s - loss: 2.1208 - acc: 0.5500 - val_loss: 2.4247 - val_acc: 0.4667\n",
      "Epoch 25/170\n",
      " - 0s - loss: 2.1011 - acc: 0.5500 - val_loss: 2.4015 - val_acc: 0.4667\n",
      "Epoch 26/170\n",
      " - 0s - loss: 2.0807 - acc: 0.5500 - val_loss: 2.3773 - val_acc: 0.4667\n",
      "Epoch 27/170\n",
      " - 0s - loss: 2.0606 - acc: 0.5500 - val_loss: 2.3550 - val_acc: 0.4667\n",
      "Epoch 28/170\n",
      " - 0s - loss: 2.0415 - acc: 0.5500 - val_loss: 2.3330 - val_acc: 0.4667\n",
      "Epoch 29/170\n",
      " - 0s - loss: 2.0224 - acc: 0.5500 - val_loss: 2.3110 - val_acc: 0.4667\n",
      "Epoch 30/170\n",
      " - 0s - loss: 2.0034 - acc: 0.5500 - val_loss: 2.2891 - val_acc: 0.4667\n",
      "Epoch 31/170\n",
      " - 0s - loss: 1.9843 - acc: 0.5500 - val_loss: 2.2670 - val_acc: 0.4667\n",
      "Epoch 32/170\n",
      " - 0s - loss: 1.9654 - acc: 0.5500 - val_loss: 2.2452 - val_acc: 0.4667\n",
      "Epoch 33/170\n",
      " - 0s - loss: 1.9470 - acc: 0.5500 - val_loss: 2.2243 - val_acc: 0.4667\n",
      "Epoch 34/170\n",
      " - 0s - loss: 1.9290 - acc: 0.5500 - val_loss: 2.2037 - val_acc: 0.4667\n",
      "Epoch 35/170\n",
      " - 0s - loss: 1.9113 - acc: 0.5500 - val_loss: 2.1833 - val_acc: 0.4667\n",
      "Epoch 36/170\n",
      " - 0s - loss: 1.8935 - acc: 0.5500 - val_loss: 2.1628 - val_acc: 0.4667\n",
      "Epoch 37/170\n",
      " - 0s - loss: 1.8753 - acc: 0.5500 - val_loss: 2.1413 - val_acc: 0.4667\n",
      "Epoch 38/170\n",
      " - 0s - loss: 1.8568 - acc: 0.5500 - val_loss: 2.1200 - val_acc: 0.4667\n",
      "Epoch 39/170\n",
      " - 0s - loss: 1.8380 - acc: 0.5500 - val_loss: 2.0977 - val_acc: 0.4667\n",
      "Epoch 40/170\n",
      " - 0s - loss: 1.8192 - acc: 0.5500 - val_loss: 2.0762 - val_acc: 0.4667\n",
      "Epoch 41/170\n",
      " - 0s - loss: 1.8001 - acc: 0.5500 - val_loss: 2.0537 - val_acc: 0.4667\n",
      "Epoch 42/170\n",
      " - 0s - loss: 1.7817 - acc: 0.5500 - val_loss: 2.0331 - val_acc: 0.4667\n",
      "Epoch 43/170\n",
      " - 0s - loss: 1.7635 - acc: 0.5500 - val_loss: 2.0119 - val_acc: 0.4667\n",
      "Epoch 44/170\n",
      " - 0s - loss: 1.7447 - acc: 0.5500 - val_loss: 1.9896 - val_acc: 0.4667\n",
      "Epoch 45/170\n",
      " - 0s - loss: 1.7255 - acc: 0.5500 - val_loss: 1.9674 - val_acc: 0.4667\n",
      "Epoch 46/170\n",
      " - 0s - loss: 1.7063 - acc: 0.5500 - val_loss: 1.9449 - val_acc: 0.4667\n",
      "Epoch 47/170\n",
      " - 0s - loss: 1.6875 - acc: 0.5500 - val_loss: 1.9237 - val_acc: 0.4667\n",
      "Epoch 48/170\n",
      " - 0s - loss: 1.6687 - acc: 0.5500 - val_loss: 1.9015 - val_acc: 0.4667\n",
      "Epoch 49/170\n",
      " - 0s - loss: 1.6494 - acc: 0.5500 - val_loss: 1.8787 - val_acc: 0.4667\n",
      "Epoch 50/170\n",
      " - 0s - loss: 1.6309 - acc: 0.5500 - val_loss: 1.8585 - val_acc: 0.4667\n",
      "Epoch 51/170\n",
      " - 0s - loss: 1.6126 - acc: 0.5500 - val_loss: 1.8366 - val_acc: 0.4667\n",
      "Epoch 52/170\n",
      " - 0s - loss: 1.5938 - acc: 0.5500 - val_loss: 1.8148 - val_acc: 0.4667\n",
      "Epoch 53/170\n",
      " - 0s - loss: 1.5760 - acc: 0.5500 - val_loss: 1.7949 - val_acc: 0.4667\n",
      "Epoch 54/170\n",
      " - 0s - loss: 1.5590 - acc: 0.5500 - val_loss: 1.7755 - val_acc: 0.4667\n",
      "Epoch 55/170\n",
      " - 0s - loss: 1.5417 - acc: 0.5500 - val_loss: 1.7551 - val_acc: 0.4667\n",
      "Epoch 56/170\n",
      " - 0s - loss: 1.5247 - acc: 0.5500 - val_loss: 1.7359 - val_acc: 0.4667\n",
      "Epoch 57/170\n",
      " - 0s - loss: 1.5089 - acc: 0.5500 - val_loss: 1.7181 - val_acc: 0.4667\n",
      "Epoch 58/170\n",
      " - 0s - loss: 1.4915 - acc: 0.5500 - val_loss: 1.6958 - val_acc: 0.4667\n",
      "Epoch 59/170\n",
      " - 0s - loss: 1.4733 - acc: 0.5500 - val_loss: 1.6758 - val_acc: 0.4667\n",
      "Epoch 60/170\n",
      " - 0s - loss: 1.4559 - acc: 0.5500 - val_loss: 1.6553 - val_acc: 0.4667\n",
      "Epoch 61/170\n",
      " - 0s - loss: 1.4393 - acc: 0.5500 - val_loss: 1.6368 - val_acc: 0.4667\n",
      "Epoch 62/170\n",
      " - 0s - loss: 1.4230 - acc: 0.5500 - val_loss: 1.6176 - val_acc: 0.4667\n",
      "Epoch 63/170\n",
      " - 0s - loss: 1.4066 - acc: 0.5500 - val_loss: 1.5986 - val_acc: 0.4667\n",
      "Epoch 64/170\n",
      " - 0s - loss: 1.3903 - acc: 0.5500 - val_loss: 1.5797 - val_acc: 0.4667\n",
      "Epoch 65/170\n",
      " - 0s - loss: 1.3737 - acc: 0.5500 - val_loss: 1.5599 - val_acc: 0.4667\n",
      "Epoch 66/170\n",
      " - 0s - loss: 1.3574 - acc: 0.5500 - val_loss: 1.5414 - val_acc: 0.4667\n",
      "Epoch 67/170\n",
      " - 0s - loss: 1.3408 - acc: 0.5500 - val_loss: 1.5211 - val_acc: 0.4667\n",
      "Epoch 68/170\n",
      " - 0s - loss: 1.3239 - acc: 0.5500 - val_loss: 1.5019 - val_acc: 0.4667\n",
      "Epoch 69/170\n",
      " - 0s - loss: 1.3075 - acc: 0.5500 - val_loss: 1.4826 - val_acc: 0.4667\n",
      "Epoch 70/170\n",
      " - 0s - loss: 1.2912 - acc: 0.5500 - val_loss: 1.4635 - val_acc: 0.4667\n",
      "Epoch 71/170\n",
      " - 0s - loss: 1.2751 - acc: 0.5500 - val_loss: 1.4449 - val_acc: 0.4667\n",
      "Epoch 72/170\n",
      " - 0s - loss: 1.2601 - acc: 0.5500 - val_loss: 1.4280 - val_acc: 0.4667\n",
      "Epoch 73/170\n",
      " - 0s - loss: 1.2444 - acc: 0.5500 - val_loss: 1.4081 - val_acc: 0.4667\n",
      "Epoch 74/170\n",
      " - 0s - loss: 1.2281 - acc: 0.5500 - val_loss: 1.3899 - val_acc: 0.4667\n",
      "Epoch 75/170\n",
      " - 0s - loss: 1.2129 - acc: 0.5500 - val_loss: 1.3722 - val_acc: 0.4667\n",
      "Epoch 76/170\n",
      " - 0s - loss: 1.1980 - acc: 0.5500 - val_loss: 1.3546 - val_acc: 0.4667\n",
      "Epoch 77/170\n",
      " - 0s - loss: 1.1828 - acc: 0.5500 - val_loss: 1.3364 - val_acc: 0.4667\n",
      "Epoch 78/170\n",
      " - 0s - loss: 1.1684 - acc: 0.5500 - val_loss: 1.3204 - val_acc: 0.4667\n",
      "Epoch 79/170\n",
      " - 0s - loss: 1.1550 - acc: 0.5500 - val_loss: 1.3046 - val_acc: 0.4667\n",
      "Epoch 80/170\n",
      " - 0s - loss: 1.1422 - acc: 0.5500 - val_loss: 1.2900 - val_acc: 0.4667\n",
      "Epoch 81/170\n",
      " - 0s - loss: 1.1292 - acc: 0.5500 - val_loss: 1.2742 - val_acc: 0.4667\n",
      "Epoch 82/170\n",
      " - 0s - loss: 1.1161 - acc: 0.5500 - val_loss: 1.2589 - val_acc: 0.4667\n",
      "Epoch 83/170\n",
      " - 0s - loss: 1.1033 - acc: 0.5500 - val_loss: 1.2436 - val_acc: 0.4667\n",
      "Epoch 84/170\n",
      " - 0s - loss: 1.0920 - acc: 0.5500 - val_loss: 1.2311 - val_acc: 0.4667\n",
      "Epoch 85/170\n",
      " - 0s - loss: 1.0811 - acc: 0.5500 - val_loss: 1.2184 - val_acc: 0.4667\n",
      "Epoch 86/170\n",
      " - 0s - loss: 1.0694 - acc: 0.5500 - val_loss: 1.2036 - val_acc: 0.4667\n",
      "Epoch 87/170\n",
      " - 0s - loss: 1.0588 - acc: 0.5500 - val_loss: 1.1922 - val_acc: 0.4667\n",
      "Epoch 88/170\n",
      " - 0s - loss: 1.0479 - acc: 0.5500 - val_loss: 1.1782 - val_acc: 0.4667\n",
      "Epoch 89/170\n",
      " - 0s - loss: 1.0363 - acc: 0.5500 - val_loss: 1.1640 - val_acc: 0.4667\n",
      "Epoch 90/170\n",
      " - 0s - loss: 1.0262 - acc: 0.5500 - val_loss: 1.1532 - val_acc: 0.4667\n",
      "Epoch 91/170\n",
      " - 0s - loss: 1.0164 - acc: 0.5500 - val_loss: 1.1413 - val_acc: 0.4667\n",
      "Epoch 92/170\n",
      " - 0s - loss: 1.0067 - acc: 0.5500 - val_loss: 1.1298 - val_acc: 0.4667\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.9968 - acc: 0.5500 - val_loss: 1.1173 - val_acc: 0.4667\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.9867 - acc: 0.5500 - val_loss: 1.1054 - val_acc: 0.4667\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.9767 - acc: 0.5500 - val_loss: 1.0928 - val_acc: 0.4667\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.9671 - acc: 0.5500 - val_loss: 1.0819 - val_acc: 0.4667\n",
      "Epoch 97/170\n",
      " - 0s - loss: 0.9583 - acc: 0.5500 - val_loss: 1.0714 - val_acc: 0.4667\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.9495 - acc: 0.5500 - val_loss: 1.0605 - val_acc: 0.4667\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.9403 - acc: 0.5500 - val_loss: 1.0486 - val_acc: 0.4667\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.9318 - acc: 0.5500 - val_loss: 1.0394 - val_acc: 0.4667\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.9240 - acc: 0.5500 - val_loss: 1.0299 - val_acc: 0.4667\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.9165 - acc: 0.5500 - val_loss: 1.0209 - val_acc: 0.4667\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.9088 - acc: 0.5500 - val_loss: 1.0113 - val_acc: 0.4667\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.9008 - acc: 0.5500 - val_loss: 1.0005 - val_acc: 0.4667\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.8926 - acc: 0.5500 - val_loss: 0.9912 - val_acc: 0.4667\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.8850 - acc: 0.5500 - val_loss: 0.9816 - val_acc: 0.4667\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.8782 - acc: 0.5500 - val_loss: 0.9739 - val_acc: 0.4667\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.8715 - acc: 0.5500 - val_loss: 0.9654 - val_acc: 0.4667\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.8648 - acc: 0.5500 - val_loss: 0.9572 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/170\n",
      " - 0s - loss: 0.8588 - acc: 0.5500 - val_loss: 0.9503 - val_acc: 0.4667\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.8525 - acc: 0.5500 - val_loss: 0.9421 - val_acc: 0.4667\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.8462 - acc: 0.5500 - val_loss: 0.9346 - val_acc: 0.4667\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.8404 - acc: 0.5500 - val_loss: 0.9276 - val_acc: 0.4667\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.8355 - acc: 0.5500 - val_loss: 0.9221 - val_acc: 0.4667\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.8298 - acc: 0.5500 - val_loss: 0.9147 - val_acc: 0.4667\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.8243 - acc: 0.5500 - val_loss: 0.9084 - val_acc: 0.4667\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.8187 - acc: 0.5500 - val_loss: 0.9007 - val_acc: 0.4667\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.8129 - acc: 0.5500 - val_loss: 0.8940 - val_acc: 0.4667\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.8075 - acc: 0.5500 - val_loss: 0.8874 - val_acc: 0.4667\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.8021 - acc: 0.5500 - val_loss: 0.8804 - val_acc: 0.4667\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.7973 - acc: 0.5500 - val_loss: 0.8750 - val_acc: 0.4667\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.7922 - acc: 0.5500 - val_loss: 0.8684 - val_acc: 0.4667\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.7878 - acc: 0.5500 - val_loss: 0.8638 - val_acc: 0.4667\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.7830 - acc: 0.5500 - val_loss: 0.8567 - val_acc: 0.4667\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.7778 - acc: 0.5500 - val_loss: 0.8513 - val_acc: 0.4667\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.7733 - acc: 0.5500 - val_loss: 0.8462 - val_acc: 0.4667\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.7690 - acc: 0.5500 - val_loss: 0.8411 - val_acc: 0.4667\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.7647 - acc: 0.5500 - val_loss: 0.8358 - val_acc: 0.4667\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.7602 - acc: 0.5500 - val_loss: 0.8300 - val_acc: 0.4667\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.7557 - acc: 0.5500 - val_loss: 0.8247 - val_acc: 0.4667\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.7515 - acc: 0.5500 - val_loss: 0.8199 - val_acc: 0.4667\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.7472 - acc: 0.5500 - val_loss: 0.8147 - val_acc: 0.4667\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.7430 - acc: 0.5500 - val_loss: 0.8093 - val_acc: 0.4667\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.7387 - acc: 0.5500 - val_loss: 0.8043 - val_acc: 0.4667\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.7347 - acc: 0.5500 - val_loss: 0.7999 - val_acc: 0.4667\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.7307 - acc: 0.5500 - val_loss: 0.7951 - val_acc: 0.4667\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.7268 - acc: 0.5500 - val_loss: 0.7907 - val_acc: 0.4667\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.7232 - acc: 0.5500 - val_loss: 0.7868 - val_acc: 0.4667\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.7194 - acc: 0.5500 - val_loss: 0.7824 - val_acc: 0.4667\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.7157 - acc: 0.5500 - val_loss: 0.7769 - val_acc: 0.4667\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.7113 - acc: 0.5500 - val_loss: 0.7721 - val_acc: 0.4667\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.7073 - acc: 0.5500 - val_loss: 0.7675 - val_acc: 0.4667\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.7037 - acc: 0.5500 - val_loss: 0.7621 - val_acc: 0.4667\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.6997 - acc: 0.5500 - val_loss: 0.7583 - val_acc: 0.4667\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.6960 - acc: 0.5500 - val_loss: 0.7534 - val_acc: 0.4667\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.6921 - acc: 0.5500 - val_loss: 0.7488 - val_acc: 0.4667\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.6887 - acc: 0.5500 - val_loss: 0.7452 - val_acc: 0.4667\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.6853 - acc: 0.5500 - val_loss: 0.7417 - val_acc: 0.4667\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.6818 - acc: 0.5500 - val_loss: 0.7381 - val_acc: 0.4667\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.6788 - acc: 0.5500 - val_loss: 0.7351 - val_acc: 0.4667\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.6758 - acc: 0.5500 - val_loss: 0.7324 - val_acc: 0.4667\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.6725 - acc: 0.5500 - val_loss: 0.7289 - val_acc: 0.4667\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.6693 - acc: 0.5500 - val_loss: 0.7255 - val_acc: 0.4667\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.6661 - acc: 0.5500 - val_loss: 0.7222 - val_acc: 0.4667\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.6630 - acc: 0.5500 - val_loss: 0.7186 - val_acc: 0.4667\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.6597 - acc: 0.5500 - val_loss: 0.7145 - val_acc: 0.4667\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.6564 - acc: 0.5500 - val_loss: 0.7111 - val_acc: 0.4667\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.6532 - acc: 0.5500 - val_loss: 0.7077 - val_acc: 0.4667\n",
      "Epoch 159/170\n",
      " - 0s - loss: 0.6500 - acc: 0.5500 - val_loss: 0.7041 - val_acc: 0.4667\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.6468 - acc: 0.5500 - val_loss: 0.7004 - val_acc: 0.4667\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.6438 - acc: 0.5500 - val_loss: 0.6973 - val_acc: 0.4667\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.6407 - acc: 0.5500 - val_loss: 0.6940 - val_acc: 0.4667\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.6377 - acc: 0.5500 - val_loss: 0.6908 - val_acc: 0.4667\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.6346 - acc: 0.5500 - val_loss: 0.6873 - val_acc: 0.4667\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.6316 - acc: 0.5500 - val_loss: 0.6843 - val_acc: 0.4667\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.6286 - acc: 0.5500 - val_loss: 0.6808 - val_acc: 0.4667\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.6255 - acc: 0.5500 - val_loss: 0.6770 - val_acc: 0.4667\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.6225 - acc: 0.5500 - val_loss: 0.6740 - val_acc: 0.4667\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.6195 - acc: 0.5500 - val_loss: 0.6702 - val_acc: 0.4667\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.6163 - acc: 0.5500 - val_loss: 0.6668 - val_acc: 0.4667\n",
      "Test accuracy: 0.4666666626930237\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 2.2162 - acc: 0.5500 - val_loss: 2.5267 - val_acc: 0.4667\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.1981 - acc: 0.5500 - val_loss: 2.5105 - val_acc: 0.4667\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.1840 - acc: 0.5500 - val_loss: 2.4935 - val_acc: 0.4667\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.1692 - acc: 0.5500 - val_loss: 2.4754 - val_acc: 0.4667\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.1533 - acc: 0.5500 - val_loss: 2.4562 - val_acc: 0.4667\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.1367 - acc: 0.5500 - val_loss: 2.4365 - val_acc: 0.4667\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.1195 - acc: 0.5500 - val_loss: 2.4160 - val_acc: 0.4667\n",
      "Epoch 8/100\n",
      " - 0s - loss: 2.1021 - acc: 0.5500 - val_loss: 2.3960 - val_acc: 0.4667\n",
      "Epoch 9/100\n",
      " - 0s - loss: 2.0851 - acc: 0.5500 - val_loss: 2.3764 - val_acc: 0.4667\n",
      "Epoch 10/100\n",
      " - 0s - loss: 2.0669 - acc: 0.5500 - val_loss: 2.3537 - val_acc: 0.4667\n",
      "Epoch 11/100\n",
      " - 0s - loss: 2.0481 - acc: 0.5500 - val_loss: 2.3328 - val_acc: 0.4667\n",
      "Epoch 12/100\n",
      " - 0s - loss: 2.0288 - acc: 0.5500 - val_loss: 2.3090 - val_acc: 0.4667\n",
      "Epoch 13/100\n",
      " - 0s - loss: 2.0089 - acc: 0.5500 - val_loss: 2.2868 - val_acc: 0.4667\n",
      "Epoch 14/100\n",
      " - 0s - loss: 1.9901 - acc: 0.5500 - val_loss: 2.2655 - val_acc: 0.4667\n",
      "Epoch 15/100\n",
      " - 0s - loss: 1.9708 - acc: 0.5500 - val_loss: 2.2423 - val_acc: 0.4667\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.9515 - acc: 0.5500 - val_loss: 2.2207 - val_acc: 0.4667\n",
      "Epoch 17/100\n",
      " - 0s - loss: 1.9337 - acc: 0.5500 - val_loss: 2.2010 - val_acc: 0.4667\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.9163 - acc: 0.5500 - val_loss: 2.1807 - val_acc: 0.4667\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.8977 - acc: 0.5500 - val_loss: 2.1584 - val_acc: 0.4667\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.8785 - acc: 0.5500 - val_loss: 2.1362 - val_acc: 0.4667\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.8593 - acc: 0.5500 - val_loss: 2.1139 - val_acc: 0.4667\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.8399 - acc: 0.5500 - val_loss: 2.0915 - val_acc: 0.4667\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.8209 - acc: 0.5500 - val_loss: 2.0699 - val_acc: 0.4667\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.8023 - acc: 0.5500 - val_loss: 2.0484 - val_acc: 0.4667\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.7831 - acc: 0.5500 - val_loss: 2.0259 - val_acc: 0.4667\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.7637 - acc: 0.5500 - val_loss: 2.0035 - val_acc: 0.4667\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.7443 - acc: 0.5500 - val_loss: 1.9811 - val_acc: 0.4667\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.7249 - acc: 0.5500 - val_loss: 1.9586 - val_acc: 0.4667\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.7068 - acc: 0.5500 - val_loss: 1.9389 - val_acc: 0.4667\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.6903 - acc: 0.5500 - val_loss: 1.9205 - val_acc: 0.4667\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.6733 - acc: 0.5500 - val_loss: 1.9006 - val_acc: 0.4667\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.6545 - acc: 0.5500 - val_loss: 1.8768 - val_acc: 0.4667\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.6351 - acc: 0.5500 - val_loss: 1.8558 - val_acc: 0.4667\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.6160 - acc: 0.5500 - val_loss: 1.8329 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      " - 0s - loss: 1.5965 - acc: 0.5500 - val_loss: 1.8108 - val_acc: 0.4667\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.5771 - acc: 0.5500 - val_loss: 1.7880 - val_acc: 0.4667\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.5567 - acc: 0.5500 - val_loss: 1.7634 - val_acc: 0.4667\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.5362 - acc: 0.5500 - val_loss: 1.7409 - val_acc: 0.4667\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.5170 - acc: 0.5500 - val_loss: 1.7189 - val_acc: 0.4667\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.4987 - acc: 0.5500 - val_loss: 1.6985 - val_acc: 0.4667\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.4813 - acc: 0.5500 - val_loss: 1.6788 - val_acc: 0.4667\n",
      "Epoch 42/100\n",
      " - 0s - loss: 1.4636 - acc: 0.5500 - val_loss: 1.6582 - val_acc: 0.4667\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.4451 - acc: 0.5500 - val_loss: 1.6362 - val_acc: 0.4667\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.4260 - acc: 0.5500 - val_loss: 1.6140 - val_acc: 0.4667\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.4076 - acc: 0.5500 - val_loss: 1.5935 - val_acc: 0.4667\n",
      "Epoch 46/100\n",
      " - 0s - loss: 1.3899 - acc: 0.5500 - val_loss: 1.5732 - val_acc: 0.4667\n",
      "Epoch 47/100\n",
      " - 0s - loss: 1.3725 - acc: 0.5500 - val_loss: 1.5533 - val_acc: 0.4667\n",
      "Epoch 48/100\n",
      " - 0s - loss: 1.3552 - acc: 0.5500 - val_loss: 1.5334 - val_acc: 0.4667\n",
      "Epoch 49/100\n",
      " - 0s - loss: 1.3384 - acc: 0.5500 - val_loss: 1.5144 - val_acc: 0.4667\n",
      "Epoch 50/100\n",
      " - 0s - loss: 1.3213 - acc: 0.5500 - val_loss: 1.4941 - val_acc: 0.4667\n",
      "Epoch 51/100\n",
      " - 0s - loss: 1.3046 - acc: 0.5500 - val_loss: 1.4757 - val_acc: 0.4667\n",
      "Epoch 52/100\n",
      " - 0s - loss: 1.2885 - acc: 0.5500 - val_loss: 1.4571 - val_acc: 0.4667\n",
      "Epoch 53/100\n",
      " - 0s - loss: 1.2726 - acc: 0.5500 - val_loss: 1.4390 - val_acc: 0.4667\n",
      "Epoch 54/100\n",
      " - 0s - loss: 1.2558 - acc: 0.5500 - val_loss: 1.4185 - val_acc: 0.4667\n",
      "Epoch 55/100\n",
      " - 0s - loss: 1.2381 - acc: 0.5500 - val_loss: 1.3979 - val_acc: 0.4667\n",
      "Epoch 56/100\n",
      " - 0s - loss: 1.2211 - acc: 0.5500 - val_loss: 1.3790 - val_acc: 0.4667\n",
      "Epoch 57/100\n",
      " - 0s - loss: 1.2047 - acc: 0.5500 - val_loss: 1.3602 - val_acc: 0.4667\n",
      "Epoch 58/100\n",
      " - 0s - loss: 1.1877 - acc: 0.5500 - val_loss: 1.3397 - val_acc: 0.4667\n",
      "Epoch 59/100\n",
      " - 0s - loss: 1.1714 - acc: 0.5500 - val_loss: 1.3219 - val_acc: 0.4667\n",
      "Epoch 60/100\n",
      " - 0s - loss: 1.1556 - acc: 0.5500 - val_loss: 1.3036 - val_acc: 0.4667\n",
      "Epoch 61/100\n",
      " - 0s - loss: 1.1395 - acc: 0.5500 - val_loss: 1.2846 - val_acc: 0.4667\n",
      "Epoch 62/100\n",
      " - 0s - loss: 1.1240 - acc: 0.5500 - val_loss: 1.2674 - val_acc: 0.4667\n",
      "Epoch 63/100\n",
      " - 0s - loss: 1.1084 - acc: 0.5500 - val_loss: 1.2489 - val_acc: 0.4667\n",
      "Epoch 64/100\n",
      " - 0s - loss: 1.0927 - acc: 0.5500 - val_loss: 1.2310 - val_acc: 0.4667\n",
      "Epoch 65/100\n",
      " - 0s - loss: 1.0770 - acc: 0.5500 - val_loss: 1.2125 - val_acc: 0.4667\n",
      "Epoch 66/100\n",
      " - 0s - loss: 1.0612 - acc: 0.5500 - val_loss: 1.1942 - val_acc: 0.4667\n",
      "Epoch 67/100\n",
      " - 0s - loss: 1.0454 - acc: 0.5500 - val_loss: 1.1759 - val_acc: 0.4667\n",
      "Epoch 68/100\n",
      " - 0s - loss: 1.0292 - acc: 0.5500 - val_loss: 1.1565 - val_acc: 0.4667\n",
      "Epoch 69/100\n",
      " - 0s - loss: 1.0141 - acc: 0.5500 - val_loss: 1.1404 - val_acc: 0.4667\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.9991 - acc: 0.5500 - val_loss: 1.1220 - val_acc: 0.4667\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.9832 - acc: 0.5500 - val_loss: 1.1031 - val_acc: 0.4667\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.9679 - acc: 0.5500 - val_loss: 1.0864 - val_acc: 0.4667\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.9538 - acc: 0.5500 - val_loss: 1.0703 - val_acc: 0.4667\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.9396 - acc: 0.5500 - val_loss: 1.0535 - val_acc: 0.4667\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.9258 - acc: 0.5500 - val_loss: 1.0379 - val_acc: 0.4667\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.9126 - acc: 0.5500 - val_loss: 1.0226 - val_acc: 0.4667\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.8987 - acc: 0.5500 - val_loss: 1.0055 - val_acc: 0.4667\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.8844 - acc: 0.5500 - val_loss: 0.9892 - val_acc: 0.4667\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.8722 - acc: 0.5500 - val_loss: 0.9760 - val_acc: 0.4667\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.8593 - acc: 0.5500 - val_loss: 0.9597 - val_acc: 0.4667\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.8461 - acc: 0.5500 - val_loss: 0.9450 - val_acc: 0.4667\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.8337 - acc: 0.5500 - val_loss: 0.9305 - val_acc: 0.4667\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.8219 - acc: 0.5500 - val_loss: 0.9171 - val_acc: 0.4667\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.8100 - acc: 0.5500 - val_loss: 0.9027 - val_acc: 0.4667\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7980 - acc: 0.5500 - val_loss: 0.8884 - val_acc: 0.4667\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.7867 - acc: 0.5500 - val_loss: 0.8758 - val_acc: 0.4667\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.7763 - acc: 0.5500 - val_loss: 0.8638 - val_acc: 0.4667\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.7669 - acc: 0.5500 - val_loss: 0.8532 - val_acc: 0.4667\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.7572 - acc: 0.5500 - val_loss: 0.8414 - val_acc: 0.4667\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.7476 - acc: 0.5500 - val_loss: 0.8303 - val_acc: 0.4667\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.7377 - acc: 0.5500 - val_loss: 0.8176 - val_acc: 0.4667\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.7280 - acc: 0.5500 - val_loss: 0.8069 - val_acc: 0.4667\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.7187 - acc: 0.5500 - val_loss: 0.7955 - val_acc: 0.4667\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.7093 - acc: 0.5500 - val_loss: 0.7842 - val_acc: 0.4667\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.7000 - acc: 0.5500 - val_loss: 0.7731 - val_acc: 0.4667\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6911 - acc: 0.5500 - val_loss: 0.7625 - val_acc: 0.4667\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6826 - acc: 0.5500 - val_loss: 0.7525 - val_acc: 0.4667\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6747 - acc: 0.5500 - val_loss: 0.7431 - val_acc: 0.4667\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6669 - acc: 0.5500 - val_loss: 0.7338 - val_acc: 0.4667\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6597 - acc: 0.5500 - val_loss: 0.7254 - val_acc: 0.4667\n",
      "Test accuracy: 0.4666666626930237\n",
      "Evalutation of best performing model:\n",
      "60/60 [==============================] - 0s 17us/step\n",
      "[0.2943368494510651, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    Make sure to have every relevant import statement included here and return data as\n",
    "    used in model function below. This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    url = \"../data/iris.csv\"\n",
    "    data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "    class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "    data.iloc[:,-1] = index\n",
    "    data = data.loc[data[4] != 2]\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=4, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer={{choice(['adam', 'nadam'])}},\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([10, 30])}},\n",
    "              epochs={{choice([100, 170])}},\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='experiment')\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hyperas(Linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils.validation import column_or_1d\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import copy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import datasets\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from automation_script import get_dataset_info\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.datasets import make_regression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano.tensor as T\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from theano.compile.ops import as_op\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [10, 30]),\n",
      "        'epochs': hp.choice('epochs', [100, 170]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: Make sure to have every relevant import statement included here and return data as\n",
      "  6: used in model function below. This function is separated from model() so that hyperopt\n",
      "  7: won't reload data for each evaluation run.\n",
      "  8: '''\n",
      "  9: from os import path\n",
      " 10: import pandas as pd\n",
      " 11: from sklearn import preprocessing\n",
      " 12: from sklearn.preprocessing import StandardScaler\n",
      " 13: \n",
      " 14: url = \"../data/diabetes.csv\"\n",
      " 15: data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
      " 16: sc = StandardScaler()\n",
      " 17: data = sc.fit_transform(data)\n",
      " 18: data = pd.DataFrame(data)\n",
      " 19: \n",
      " 20: \n",
      " 21: X = data.iloc[:,:-1]\n",
      " 22: Y = data.iloc[:,-1]\n",
      " 23: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
      " 24: \n",
      " 25: \n",
      " 26: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13: \n",
      "  14:     model = Sequential()\n",
      "  15:     model.add(Dense(1, input_dim=10, activation='linear'))\n",
      "  16: \n",
      "  17:     model.compile(loss='mse', optimizer=space['optimizer'])\n",
      "  18: \n",
      "  19:     model.fit(x_train, y_train,\n",
      "  20:               batch_size=space['batch_size'],\n",
      "  21:               epochs=space['epochs'],\n",
      "  22:               verbose=2,\n",
      "  23:               validation_data=(x_test, y_test))\n",
      "  24:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  25:     print('Test accuracy:', acc)\n",
      "  26:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  27: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 1.6659 - acc: 0.0000e+00 - val_loss: 1.5142 - val_acc: 0.0000e+00\n",
      "Epoch 2/170\n",
      " - 0s - loss: 1.6094 - acc: 0.0000e+00 - val_loss: 1.4674 - val_acc: 0.0000e+00\n",
      "Epoch 3/170\n",
      " - 0s - loss: 1.5604 - acc: 0.0000e+00 - val_loss: 1.4252 - val_acc: 0.0000e+00\n",
      "Epoch 4/170\n",
      " - 0s - loss: 1.5204 - acc: 0.0000e+00 - val_loss: 1.3813 - val_acc: 0.0000e+00\n",
      "Epoch 5/170\n",
      " - 0s - loss: 1.4753 - acc: 0.0000e+00 - val_loss: 1.3462 - val_acc: 0.0000e+00\n",
      "Epoch 6/170\n",
      " - 0s - loss: 1.4360 - acc: 0.0000e+00 - val_loss: 1.3165 - val_acc: 0.0000e+00\n",
      "Epoch 7/170\n",
      " - 0s - loss: 1.3996 - acc: 0.0000e+00 - val_loss: 1.2852 - val_acc: 0.0000e+00\n",
      "Epoch 8/170\n",
      " - 0s - loss: 1.3632 - acc: 0.0000e+00 - val_loss: 1.2518 - val_acc: 0.0000e+00\n",
      "Epoch 9/170\n",
      " - 0s - loss: 1.3302 - acc: 0.0000e+00 - val_loss: 1.2223 - val_acc: 0.0000e+00\n",
      "Epoch 10/170\n",
      " - 0s - loss: 1.2957 - acc: 0.0000e+00 - val_loss: 1.1963 - val_acc: 0.0000e+00\n",
      "Epoch 11/170\n",
      " - 0s - loss: 1.2650 - acc: 0.0000e+00 - val_loss: 1.1693 - val_acc: 0.0000e+00\n",
      "Epoch 12/170\n",
      " - 0s - loss: 1.2356 - acc: 0.0000e+00 - val_loss: 1.1414 - val_acc: 0.0000e+00\n",
      "Epoch 13/170\n",
      " - 0s - loss: 1.2079 - acc: 0.0000e+00 - val_loss: 1.1171 - val_acc: 0.0000e+00\n",
      "Epoch 14/170\n",
      " - 0s - loss: 1.1778 - acc: 0.0000e+00 - val_loss: 1.0964 - val_acc: 0.0000e+00\n",
      "Epoch 15/170\n",
      " - 0s - loss: 1.1536 - acc: 0.0000e+00 - val_loss: 1.0736 - val_acc: 0.0000e+00\n",
      "Epoch 16/170\n",
      " - 0s - loss: 1.1273 - acc: 0.0000e+00 - val_loss: 1.0521 - val_acc: 0.0000e+00\n",
      "Epoch 17/170\n",
      " - 0s - loss: 1.1030 - acc: 0.0000e+00 - val_loss: 1.0333 - val_acc: 0.0000e+00\n",
      "Epoch 18/170\n",
      " - 0s - loss: 1.0797 - acc: 0.0000e+00 - val_loss: 1.0138 - val_acc: 0.0000e+00\n",
      "Epoch 19/170\n",
      " - 0s - loss: 1.0573 - acc: 0.0000e+00 - val_loss: 0.9942 - val_acc: 0.0000e+00\n",
      "Epoch 20/170\n",
      " - 0s - loss: 1.0365 - acc: 0.0000e+00 - val_loss: 0.9776 - val_acc: 0.0000e+00\n",
      "Epoch 21/170\n",
      " - 0s - loss: 1.0154 - acc: 0.0000e+00 - val_loss: 0.9591 - val_acc: 0.0000e+00\n",
      "Epoch 22/170\n",
      " - 0s - loss: 0.9958 - acc: 0.0000e+00 - val_loss: 0.9389 - val_acc: 0.0000e+00\n",
      "Epoch 23/170\n",
      " - 0s - loss: 0.9760 - acc: 0.0000e+00 - val_loss: 0.9232 - val_acc: 0.0000e+00\n",
      "Epoch 24/170\n",
      " - 0s - loss: 0.9569 - acc: 0.0000e+00 - val_loss: 0.9071 - val_acc: 0.0000e+00\n",
      "Epoch 25/170\n",
      " - 0s - loss: 0.9396 - acc: 0.0000e+00 - val_loss: 0.8925 - val_acc: 0.0000e+00\n",
      "Epoch 26/170\n",
      " - 0s - loss: 0.9217 - acc: 0.0000e+00 - val_loss: 0.8776 - val_acc: 0.0000e+00\n",
      "Epoch 27/170\n",
      " - 0s - loss: 0.9045 - acc: 0.0000e+00 - val_loss: 0.8651 - val_acc: 0.0000e+00\n",
      "Epoch 28/170\n",
      " - 0s - loss: 0.8893 - acc: 0.0000e+00 - val_loss: 0.8498 - val_acc: 0.0000e+00\n",
      "Epoch 29/170\n",
      " - 0s - loss: 0.8741 - acc: 0.0000e+00 - val_loss: 0.8364 - val_acc: 0.0000e+00\n",
      "Epoch 30/170\n",
      " - 0s - loss: 0.8589 - acc: 0.0000e+00 - val_loss: 0.8254 - val_acc: 0.0000e+00\n",
      "Epoch 31/170\n",
      " - 0s - loss: 0.8440 - acc: 0.0000e+00 - val_loss: 0.8124 - val_acc: 0.0000e+00\n",
      "Epoch 32/170\n",
      " - 0s - loss: 0.8306 - acc: 0.0000e+00 - val_loss: 0.7989 - val_acc: 0.0000e+00\n",
      "Epoch 33/170\n",
      " - 0s - loss: 0.8164 - acc: 0.0000e+00 - val_loss: 0.7878 - val_acc: 0.0000e+00\n",
      "Epoch 34/170\n",
      " - 0s - loss: 0.8037 - acc: 0.0000e+00 - val_loss: 0.7759 - val_acc: 0.0000e+00\n",
      "Epoch 35/170\n",
      " - 0s - loss: 0.7916 - acc: 0.0000e+00 - val_loss: 0.7653 - val_acc: 0.0000e+00\n",
      "Epoch 36/170\n",
      " - 0s - loss: 0.7790 - acc: 0.0000e+00 - val_loss: 0.7573 - val_acc: 0.0000e+00\n",
      "Epoch 37/170\n",
      " - 0s - loss: 0.7675 - acc: 0.0000e+00 - val_loss: 0.7474 - val_acc: 0.0000e+00\n",
      "Epoch 38/170\n",
      " - 0s - loss: 0.7564 - acc: 0.0000e+00 - val_loss: 0.7361 - val_acc: 0.0000e+00\n",
      "Epoch 39/170\n",
      " - 0s - loss: 0.7450 - acc: 0.0000e+00 - val_loss: 0.7267 - val_acc: 0.0000e+00\n",
      "Epoch 40/170\n",
      " - 0s - loss: 0.7349 - acc: 0.0000e+00 - val_loss: 0.7191 - val_acc: 0.0000e+00\n",
      "Epoch 41/170\n",
      " - 0s - loss: 0.7244 - acc: 0.0000e+00 - val_loss: 0.7100 - val_acc: 0.0000e+00\n",
      "Epoch 42/170\n",
      " - 0s - loss: 0.7147 - acc: 0.0000e+00 - val_loss: 0.7014 - val_acc: 0.0000e+00\n",
      "Epoch 43/170\n",
      " - 0s - loss: 0.7063 - acc: 0.0000e+00 - val_loss: 0.6924 - val_acc: 0.0000e+00\n",
      "Epoch 44/170\n",
      " - 0s - loss: 0.6958 - acc: 0.0000e+00 - val_loss: 0.6877 - val_acc: 0.0000e+00\n",
      "Epoch 45/170\n",
      " - 0s - loss: 0.6875 - acc: 0.0000e+00 - val_loss: 0.6810 - val_acc: 0.0000e+00\n",
      "Epoch 46/170\n",
      " - 0s - loss: 0.6791 - acc: 0.0000e+00 - val_loss: 0.6740 - val_acc: 0.0000e+00\n",
      "Epoch 47/170\n",
      " - 0s - loss: 0.6702 - acc: 0.0000e+00 - val_loss: 0.6662 - val_acc: 0.0000e+00\n",
      "Epoch 48/170\n",
      " - 0s - loss: 0.6623 - acc: 0.0000e+00 - val_loss: 0.6592 - val_acc: 0.0000e+00\n",
      "Epoch 49/170\n",
      " - 0s - loss: 0.6549 - acc: 0.0000e+00 - val_loss: 0.6542 - val_acc: 0.0000e+00\n",
      "Epoch 50/170\n",
      " - 0s - loss: 0.6475 - acc: 0.0000e+00 - val_loss: 0.6479 - val_acc: 0.0000e+00\n",
      "Epoch 51/170\n",
      " - 0s - loss: 0.6406 - acc: 0.0000e+00 - val_loss: 0.6424 - val_acc: 0.0000e+00\n",
      "Epoch 52/170\n",
      " - 0s - loss: 0.6333 - acc: 0.0000e+00 - val_loss: 0.6384 - val_acc: 0.0000e+00\n",
      "Epoch 53/170\n",
      " - 0s - loss: 0.6274 - acc: 0.0000e+00 - val_loss: 0.6336 - val_acc: 0.0000e+00\n",
      "Epoch 54/170\n",
      " - 0s - loss: 0.6213 - acc: 0.0000e+00 - val_loss: 0.6271 - val_acc: 0.0000e+00\n",
      "Epoch 55/170\n",
      " - 0s - loss: 0.6146 - acc: 0.0000e+00 - val_loss: 0.6228 - val_acc: 0.0000e+00\n",
      "Epoch 56/170\n",
      " - 0s - loss: 0.6087 - acc: 0.0000e+00 - val_loss: 0.6168 - val_acc: 0.0000e+00\n",
      "Epoch 57/170\n",
      " - 0s - loss: 0.6027 - acc: 0.0000e+00 - val_loss: 0.6127 - val_acc: 0.0000e+00\n",
      "Epoch 58/170\n",
      " - 0s - loss: 0.5972 - acc: 0.0000e+00 - val_loss: 0.6092 - val_acc: 0.0000e+00\n",
      "Epoch 59/170\n",
      " - 0s - loss: 0.5920 - acc: 0.0000e+00 - val_loss: 0.6043 - val_acc: 0.0000e+00\n",
      "Epoch 60/170\n",
      " - 0s - loss: 0.5866 - acc: 0.0000e+00 - val_loss: 0.6005 - val_acc: 0.0000e+00\n",
      "Epoch 61/170\n",
      " - 0s - loss: 0.5813 - acc: 0.0000e+00 - val_loss: 0.5970 - val_acc: 0.0000e+00\n",
      "Epoch 62/170\n",
      " - 0s - loss: 0.5769 - acc: 0.0000e+00 - val_loss: 0.5940 - val_acc: 0.0000e+00\n",
      "Epoch 63/170\n",
      " - 0s - loss: 0.5719 - acc: 0.0000e+00 - val_loss: 0.5907 - val_acc: 0.0000e+00\n",
      "Epoch 64/170\n",
      " - 0s - loss: 0.5675 - acc: 0.0000e+00 - val_loss: 0.5877 - val_acc: 0.0000e+00\n",
      "Epoch 65/170\n",
      " - 0s - loss: 0.5627 - acc: 0.0000e+00 - val_loss: 0.5847 - val_acc: 0.0000e+00\n",
      "Epoch 66/170\n",
      " - 0s - loss: 0.5592 - acc: 0.0000e+00 - val_loss: 0.5815 - val_acc: 0.0000e+00\n",
      "Epoch 67/170\n",
      " - 0s - loss: 0.5546 - acc: 0.0000e+00 - val_loss: 0.5796 - val_acc: 0.0000e+00\n",
      "Epoch 68/170\n",
      " - 0s - loss: 0.5510 - acc: 0.0000e+00 - val_loss: 0.5770 - val_acc: 0.0000e+00\n",
      "Epoch 69/170\n",
      " - 0s - loss: 0.5473 - acc: 0.0000e+00 - val_loss: 0.5758 - val_acc: 0.0000e+00\n",
      "Epoch 70/170\n",
      " - 0s - loss: 0.5439 - acc: 0.0000e+00 - val_loss: 0.5723 - val_acc: 0.0000e+00\n",
      "Epoch 71/170\n",
      " - 0s - loss: 0.5394 - acc: 0.0000e+00 - val_loss: 0.5701 - val_acc: 0.0000e+00\n",
      "Epoch 72/170\n",
      " - 0s - loss: 0.5368 - acc: 0.0000e+00 - val_loss: 0.5685 - val_acc: 0.0000e+00\n",
      "Epoch 73/170\n",
      " - 0s - loss: 0.5329 - acc: 0.0000e+00 - val_loss: 0.5669 - val_acc: 0.0000e+00\n",
      "Epoch 74/170\n",
      " - 0s - loss: 0.5301 - acc: 0.0000e+00 - val_loss: 0.5640 - val_acc: 0.0000e+00\n",
      "Epoch 75/170\n",
      " - 0s - loss: 0.5271 - acc: 0.0000e+00 - val_loss: 0.5629 - val_acc: 0.0000e+00\n",
      "Epoch 76/170\n",
      " - 0s - loss: 0.5238 - acc: 0.0000e+00 - val_loss: 0.5614 - val_acc: 0.0000e+00\n",
      "Epoch 77/170\n",
      " - 0s - loss: 0.5214 - acc: 0.0000e+00 - val_loss: 0.5586 - val_acc: 0.0000e+00\n",
      "Epoch 78/170\n",
      " - 0s - loss: 0.5193 - acc: 0.0000e+00 - val_loss: 0.5578 - val_acc: 0.0000e+00\n",
      "Epoch 79/170\n",
      " - 0s - loss: 0.5163 - acc: 0.0000e+00 - val_loss: 0.5559 - val_acc: 0.0000e+00\n",
      "Epoch 80/170\n",
      " - 0s - loss: 0.5134 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 81/170\n",
      " - 0s - loss: 0.5112 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 82/170\n",
      " - 0s - loss: 0.5083 - acc: 0.0000e+00 - val_loss: 0.5533 - val_acc: 0.0000e+00\n",
      "Epoch 83/170\n",
      " - 0s - loss: 0.5062 - acc: 0.0000e+00 - val_loss: 0.5522 - val_acc: 0.0000e+00\n",
      "Epoch 84/170\n",
      " - 0s - loss: 0.5039 - acc: 0.0000e+00 - val_loss: 0.5512 - val_acc: 0.0000e+00\n",
      "Epoch 85/170\n",
      " - 0s - loss: 0.5019 - acc: 0.0000e+00 - val_loss: 0.5497 - val_acc: 0.0000e+00\n",
      "Epoch 86/170\n",
      " - 0s - loss: 0.5004 - acc: 0.0000e+00 - val_loss: 0.5487 - val_acc: 0.0000e+00\n",
      "Epoch 87/170\n",
      " - 0s - loss: 0.4983 - acc: 0.0000e+00 - val_loss: 0.5486 - val_acc: 0.0000e+00\n",
      "Epoch 88/170\n",
      " - 0s - loss: 0.4968 - acc: 0.0000e+00 - val_loss: 0.5480 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/170\n",
      " - 0s - loss: 0.4939 - acc: 0.0000e+00 - val_loss: 0.5473 - val_acc: 0.0000e+00\n",
      "Epoch 90/170\n",
      " - 0s - loss: 0.4932 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n",
      "Epoch 91/170\n",
      " - 0s - loss: 0.4908 - acc: 0.0000e+00 - val_loss: 0.5469 - val_acc: 0.0000e+00\n",
      "Epoch 92/170\n",
      " - 0s - loss: 0.4895 - acc: 0.0000e+00 - val_loss: 0.5461 - val_acc: 0.0000e+00\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.4880 - acc: 0.0000e+00 - val_loss: 0.5459 - val_acc: 0.0000e+00\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.4863 - acc: 0.0000e+00 - val_loss: 0.5453 - val_acc: 0.0000e+00\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.4846 - acc: 0.0000e+00 - val_loss: 0.5446 - val_acc: 0.0000e+00\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.4838 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 97/170\n",
      " - 0s - loss: 0.4822 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.4813 - acc: 0.0000e+00 - val_loss: 0.5443 - val_acc: 0.0000e+00\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.4800 - acc: 0.0000e+00 - val_loss: 0.5433 - val_acc: 0.0000e+00\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.4791 - acc: 0.0000e+00 - val_loss: 0.5433 - val_acc: 0.0000e+00\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.4773 - acc: 0.0000e+00 - val_loss: 0.5428 - val_acc: 0.0000e+00\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.4764 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.4756 - acc: 0.0000e+00 - val_loss: 0.5430 - val_acc: 0.0000e+00\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.4742 - acc: 0.0000e+00 - val_loss: 0.5425 - val_acc: 0.0000e+00\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.4735 - acc: 0.0000e+00 - val_loss: 0.5430 - val_acc: 0.0000e+00\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.4726 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.4712 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.4709 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.4697 - acc: 0.0000e+00 - val_loss: 0.5435 - val_acc: 0.0000e+00\n",
      "Epoch 110/170\n",
      " - 0s - loss: 0.4694 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.4686 - acc: 0.0000e+00 - val_loss: 0.5434 - val_acc: 0.0000e+00\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.4674 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.4672 - acc: 0.0000e+00 - val_loss: 0.5439 - val_acc: 0.0000e+00\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.4662 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.4656 - acc: 0.0000e+00 - val_loss: 0.5443 - val_acc: 0.0000e+00\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.4650 - acc: 0.0000e+00 - val_loss: 0.5446 - val_acc: 0.0000e+00\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.4653 - acc: 0.0000e+00 - val_loss: 0.5448 - val_acc: 0.0000e+00\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.4638 - acc: 0.0000e+00 - val_loss: 0.5456 - val_acc: 0.0000e+00\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.4636 - acc: 0.0000e+00 - val_loss: 0.5446 - val_acc: 0.0000e+00\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.4627 - acc: 0.0000e+00 - val_loss: 0.5452 - val_acc: 0.0000e+00\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.4624 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.4623 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.4617 - acc: 0.0000e+00 - val_loss: 0.5464 - val_acc: 0.0000e+00\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.4611 - acc: 0.0000e+00 - val_loss: 0.5469 - val_acc: 0.0000e+00\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.4607 - acc: 0.0000e+00 - val_loss: 0.5468 - val_acc: 0.0000e+00\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.4604 - acc: 0.0000e+00 - val_loss: 0.5479 - val_acc: 0.0000e+00\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.4604 - acc: 0.0000e+00 - val_loss: 0.5478 - val_acc: 0.0000e+00\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.4600 - acc: 0.0000e+00 - val_loss: 0.5489 - val_acc: 0.0000e+00\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.4588 - acc: 0.0000e+00 - val_loss: 0.5486 - val_acc: 0.0000e+00\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.4594 - acc: 0.0000e+00 - val_loss: 0.5487 - val_acc: 0.0000e+00\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.4581 - acc: 0.0000e+00 - val_loss: 0.5495 - val_acc: 0.0000e+00\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.4577 - acc: 0.0000e+00 - val_loss: 0.5498 - val_acc: 0.0000e+00\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.4574 - acc: 0.0000e+00 - val_loss: 0.5498 - val_acc: 0.0000e+00\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.4573 - acc: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.0000e+00\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.4573 - acc: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.0000e+00\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.4568 - acc: 0.0000e+00 - val_loss: 0.5508 - val_acc: 0.0000e+00\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.4572 - acc: 0.0000e+00 - val_loss: 0.5513 - val_acc: 0.0000e+00\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.4562 - acc: 0.0000e+00 - val_loss: 0.5517 - val_acc: 0.0000e+00\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5520 - val_acc: 0.0000e+00\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.4556 - acc: 0.0000e+00 - val_loss: 0.5526 - val_acc: 0.0000e+00\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.4564 - acc: 0.0000e+00 - val_loss: 0.5520 - val_acc: 0.0000e+00\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.4551 - acc: 0.0000e+00 - val_loss: 0.5530 - val_acc: 0.0000e+00\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.4554 - acc: 0.0000e+00 - val_loss: 0.5533 - val_acc: 0.0000e+00\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.4547 - acc: 0.0000e+00 - val_loss: 0.5537 - val_acc: 0.0000e+00\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.4549 - acc: 0.0000e+00 - val_loss: 0.5536 - val_acc: 0.0000e+00\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.4546 - acc: 0.0000e+00 - val_loss: 0.5544 - val_acc: 0.0000e+00\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.4543 - acc: 0.0000e+00 - val_loss: 0.5543 - val_acc: 0.0000e+00\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.4540 - acc: 0.0000e+00 - val_loss: 0.5550 - val_acc: 0.0000e+00\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.4538 - acc: 0.0000e+00 - val_loss: 0.5551 - val_acc: 0.0000e+00\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.4534 - acc: 0.0000e+00 - val_loss: 0.5549 - val_acc: 0.0000e+00\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5556 - val_acc: 0.0000e+00\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.4537 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.4532 - acc: 0.0000e+00 - val_loss: 0.5564 - val_acc: 0.0000e+00\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.4530 - acc: 0.0000e+00 - val_loss: 0.5563 - val_acc: 0.0000e+00\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.4528 - acc: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.0000e+00\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.4527 - acc: 0.0000e+00 - val_loss: 0.5571 - val_acc: 0.0000e+00\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.4527 - acc: 0.0000e+00 - val_loss: 0.5576 - val_acc: 0.0000e+00\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.4531 - acc: 0.0000e+00 - val_loss: 0.5575 - val_acc: 0.0000e+00\n",
      "Epoch 159/170\n",
      " - 0s - loss: 0.4524 - acc: 0.0000e+00 - val_loss: 0.5576 - val_acc: 0.0000e+00\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.4525 - acc: 0.0000e+00 - val_loss: 0.5582 - val_acc: 0.0000e+00\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.4524 - acc: 0.0000e+00 - val_loss: 0.5587 - val_acc: 0.0000e+00\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5590 - val_acc: 0.0000e+00\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.4522 - acc: 0.0000e+00 - val_loss: 0.5585 - val_acc: 0.0000e+00\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.4525 - acc: 0.0000e+00 - val_loss: 0.5586 - val_acc: 0.0000e+00\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.4520 - acc: 0.0000e+00 - val_loss: 0.5595 - val_acc: 0.0000e+00\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.4516 - acc: 0.0000e+00 - val_loss: 0.5607 - val_acc: 0.0000e+00\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.4520 - acc: 0.0000e+00 - val_loss: 0.5597 - val_acc: 0.0000e+00\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5604 - val_acc: 0.0000e+00\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.4513 - acc: 0.0000e+00 - val_loss: 0.5608 - val_acc: 0.0000e+00\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.4514 - acc: 0.0000e+00 - val_loss: 0.5606 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 2.8030 - acc: 0.0000e+00 - val_loss: 2.1154 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.7350 - acc: 0.0000e+00 - val_loss: 2.0676 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.6646 - acc: 0.0000e+00 - val_loss: 2.0222 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.6021 - acc: 0.0000e+00 - val_loss: 1.9775 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.5360 - acc: 0.0000e+00 - val_loss: 1.9353 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      " - 0s - loss: 2.4758 - acc: 0.0000e+00 - val_loss: 1.8944 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.4168 - acc: 0.0000e+00 - val_loss: 1.8551 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 2.3611 - acc: 0.0000e+00 - val_loss: 1.8173 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 2.3044 - acc: 0.0000e+00 - val_loss: 1.7816 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 2.2538 - acc: 0.0000e+00 - val_loss: 1.7467 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 2.2037 - acc: 0.0000e+00 - val_loss: 1.7129 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 2.1562 - acc: 0.0000e+00 - val_loss: 1.6803 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 2.1104 - acc: 0.0000e+00 - val_loss: 1.6489 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 2.0637 - acc: 0.0000e+00 - val_loss: 1.6192 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 2.0216 - acc: 0.0000e+00 - val_loss: 1.5903 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.9791 - acc: 0.0000e+00 - val_loss: 1.5629 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 1.9408 - acc: 0.0000e+00 - val_loss: 1.5359 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.9026 - acc: 0.0000e+00 - val_loss: 1.5097 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.8641 - acc: 0.0000e+00 - val_loss: 1.4847 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.8284 - acc: 0.0000e+00 - val_loss: 1.4602 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.7940 - acc: 0.0000e+00 - val_loss: 1.4367 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.7600 - acc: 0.0000e+00 - val_loss: 1.4140 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.7283 - acc: 0.0000e+00 - val_loss: 1.3919 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.6965 - acc: 0.0000e+00 - val_loss: 1.3706 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.6650 - acc: 0.0000e+00 - val_loss: 1.3502 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.6369 - acc: 0.0000e+00 - val_loss: 1.3297 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.6074 - acc: 0.0000e+00 - val_loss: 1.3102 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.5803 - acc: 0.0000e+00 - val_loss: 1.2910 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.5517 - acc: 0.0000e+00 - val_loss: 1.2730 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.5260 - acc: 0.0000e+00 - val_loss: 1.2553 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.5021 - acc: 0.0000e+00 - val_loss: 1.2372 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.4757 - acc: 0.0000e+00 - val_loss: 1.2205 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.4530 - acc: 0.0000e+00 - val_loss: 1.2035 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.4278 - acc: 0.0000e+00 - val_loss: 1.1875 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 1.4066 - acc: 0.0000e+00 - val_loss: 1.1712 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.3831 - acc: 0.0000e+00 - val_loss: 1.1558 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.3615 - acc: 0.0000e+00 - val_loss: 1.1407 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.3401 - acc: 0.0000e+00 - val_loss: 1.1260 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.3203 - acc: 0.0000e+00 - val_loss: 1.1116 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.2988 - acc: 0.0000e+00 - val_loss: 1.0979 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.2791 - acc: 0.0000e+00 - val_loss: 1.0845 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 1.2606 - acc: 0.0000e+00 - val_loss: 1.0711 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.2420 - acc: 0.0000e+00 - val_loss: 1.0582 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.2234 - acc: 0.0000e+00 - val_loss: 1.0457 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.2054 - acc: 0.0000e+00 - val_loss: 1.0335 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 1.1883 - acc: 0.0000e+00 - val_loss: 1.0212 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 1.1713 - acc: 0.0000e+00 - val_loss: 1.0095 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 1.1542 - acc: 0.0000e+00 - val_loss: 0.9979 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 1.1377 - acc: 0.0000e+00 - val_loss: 0.9866 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 1.1219 - acc: 0.0000e+00 - val_loss: 0.9756 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 1.1059 - acc: 0.0000e+00 - val_loss: 0.9649 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 1.0911 - acc: 0.0000e+00 - val_loss: 0.9542 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 1.0765 - acc: 0.0000e+00 - val_loss: 0.9437 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 1.0609 - acc: 0.0000e+00 - val_loss: 0.9340 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 1.0476 - acc: 0.0000e+00 - val_loss: 0.9241 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 1.0332 - acc: 0.0000e+00 - val_loss: 0.9146 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 1.0202 - acc: 0.0000e+00 - val_loss: 0.9051 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 1.0069 - acc: 0.0000e+00 - val_loss: 0.8962 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.9937 - acc: 0.0000e+00 - val_loss: 0.8874 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.9803 - acc: 0.0000e+00 - val_loss: 0.8789 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.9683 - acc: 0.0000e+00 - val_loss: 0.8706 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.9562 - acc: 0.0000e+00 - val_loss: 0.8625 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.9448 - acc: 0.0000e+00 - val_loss: 0.8543 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.9329 - acc: 0.0000e+00 - val_loss: 0.8466 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.9219 - acc: 0.0000e+00 - val_loss: 0.8388 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.9097 - acc: 0.0000e+00 - val_loss: 0.8318 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.8995 - acc: 0.0000e+00 - val_loss: 0.8248 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.8899 - acc: 0.0000e+00 - val_loss: 0.8175 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.8786 - acc: 0.0000e+00 - val_loss: 0.8106 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.8684 - acc: 0.0000e+00 - val_loss: 0.8039 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.8590 - acc: 0.0000e+00 - val_loss: 0.7974 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.8493 - acc: 0.0000e+00 - val_loss: 0.7911 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.8394 - acc: 0.0000e+00 - val_loss: 0.7850 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.8304 - acc: 0.0000e+00 - val_loss: 0.7789 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.8212 - acc: 0.0000e+00 - val_loss: 0.7732 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.8126 - acc: 0.0000e+00 - val_loss: 0.7675 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.8035 - acc: 0.0000e+00 - val_loss: 0.7620 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.7949 - acc: 0.0000e+00 - val_loss: 0.7566 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.7867 - acc: 0.0000e+00 - val_loss: 0.7515 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.7795 - acc: 0.0000e+00 - val_loss: 0.7461 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.7708 - acc: 0.0000e+00 - val_loss: 0.7411 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.7635 - acc: 0.0000e+00 - val_loss: 0.7362 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.7555 - acc: 0.0000e+00 - val_loss: 0.7315 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.7487 - acc: 0.0000e+00 - val_loss: 0.7268 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7410 - acc: 0.0000e+00 - val_loss: 0.7224 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.7338 - acc: 0.0000e+00 - val_loss: 0.7182 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.7271 - acc: 0.0000e+00 - val_loss: 0.7139 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.7208 - acc: 0.0000e+00 - val_loss: 0.7096 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.7134 - acc: 0.0000e+00 - val_loss: 0.7057 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.7073 - acc: 0.0000e+00 - val_loss: 0.7018 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.7008 - acc: 0.0000e+00 - val_loss: 0.6980 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.6951 - acc: 0.0000e+00 - val_loss: 0.6944 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.6889 - acc: 0.0000e+00 - val_loss: 0.6907 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.6830 - acc: 0.0000e+00 - val_loss: 0.6874 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      " - 0s - loss: 0.6772 - acc: 0.0000e+00 - val_loss: 0.6841 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6718 - acc: 0.0000e+00 - val_loss: 0.6807 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6665 - acc: 0.0000e+00 - val_loss: 0.6776 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6608 - acc: 0.0000e+00 - val_loss: 0.6747 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6558 - acc: 0.0000e+00 - val_loss: 0.6716 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6508 - acc: 0.0000e+00 - val_loss: 0.6686 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 5.3148 - acc: 0.0000e+00 - val_loss: 3.6639 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.5647 - acc: 0.0000e+00 - val_loss: 3.1159 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 3.8448 - acc: 0.0000e+00 - val_loss: 2.6434 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.2392 - acc: 0.0000e+00 - val_loss: 2.2378 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.7215 - acc: 0.0000e+00 - val_loss: 1.9033 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.2965 - acc: 0.0000e+00 - val_loss: 1.6285 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 1.9373 - acc: 0.0000e+00 - val_loss: 1.4080 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 1.6528 - acc: 0.0000e+00 - val_loss: 1.2294 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 1.4198 - acc: 0.0000e+00 - val_loss: 1.0908 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 1.2332 - acc: 0.0000e+00 - val_loss: 0.9833 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 1.0887 - acc: 0.0000e+00 - val_loss: 0.8992 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.9715 - acc: 0.0000e+00 - val_loss: 0.8368 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.8836 - acc: 0.0000e+00 - val_loss: 0.7885 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.8119 - acc: 0.0000e+00 - val_loss: 0.7533 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.7584 - acc: 0.0000e+00 - val_loss: 0.7261 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.7163 - acc: 0.0000e+00 - val_loss: 0.7058 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.6830 - acc: 0.0000e+00 - val_loss: 0.6909 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.6583 - acc: 0.0000e+00 - val_loss: 0.6789 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.6366 - acc: 0.0000e+00 - val_loss: 0.6702 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.6204 - acc: 0.0000e+00 - val_loss: 0.6629 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.6063 - acc: 0.0000e+00 - val_loss: 0.6562 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5941 - acc: 0.0000e+00 - val_loss: 0.6508 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5840 - acc: 0.0000e+00 - val_loss: 0.6459 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5754 - acc: 0.0000e+00 - val_loss: 0.6416 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5679 - acc: 0.0000e+00 - val_loss: 0.6373 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5601 - acc: 0.0000e+00 - val_loss: 0.6332 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5532 - acc: 0.0000e+00 - val_loss: 0.6294 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5469 - acc: 0.0000e+00 - val_loss: 0.6251 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5417 - acc: 0.0000e+00 - val_loss: 0.6222 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5355 - acc: 0.0000e+00 - val_loss: 0.6188 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5306 - acc: 0.0000e+00 - val_loss: 0.6152 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5253 - acc: 0.0000e+00 - val_loss: 0.6121 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5211 - acc: 0.0000e+00 - val_loss: 0.6086 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5174 - acc: 0.0000e+00 - val_loss: 0.6057 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5129 - acc: 0.0000e+00 - val_loss: 0.6024 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.5092 - acc: 0.0000e+00 - val_loss: 0.6004 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.5058 - acc: 0.0000e+00 - val_loss: 0.5978 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.5030 - acc: 0.0000e+00 - val_loss: 0.5953 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.4991 - acc: 0.0000e+00 - val_loss: 0.5918 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4965 - acc: 0.0000e+00 - val_loss: 0.5892 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4935 - acc: 0.0000e+00 - val_loss: 0.5872 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4911 - acc: 0.0000e+00 - val_loss: 0.5854 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4884 - acc: 0.0000e+00 - val_loss: 0.5830 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4864 - acc: 0.0000e+00 - val_loss: 0.5816 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4844 - acc: 0.0000e+00 - val_loss: 0.5800 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.4823 - acc: 0.0000e+00 - val_loss: 0.5779 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4801 - acc: 0.0000e+00 - val_loss: 0.5761 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4785 - acc: 0.0000e+00 - val_loss: 0.5745 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4763 - acc: 0.0000e+00 - val_loss: 0.5727 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.5707 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.4737 - acc: 0.0000e+00 - val_loss: 0.5691 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.4722 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.4707 - acc: 0.0000e+00 - val_loss: 0.5670 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.4696 - acc: 0.0000e+00 - val_loss: 0.5656 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.4683 - acc: 0.0000e+00 - val_loss: 0.5644 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.4676 - acc: 0.0000e+00 - val_loss: 0.5630 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.4665 - acc: 0.0000e+00 - val_loss: 0.5622 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.4656 - acc: 0.0000e+00 - val_loss: 0.5610 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.4643 - acc: 0.0000e+00 - val_loss: 0.5593 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.4637 - acc: 0.0000e+00 - val_loss: 0.5579 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4627 - acc: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4620 - acc: 0.0000e+00 - val_loss: 0.5552 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4612 - acc: 0.0000e+00 - val_loss: 0.5550 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.4605 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.4596 - acc: 0.0000e+00 - val_loss: 0.5538 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.4602 - acc: 0.0000e+00 - val_loss: 0.5532 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.4583 - acc: 0.0000e+00 - val_loss: 0.5528 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.4586 - acc: 0.0000e+00 - val_loss: 0.5523 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.4575 - acc: 0.0000e+00 - val_loss: 0.5518 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.4570 - acc: 0.0000e+00 - val_loss: 0.5514 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.4568 - acc: 0.0000e+00 - val_loss: 0.5518 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.4556 - acc: 0.0000e+00 - val_loss: 0.5495 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.5493 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.5489 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.4536 - acc: 0.0000e+00 - val_loss: 0.5484 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.4543 - acc: 0.0000e+00 - val_loss: 0.5478 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.4531 - acc: 0.0000e+00 - val_loss: 0.5473 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5472 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.4532 - acc: 0.0000e+00 - val_loss: 0.5468 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5461 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.4518 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.4518 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.4518 - acc: 0.0000e+00 - val_loss: 0.5463 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.4508 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.4505 - acc: 0.0000e+00 - val_loss: 0.5452 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4501 - acc: 0.0000e+00 - val_loss: 0.5451 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.4499 - acc: 0.0000e+00 - val_loss: 0.5444 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4502 - acc: 0.0000e+00 - val_loss: 0.5440 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.4497 - acc: 0.0000e+00 - val_loss: 0.5447 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.4493 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4490 - acc: 0.0000e+00 - val_loss: 0.5443 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4489 - acc: 0.0000e+00 - val_loss: 0.5444 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4488 - acc: 0.0000e+00 - val_loss: 0.5436 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4485 - acc: 0.0000e+00 - val_loss: 0.5435 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4480 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4476 - acc: 0.0000e+00 - val_loss: 0.5423 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4478 - acc: 0.0000e+00 - val_loss: 0.5421 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.4477 - acc: 0.0000e+00 - val_loss: 0.5422 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 1.6608 - acc: 0.0000e+00 - val_loss: 1.1954 - val_acc: 0.0000e+00\n",
      "Epoch 2/170\n",
      " - 0s - loss: 1.6198 - acc: 0.0000e+00 - val_loss: 1.1673 - val_acc: 0.0000e+00\n",
      "Epoch 3/170\n",
      " - 0s - loss: 1.5783 - acc: 0.0000e+00 - val_loss: 1.1391 - val_acc: 0.0000e+00\n",
      "Epoch 4/170\n",
      " - 0s - loss: 1.5354 - acc: 0.0000e+00 - val_loss: 1.1111 - val_acc: 0.0000e+00\n",
      "Epoch 5/170\n",
      " - 0s - loss: 1.4917 - acc: 0.0000e+00 - val_loss: 1.0835 - val_acc: 0.0000e+00\n",
      "Epoch 6/170\n",
      " - 0s - loss: 1.4479 - acc: 0.0000e+00 - val_loss: 1.0568 - val_acc: 0.0000e+00\n",
      "Epoch 7/170\n",
      " - 0s - loss: 1.4080 - acc: 0.0000e+00 - val_loss: 1.0314 - val_acc: 0.0000e+00\n",
      "Epoch 8/170\n",
      " - 0s - loss: 1.3663 - acc: 0.0000e+00 - val_loss: 1.0075 - val_acc: 0.0000e+00\n",
      "Epoch 9/170\n",
      " - 0s - loss: 1.3285 - acc: 0.0000e+00 - val_loss: 0.9844 - val_acc: 0.0000e+00\n",
      "Epoch 10/170\n",
      " - 0s - loss: 1.2912 - acc: 0.0000e+00 - val_loss: 0.9626 - val_acc: 0.0000e+00\n",
      "Epoch 11/170\n",
      " - 0s - loss: 1.2558 - acc: 0.0000e+00 - val_loss: 0.9423 - val_acc: 0.0000e+00\n",
      "Epoch 12/170\n",
      " - 0s - loss: 1.2206 - acc: 0.0000e+00 - val_loss: 0.9232 - val_acc: 0.0000e+00\n",
      "Epoch 13/170\n",
      " - 0s - loss: 1.1898 - acc: 0.0000e+00 - val_loss: 0.9049 - val_acc: 0.0000e+00\n",
      "Epoch 14/170\n",
      " - 0s - loss: 1.1581 - acc: 0.0000e+00 - val_loss: 0.8876 - val_acc: 0.0000e+00\n",
      "Epoch 15/170\n",
      " - 0s - loss: 1.1269 - acc: 0.0000e+00 - val_loss: 0.8714 - val_acc: 0.0000e+00\n",
      "Epoch 16/170\n",
      " - 0s - loss: 1.0976 - acc: 0.0000e+00 - val_loss: 0.8561 - val_acc: 0.0000e+00\n",
      "Epoch 17/170\n",
      " - 0s - loss: 1.0703 - acc: 0.0000e+00 - val_loss: 0.8415 - val_acc: 0.0000e+00\n",
      "Epoch 18/170\n",
      " - 0s - loss: 1.0441 - acc: 0.0000e+00 - val_loss: 0.8275 - val_acc: 0.0000e+00\n",
      "Epoch 19/170\n",
      " - 0s - loss: 1.0197 - acc: 0.0000e+00 - val_loss: 0.8143 - val_acc: 0.0000e+00\n",
      "Epoch 20/170\n",
      " - 0s - loss: 0.9943 - acc: 0.0000e+00 - val_loss: 0.8021 - val_acc: 0.0000e+00\n",
      "Epoch 21/170\n",
      " - 0s - loss: 0.9717 - acc: 0.0000e+00 - val_loss: 0.7905 - val_acc: 0.0000e+00\n",
      "Epoch 22/170\n",
      " - 0s - loss: 0.9492 - acc: 0.0000e+00 - val_loss: 0.7796 - val_acc: 0.0000e+00\n",
      "Epoch 23/170\n",
      " - 0s - loss: 0.9283 - acc: 0.0000e+00 - val_loss: 0.7694 - val_acc: 0.0000e+00\n",
      "Epoch 24/170\n",
      " - 0s - loss: 0.9075 - acc: 0.0000e+00 - val_loss: 0.7597 - val_acc: 0.0000e+00\n",
      "Epoch 25/170\n",
      " - 0s - loss: 0.8876 - acc: 0.0000e+00 - val_loss: 0.7506 - val_acc: 0.0000e+00\n",
      "Epoch 26/170\n",
      " - 0s - loss: 0.8697 - acc: 0.0000e+00 - val_loss: 0.7418 - val_acc: 0.0000e+00\n",
      "Epoch 27/170\n",
      " - 0s - loss: 0.8518 - acc: 0.0000e+00 - val_loss: 0.7335 - val_acc: 0.0000e+00\n",
      "Epoch 28/170\n",
      " - 0s - loss: 0.8337 - acc: 0.0000e+00 - val_loss: 0.7258 - val_acc: 0.0000e+00\n",
      "Epoch 29/170\n",
      " - 0s - loss: 0.8178 - acc: 0.0000e+00 - val_loss: 0.7185 - val_acc: 0.0000e+00\n",
      "Epoch 30/170\n",
      " - 0s - loss: 0.8028 - acc: 0.0000e+00 - val_loss: 0.7116 - val_acc: 0.0000e+00\n",
      "Epoch 31/170\n",
      " - 0s - loss: 0.7874 - acc: 0.0000e+00 - val_loss: 0.7050 - val_acc: 0.0000e+00\n",
      "Epoch 32/170\n",
      " - 0s - loss: 0.7728 - acc: 0.0000e+00 - val_loss: 0.6988 - val_acc: 0.0000e+00\n",
      "Epoch 33/170\n",
      " - 0s - loss: 0.7598 - acc: 0.0000e+00 - val_loss: 0.6930 - val_acc: 0.0000e+00\n",
      "Epoch 34/170\n",
      " - 0s - loss: 0.7465 - acc: 0.0000e+00 - val_loss: 0.6876 - val_acc: 0.0000e+00\n",
      "Epoch 35/170\n",
      " - 0s - loss: 0.7338 - acc: 0.0000e+00 - val_loss: 0.6823 - val_acc: 0.0000e+00\n",
      "Epoch 36/170\n",
      " - 0s - loss: 0.7220 - acc: 0.0000e+00 - val_loss: 0.6771 - val_acc: 0.0000e+00\n",
      "Epoch 37/170\n",
      " - 0s - loss: 0.7107 - acc: 0.0000e+00 - val_loss: 0.6723 - val_acc: 0.0000e+00\n",
      "Epoch 38/170\n",
      " - 0s - loss: 0.6994 - acc: 0.0000e+00 - val_loss: 0.6679 - val_acc: 0.0000e+00\n",
      "Epoch 39/170\n",
      " - 0s - loss: 0.6901 - acc: 0.0000e+00 - val_loss: 0.6636 - val_acc: 0.0000e+00\n",
      "Epoch 40/170\n",
      " - 0s - loss: 0.6789 - acc: 0.0000e+00 - val_loss: 0.6594 - val_acc: 0.0000e+00\n",
      "Epoch 41/170\n",
      " - 0s - loss: 0.6700 - acc: 0.0000e+00 - val_loss: 0.6553 - val_acc: 0.0000e+00\n",
      "Epoch 42/170\n",
      " - 0s - loss: 0.6604 - acc: 0.0000e+00 - val_loss: 0.6515 - val_acc: 0.0000e+00\n",
      "Epoch 43/170\n",
      " - 0s - loss: 0.6513 - acc: 0.0000e+00 - val_loss: 0.6480 - val_acc: 0.0000e+00\n",
      "Epoch 44/170\n",
      " - 0s - loss: 0.6429 - acc: 0.0000e+00 - val_loss: 0.6446 - val_acc: 0.0000e+00\n",
      "Epoch 45/170\n",
      " - 0s - loss: 0.6353 - acc: 0.0000e+00 - val_loss: 0.6414 - val_acc: 0.0000e+00\n",
      "Epoch 46/170\n",
      " - 0s - loss: 0.6268 - acc: 0.0000e+00 - val_loss: 0.6383 - val_acc: 0.0000e+00\n",
      "Epoch 47/170\n",
      " - 0s - loss: 0.6202 - acc: 0.0000e+00 - val_loss: 0.6354 - val_acc: 0.0000e+00\n",
      "Epoch 48/170\n",
      " - 0s - loss: 0.6131 - acc: 0.0000e+00 - val_loss: 0.6325 - val_acc: 0.0000e+00\n",
      "Epoch 49/170\n",
      " - 0s - loss: 0.6055 - acc: 0.0000e+00 - val_loss: 0.6298 - val_acc: 0.0000e+00\n",
      "Epoch 50/170\n",
      " - 0s - loss: 0.5997 - acc: 0.0000e+00 - val_loss: 0.6273 - val_acc: 0.0000e+00\n",
      "Epoch 51/170\n",
      " - 0s - loss: 0.5930 - acc: 0.0000e+00 - val_loss: 0.6248 - val_acc: 0.0000e+00\n",
      "Epoch 52/170\n",
      " - 0s - loss: 0.5877 - acc: 0.0000e+00 - val_loss: 0.6224 - val_acc: 0.0000e+00\n",
      "Epoch 53/170\n",
      " - 0s - loss: 0.5817 - acc: 0.0000e+00 - val_loss: 0.6202 - val_acc: 0.0000e+00\n",
      "Epoch 54/170\n",
      " - 0s - loss: 0.5770 - acc: 0.0000e+00 - val_loss: 0.6180 - val_acc: 0.0000e+00\n",
      "Epoch 55/170\n",
      " - 0s - loss: 0.5716 - acc: 0.0000e+00 - val_loss: 0.6156 - val_acc: 0.0000e+00\n",
      "Epoch 56/170\n",
      " - 0s - loss: 0.5661 - acc: 0.0000e+00 - val_loss: 0.6134 - val_acc: 0.0000e+00\n",
      "Epoch 57/170\n",
      " - 0s - loss: 0.5611 - acc: 0.0000e+00 - val_loss: 0.6114 - val_acc: 0.0000e+00\n",
      "Epoch 58/170\n",
      " - 0s - loss: 0.5568 - acc: 0.0000e+00 - val_loss: 0.6094 - val_acc: 0.0000e+00\n",
      "Epoch 59/170\n",
      " - 0s - loss: 0.5529 - acc: 0.0000e+00 - val_loss: 0.6074 - val_acc: 0.0000e+00\n",
      "Epoch 60/170\n",
      " - 0s - loss: 0.5486 - acc: 0.0000e+00 - val_loss: 0.6057 - val_acc: 0.0000e+00\n",
      "Epoch 61/170\n",
      " - 0s - loss: 0.5443 - acc: 0.0000e+00 - val_loss: 0.6041 - val_acc: 0.0000e+00\n",
      "Epoch 62/170\n",
      " - 0s - loss: 0.5407 - acc: 0.0000e+00 - val_loss: 0.6025 - val_acc: 0.0000e+00\n",
      "Epoch 63/170\n",
      " - 0s - loss: 0.5369 - acc: 0.0000e+00 - val_loss: 0.6009 - val_acc: 0.0000e+00\n",
      "Epoch 64/170\n",
      " - 0s - loss: 0.5335 - acc: 0.0000e+00 - val_loss: 0.5993 - val_acc: 0.0000e+00\n",
      "Epoch 65/170\n",
      " - 0s - loss: 0.5300 - acc: 0.0000e+00 - val_loss: 0.5978 - val_acc: 0.0000e+00\n",
      "Epoch 66/170\n",
      " - 0s - loss: 0.5263 - acc: 0.0000e+00 - val_loss: 0.5965 - val_acc: 0.0000e+00\n",
      "Epoch 67/170\n",
      " - 0s - loss: 0.5231 - acc: 0.0000e+00 - val_loss: 0.5950 - val_acc: 0.0000e+00\n",
      "Epoch 68/170\n",
      " - 0s - loss: 0.5209 - acc: 0.0000e+00 - val_loss: 0.5934 - val_acc: 0.0000e+00\n",
      "Epoch 69/170\n",
      " - 0s - loss: 0.5179 - acc: 0.0000e+00 - val_loss: 0.5922 - val_acc: 0.0000e+00\n",
      "Epoch 70/170\n",
      " - 0s - loss: 0.5144 - acc: 0.0000e+00 - val_loss: 0.5908 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/170\n",
      " - 0s - loss: 0.5124 - acc: 0.0000e+00 - val_loss: 0.5897 - val_acc: 0.0000e+00\n",
      "Epoch 72/170\n",
      " - 0s - loss: 0.5095 - acc: 0.0000e+00 - val_loss: 0.5884 - val_acc: 0.0000e+00\n",
      "Epoch 73/170\n",
      " - 0s - loss: 0.5076 - acc: 0.0000e+00 - val_loss: 0.5872 - val_acc: 0.0000e+00\n",
      "Epoch 74/170\n",
      " - 0s - loss: 0.5050 - acc: 0.0000e+00 - val_loss: 0.5862 - val_acc: 0.0000e+00\n",
      "Epoch 75/170\n",
      " - 0s - loss: 0.5029 - acc: 0.0000e+00 - val_loss: 0.5850 - val_acc: 0.0000e+00\n",
      "Epoch 76/170\n",
      " - 0s - loss: 0.5011 - acc: 0.0000e+00 - val_loss: 0.5838 - val_acc: 0.0000e+00\n",
      "Epoch 77/170\n",
      " - 0s - loss: 0.4985 - acc: 0.0000e+00 - val_loss: 0.5829 - val_acc: 0.0000e+00\n",
      "Epoch 78/170\n",
      " - 0s - loss: 0.4973 - acc: 0.0000e+00 - val_loss: 0.5820 - val_acc: 0.0000e+00\n",
      "Epoch 79/170\n",
      " - 0s - loss: 0.4950 - acc: 0.0000e+00 - val_loss: 0.5809 - val_acc: 0.0000e+00\n",
      "Epoch 80/170\n",
      " - 0s - loss: 0.4927 - acc: 0.0000e+00 - val_loss: 0.5800 - val_acc: 0.0000e+00\n",
      "Epoch 81/170\n",
      " - 0s - loss: 0.4916 - acc: 0.0000e+00 - val_loss: 0.5792 - val_acc: 0.0000e+00\n",
      "Epoch 82/170\n",
      " - 0s - loss: 0.4900 - acc: 0.0000e+00 - val_loss: 0.5783 - val_acc: 0.0000e+00\n",
      "Epoch 83/170\n",
      " - 0s - loss: 0.4878 - acc: 0.0000e+00 - val_loss: 0.5772 - val_acc: 0.0000e+00\n",
      "Epoch 84/170\n",
      " - 0s - loss: 0.4869 - acc: 0.0000e+00 - val_loss: 0.5763 - val_acc: 0.0000e+00\n",
      "Epoch 85/170\n",
      " - 0s - loss: 0.4847 - acc: 0.0000e+00 - val_loss: 0.5756 - val_acc: 0.0000e+00\n",
      "Epoch 86/170\n",
      " - 0s - loss: 0.4837 - acc: 0.0000e+00 - val_loss: 0.5749 - val_acc: 0.0000e+00\n",
      "Epoch 87/170\n",
      " - 0s - loss: 0.4820 - acc: 0.0000e+00 - val_loss: 0.5741 - val_acc: 0.0000e+00\n",
      "Epoch 88/170\n",
      " - 0s - loss: 0.4809 - acc: 0.0000e+00 - val_loss: 0.5735 - val_acc: 0.0000e+00\n",
      "Epoch 89/170\n",
      " - 0s - loss: 0.4797 - acc: 0.0000e+00 - val_loss: 0.5728 - val_acc: 0.0000e+00\n",
      "Epoch 90/170\n",
      " - 0s - loss: 0.4786 - acc: 0.0000e+00 - val_loss: 0.5722 - val_acc: 0.0000e+00\n",
      "Epoch 91/170\n",
      " - 0s - loss: 0.4777 - acc: 0.0000e+00 - val_loss: 0.5713 - val_acc: 0.0000e+00\n",
      "Epoch 92/170\n",
      " - 0s - loss: 0.4763 - acc: 0.0000e+00 - val_loss: 0.5707 - val_acc: 0.0000e+00\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.5700 - val_acc: 0.0000e+00\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.4746 - acc: 0.0000e+00 - val_loss: 0.5694 - val_acc: 0.0000e+00\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.4732 - acc: 0.0000e+00 - val_loss: 0.5688 - val_acc: 0.0000e+00\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.4724 - acc: 0.0000e+00 - val_loss: 0.5682 - val_acc: 0.0000e+00\n",
      "Epoch 97/170\n",
      " - 0s - loss: 0.4717 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.4714 - acc: 0.0000e+00 - val_loss: 0.5672 - val_acc: 0.0000e+00\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.4697 - acc: 0.0000e+00 - val_loss: 0.5665 - val_acc: 0.0000e+00\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.4689 - acc: 0.0000e+00 - val_loss: 0.5660 - val_acc: 0.0000e+00\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.4683 - acc: 0.0000e+00 - val_loss: 0.5657 - val_acc: 0.0000e+00\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.4670 - acc: 0.0000e+00 - val_loss: 0.5652 - val_acc: 0.0000e+00\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.4667 - acc: 0.0000e+00 - val_loss: 0.5648 - val_acc: 0.0000e+00\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.4656 - acc: 0.0000e+00 - val_loss: 0.5644 - val_acc: 0.0000e+00\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.4658 - acc: 0.0000e+00 - val_loss: 0.5640 - val_acc: 0.0000e+00\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.4647 - acc: 0.0000e+00 - val_loss: 0.5635 - val_acc: 0.0000e+00\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.4637 - acc: 0.0000e+00 - val_loss: 0.5631 - val_acc: 0.0000e+00\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.4632 - acc: 0.0000e+00 - val_loss: 0.5627 - val_acc: 0.0000e+00\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.4625 - acc: 0.0000e+00 - val_loss: 0.5623 - val_acc: 0.0000e+00\n",
      "Epoch 110/170\n",
      " - 0s - loss: 0.4623 - acc: 0.0000e+00 - val_loss: 0.5620 - val_acc: 0.0000e+00\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.4619 - acc: 0.0000e+00 - val_loss: 0.5615 - val_acc: 0.0000e+00\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.4617 - acc: 0.0000e+00 - val_loss: 0.5612 - val_acc: 0.0000e+00\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.4606 - acc: 0.0000e+00 - val_loss: 0.5608 - val_acc: 0.0000e+00\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.4601 - acc: 0.0000e+00 - val_loss: 0.5604 - val_acc: 0.0000e+00\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.4597 - acc: 0.0000e+00 - val_loss: 0.5600 - val_acc: 0.0000e+00\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.4595 - acc: 0.0000e+00 - val_loss: 0.5599 - val_acc: 0.0000e+00\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.4588 - acc: 0.0000e+00 - val_loss: 0.5596 - val_acc: 0.0000e+00\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.4590 - acc: 0.0000e+00 - val_loss: 0.5594 - val_acc: 0.0000e+00\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.4580 - acc: 0.0000e+00 - val_loss: 0.5591 - val_acc: 0.0000e+00\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.4575 - acc: 0.0000e+00 - val_loss: 0.5587 - val_acc: 0.0000e+00\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.4583 - acc: 0.0000e+00 - val_loss: 0.5585 - val_acc: 0.0000e+00\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.4569 - acc: 0.0000e+00 - val_loss: 0.5583 - val_acc: 0.0000e+00\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.4563 - acc: 0.0000e+00 - val_loss: 0.5581 - val_acc: 0.0000e+00\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5578 - val_acc: 0.0000e+00\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.4555 - acc: 0.0000e+00 - val_loss: 0.5575 - val_acc: 0.0000e+00\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.5571 - val_acc: 0.0000e+00\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.4552 - acc: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.0000e+00\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.4547 - acc: 0.0000e+00 - val_loss: 0.5567 - val_acc: 0.0000e+00\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.4545 - acc: 0.0000e+00 - val_loss: 0.5565 - val_acc: 0.0000e+00\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.4541 - acc: 0.0000e+00 - val_loss: 0.5564 - val_acc: 0.0000e+00\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.4539 - acc: 0.0000e+00 - val_loss: 0.5561 - val_acc: 0.0000e+00\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5560 - val_acc: 0.0000e+00\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5560 - val_acc: 0.0000e+00\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.4529 - acc: 0.0000e+00 - val_loss: 0.5559 - val_acc: 0.0000e+00\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.4528 - acc: 0.0000e+00 - val_loss: 0.5557 - val_acc: 0.0000e+00\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.4522 - acc: 0.0000e+00 - val_loss: 0.5556 - val_acc: 0.0000e+00\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.4526 - acc: 0.0000e+00 - val_loss: 0.5555 - val_acc: 0.0000e+00\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.4524 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.4519 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.4515 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.4516 - acc: 0.0000e+00 - val_loss: 0.5551 - val_acc: 0.0000e+00\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.4515 - acc: 0.0000e+00 - val_loss: 0.5549 - val_acc: 0.0000e+00\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.4512 - acc: 0.0000e+00 - val_loss: 0.5548 - val_acc: 0.0000e+00\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.4509 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.4504 - acc: 0.0000e+00 - val_loss: 0.5544 - val_acc: 0.0000e+00\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.4505 - acc: 0.0000e+00 - val_loss: 0.5543 - val_acc: 0.0000e+00\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.4498 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.4496 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.4496 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.4498 - acc: 0.0000e+00 - val_loss: 0.5544 - val_acc: 0.0000e+00\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.4494 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.4491 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.4495 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.4489 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.4492 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.4486 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.4492 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/170\n",
      " - 0s - loss: 0.4482 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.4480 - acc: 0.0000e+00 - val_loss: 0.5538 - val_acc: 0.0000e+00\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.4490 - acc: 0.0000e+00 - val_loss: 0.5537 - val_acc: 0.0000e+00\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.4478 - acc: 0.0000e+00 - val_loss: 0.5535 - val_acc: 0.0000e+00\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.4477 - acc: 0.0000e+00 - val_loss: 0.5535 - val_acc: 0.0000e+00\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.4482 - acc: 0.0000e+00 - val_loss: 0.5533 - val_acc: 0.0000e+00\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.4470 - acc: 0.0000e+00 - val_loss: 0.5532 - val_acc: 0.0000e+00\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.4468 - acc: 0.0000e+00 - val_loss: 0.5532 - val_acc: 0.0000e+00\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.4473 - acc: 0.0000e+00 - val_loss: 0.5530 - val_acc: 0.0000e+00\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.4469 - acc: 0.0000e+00 - val_loss: 0.5531 - val_acc: 0.0000e+00\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.4471 - acc: 0.0000e+00 - val_loss: 0.5531 - val_acc: 0.0000e+00\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.4465 - acc: 0.0000e+00 - val_loss: 0.5529 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.9854 - acc: 0.0000e+00 - val_loss: 1.0209 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.9266 - acc: 0.0000e+00 - val_loss: 0.9717 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.8710 - acc: 0.0000e+00 - val_loss: 0.9245 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.8180 - acc: 0.0000e+00 - val_loss: 0.8819 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.7710 - acc: 0.0000e+00 - val_loss: 0.8439 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.7329 - acc: 0.0000e+00 - val_loss: 0.8104 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6979 - acc: 0.0000e+00 - val_loss: 0.7813 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.6689 - acc: 0.0000e+00 - val_loss: 0.7563 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6426 - acc: 0.0000e+00 - val_loss: 0.7350 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.6217 - acc: 0.0000e+00 - val_loss: 0.7159 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.6018 - acc: 0.0000e+00 - val_loss: 0.6993 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5846 - acc: 0.0000e+00 - val_loss: 0.6846 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5705 - acc: 0.0000e+00 - val_loss: 0.6716 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5564 - acc: 0.0000e+00 - val_loss: 0.6601 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5446 - acc: 0.0000e+00 - val_loss: 0.6497 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5343 - acc: 0.0000e+00 - val_loss: 0.6404 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5245 - acc: 0.0000e+00 - val_loss: 0.6321 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5153 - acc: 0.0000e+00 - val_loss: 0.6249 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5079 - acc: 0.0000e+00 - val_loss: 0.6182 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5003 - acc: 0.0000e+00 - val_loss: 0.6125 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.4939 - acc: 0.0000e+00 - val_loss: 0.6072 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.4890 - acc: 0.0000e+00 - val_loss: 0.6026 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.4835 - acc: 0.0000e+00 - val_loss: 0.5985 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.4791 - acc: 0.0000e+00 - val_loss: 0.5949 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.5917 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.4711 - acc: 0.0000e+00 - val_loss: 0.5889 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.4675 - acc: 0.0000e+00 - val_loss: 0.5863 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.4650 - acc: 0.0000e+00 - val_loss: 0.5840 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.4624 - acc: 0.0000e+00 - val_loss: 0.5820 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.4604 - acc: 0.0000e+00 - val_loss: 0.5802 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.4578 - acc: 0.0000e+00 - val_loss: 0.5787 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5774 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.4540 - acc: 0.0000e+00 - val_loss: 0.5761 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.4531 - acc: 0.0000e+00 - val_loss: 0.5750 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.4513 - acc: 0.0000e+00 - val_loss: 0.5741 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.4501 - acc: 0.0000e+00 - val_loss: 0.5734 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.4492 - acc: 0.0000e+00 - val_loss: 0.5727 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.4480 - acc: 0.0000e+00 - val_loss: 0.5721 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.4470 - acc: 0.0000e+00 - val_loss: 0.5716 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4469 - acc: 0.0000e+00 - val_loss: 0.5712 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4456 - acc: 0.0000e+00 - val_loss: 0.5708 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4454 - acc: 0.0000e+00 - val_loss: 0.5703 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4445 - acc: 0.0000e+00 - val_loss: 0.5700 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4448 - acc: 0.0000e+00 - val_loss: 0.5696 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4437 - acc: 0.0000e+00 - val_loss: 0.5693 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.4434 - acc: 0.0000e+00 - val_loss: 0.5692 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4431 - acc: 0.0000e+00 - val_loss: 0.5691 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4427 - acc: 0.0000e+00 - val_loss: 0.5690 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4423 - acc: 0.0000e+00 - val_loss: 0.5689 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.4421 - acc: 0.0000e+00 - val_loss: 0.5688 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.4427 - acc: 0.0000e+00 - val_loss: 0.5685 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.4414 - acc: 0.0000e+00 - val_loss: 0.5683 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.4413 - acc: 0.0000e+00 - val_loss: 0.5682 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.4413 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.4408 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.4408 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.4407 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.4403 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.4411 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.4405 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4401 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4399 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4398 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.4400 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.4396 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.4403 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.4400 - acc: 0.0000e+00 - val_loss: 0.5677 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.4404 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.4397 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.4394 - acc: 0.0000e+00 - val_loss: 0.5677 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.4394 - acc: 0.0000e+00 - val_loss: 0.5676 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5675 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5674 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.4396 - acc: 0.0000e+00 - val_loss: 0.5672 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.4389 - acc: 0.0000e+00 - val_loss: 0.5671 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.4393 - acc: 0.0000e+00 - val_loss: 0.5671 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5671 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.4391 - acc: 0.0000e+00 - val_loss: 0.5669 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.4399 - acc: 0.0000e+00 - val_loss: 0.5669 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5668 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.4390 - acc: 0.0000e+00 - val_loss: 0.5666 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5664 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5661 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.4387 - acc: 0.0000e+00 - val_loss: 0.5663 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5663 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5662 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.4384 - acc: 0.0000e+00 - val_loss: 0.5663 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5661 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.4395 - acc: 0.0000e+00 - val_loss: 0.5659 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4382 - acc: 0.0000e+00 - val_loss: 0.5661 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.4390 - acc: 0.0000e+00 - val_loss: 0.5660 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5660 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5659 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5659 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4387 - acc: 0.0000e+00 - val_loss: 0.5658 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4381 - acc: 0.0000e+00 - val_loss: 0.5657 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5654 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5654 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4387 - acc: 0.0000e+00 - val_loss: 0.5652 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.4379 - acc: 0.0000e+00 - val_loss: 0.5652 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Evalutation of best performing model:\n",
      "266/266 [==============================] - 0s 4us/step\n",
      "[0.5605926444207815, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    Make sure to have every relevant import statement included here and return data as\n",
    "    used in model function below. This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    from os import path\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    url = \"../data/diabetes.csv\"\n",
    "    data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "    sc = StandardScaler()\n",
    "    data = sc.fit_transform(data)\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=10, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer={{choice(['adam', 'nadam'])}})\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([10, 30])}},\n",
    "              epochs={{choice([100, 170])}},\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='experiment')\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hyperas(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils.validation import column_or_1d\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.losses import lda_loss\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import copy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import datasets\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from automation_script import get_dataset_info\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.datasets import make_regression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano.tensor as T\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from theano.compile.ops import as_op\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [10, 30]),\n",
      "        'epochs': hp.choice('epochs', [100, 170]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: Make sure to have every relevant import statement included here and return data as\n",
      "  6: used in model function below. This function is separated from model() so that hyperopt\n",
      "  7: won't reload data for each evaluation run.\n",
      "  8: '''\n",
      "  9: url = \"../data/iris.csv\"\n",
      " 10: data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
      " 11: class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
      " 12: data.iloc[:,-1] = index\n",
      " 13: data = data.loc[data[4] != 2]\n",
      " 14: X = data.iloc[:,:-1]\n",
      " 15: Y = data.iloc[:,-1]\n",
      " 16: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
      " 17: \n",
      " 18: \n",
      " 19: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13: \n",
      "  14:     model = Sequential()\n",
      "  15:     model.add(Dense(1, input_dim=4, activation='sigmoid',\n",
      "  16:                    kernel_regularizer=l2(1e-5)))\n",
      "  17: \n",
      "  18:     model.compile(loss=lda_loss(n_components=1, margin=1),\n",
      "  19:                  optimizer=space['optimizer'],\n",
      "  20:                  metrics=['accuracy'])\n",
      "  21: \n",
      "  22:     model.fit(x_train, y_train,\n",
      "  23:               batch_size=space['batch_size'],\n",
      "  24:               epochs=space['epochs'],\n",
      "  25:               verbose=2,\n",
      "  26:               validation_data=(x_test, y_test))\n",
      "  27:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  28:     print('Test accuracy:', acc)\n",
      "  29:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  30: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/170\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs\nApply node that caused the error: Eigvalsh{lower=True}(Elemwise{Composite{((i0 / i1) - i2)}}[(0, 0)].0, Elemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)].0)\nToposort index: 43\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[nan]], dtype=float32), array([[nan]], dtype=float32)]\nOutputs clients: [[Subtensor{int64::}(Eigvalsh{lower=True}.0, Constant{-1})]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\", line 244, in exhaust\n    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\", line 218, in run\n    self.serial_evaluate()\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\", line 137, in serial_evaluate\n    result = self.domain.evaluate(spec, ctrl)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\", line 840, in evaluate\n    rval = self.fn(pyll_rval)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\temp_model.py\", line 1011, in keras_fmin_fnct\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 404, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\utils\\losses.py\", line 58, in inner_lda_objective\n    evals_t = T.slinalg.eigvalsh(Sb_t, Sw_t)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\tensor\\slinalg.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, outputs)\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvalsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigvalsh\u001b[1;34m(a, b, lower, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    860\u001b[0m                 \u001b[0mturbo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mturbo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meigvals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meigvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 check_finite=check_finite)\n\u001b[0m\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigh\u001b[1;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m--> 374\u001b[1;33m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\_lib\\_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[1;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    460\u001b[0m         raise ValueError(\n\u001b[1;32m--> 461\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    462\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4bfe93f1cb86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m                                       \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                       \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                       notebook_name='experiment')\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                      verbose=verbose)\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[0;32m    131\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m              return_argmin=True),\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m         )\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    383\u001b[0m                     max_queue_len=max_queue_len)\n\u001b[0;32m    384\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[0;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                                              verbose=0)\n\u001b[0m\u001b[0;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                         \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    918\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\tensor\\slinalg.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, outputs)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvalsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigvalsh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigvalsh\u001b[1;34m(a, b, lower, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[0moverwrite_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverwrite_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m                 \u001b[0mturbo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mturbo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meigvals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meigvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 check_finite=check_finite)\n\u001b[0m\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\linalg\\decomp.py\u001b[0m in \u001b[0;36meigh\u001b[1;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \"\"\"\n\u001b[1;32m--> 374\u001b[1;33m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected square matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\scipy\\_lib\\_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[1;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'masked arrays are not supported'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'O'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AllFloat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         raise ValueError(\n\u001b[1;32m--> 461\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    462\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs\nApply node that caused the error: Eigvalsh{lower=True}(Elemwise{Composite{((i0 / i1) - i2)}}[(0, 0)].0, Elemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)].0)\nToposort index: 43\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[nan]], dtype=float32), array([[nan]], dtype=float32)]\nOutputs clients: [[Subtensor{int64::}(Eigvalsh{lower=True}.0, Constant{-1})]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\", line 244, in exhaust\n    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\", line 218, in run\n    self.serial_evaluate()\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\", line 137, in serial_evaluate\n    result = self.domain.evaluate(spec, ctrl)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\", line 840, in evaluate\n    rval = self.fn(pyll_rval)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\temp_model.py\", line 1011, in keras_fmin_fnct\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\", line 342, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_utils.py\", line 404, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\shakk\\Desktop\\MLSquare\\cook-imly\\imly\\utils\\losses.py\", line 58, in inner_lda_objective\n    evals_t = T.slinalg.eigvalsh(Sb_t, Sw_t)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from utils.losses import lda_loss\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    Make sure to have every relevant import statement included here and return data as\n",
    "    used in model function below. This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    url = \"../data/iris.csv\"\n",
    "    data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "    class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "    data.iloc[:,-1] = index\n",
    "    data = data.loc[data[4] != 2]\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=4, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-5)))\n",
    "\n",
    "    model.compile(loss=lda_loss(n_components=1, margin=1),\n",
    "                 optimizer={{choice(['adam', 'nadam'])}},\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([10, 30])}},\n",
    "              epochs={{choice([100, 170])}},\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='experiment')\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LinearRegression'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "model_name = 'linear_regression'\n",
    "model_mappings = {\n",
    "    'linear_regression': 'LinearRegression',\n",
    "    'logistic_regression': 'LogisticRegression'\n",
    "}\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "for key, value in model_mappings.items():\n",
    "    if key == model_name:\n",
    "        name = value\n",
    "\n",
    "module = __import__('sklearn.linear_model', fromlist=[name])\n",
    "imported_module = getattr(module, name)\n",
    "model = imported_module\n",
    "\n",
    "primal_model = model()\n",
    "\n",
    "# Primal\n",
    "primal_model.fit(x_train, y_train)\n",
    "primal_model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# diabetes = datasets.load_diabetes()\n",
    "# sc = StandardScaler()\n",
    "# diabetes = sc.fit_transform(diabetes)\n",
    "#####\n",
    "# # Use only one feature\n",
    "# diabetes_X = diabetes.data\n",
    "# # sc = StandardScaler()\n",
    "# # diabetes.data = sc.fit_transform(diabetes.data)\n",
    "\n",
    "# X = diabetes.data\n",
    "# Y = diabetes.target\n",
    "#####\n",
    "\n",
    "# X = preprocessing.scale(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "# # Split the data into training/testing sets\n",
    "# x_train = diabetes_X[:-20]\n",
    "# x_test = diabetes_X[-20:]\n",
    "\n",
    "# # Split the targets into training/testing sets\n",
    "# y_train = diabetes.target[:-20]\n",
    "# y_test = diabetes.target[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5481227216244245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "score = mean_squared_error(y_test, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x17f81a0aeb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm.__call__(param_name=\"log_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wrappers.sklearn.keras_classifier.SklearnKerasClassifier"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "wrapper_class = 'SklearnKerasClassifier'\n",
    "\n",
    "path = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', wrapper_class)\n",
    "module_path = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', path).lower()\n",
    "package_name = module_path.split('_')[0]\n",
    "wrapper_name = '_'.join(module_path.split('_')[1:3])\n",
    "\n",
    "module_path = 'wrappers.' + package_name + '.' + wrapper_name\n",
    "module_path\n",
    "wrapper_module = __import__(module_path, fromlist=[wrapper_class])\n",
    "function = getattr(wrapper_module, wrapper_class)\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = 'sklearn_keras_classifier'\n",
    "'_'.join(module_path.split('_')[1:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"../data/uci_carbon_nanotubes.csv\"\n",
    "data = pd.read_csv(url, delimiter=\";\")\n",
    "data\n",
    "# frames = [X, Y]\n",
    "# data = pd.concat(frames, axis=1)\n",
    "# data.to_csv('../data/uci_auto_mpg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-463eefa48d20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# score = m.score(x_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\wrappers\\sklearn\\keras_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m                                                                        \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                                                        \u001b[0mval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_metric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                                                                        metric=self.metric) \n\u001b[0m\u001b[0;32m     40\u001b[0m             self.model.fit(x_train, y_train, epochs=final_epoch,\n\u001b[0;32m     41\u001b[0m                            batch_size=final_batch_size, verbose=0)\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\optimizers\\talos\\talos.py\u001b[0m in \u001b[0;36mget_best_model\u001b[1;34m(x_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mexperiment_no\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperiment_no\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtalos_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 grid_downsample=0.5)\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, params, model, dataset_name, experiment_no, x_val, y_val, val_split, shuffle, round_limit, grid_downsample, random_method, seed, search_method, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, reduce_loss, last_epoch_value, clear_tf_session, disable_progress_bar, print_params, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;31m# input parameters section ends\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_null\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36mruntime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\talos\\scan\\scan_prepare.py\u001b[0m in \u001b[0;36mscan_prepare\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# create the data asset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[0;32m     26\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[0;32m     27\u001b[0m           initial=_NoValue):\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "### Testing concordance ###\n",
    "\n",
    "from automation_script import get_dataset_info\n",
    "from imly import dope\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_info = get_dataset_info(\"uci_iris_lda\")\n",
    "url = dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "# sc = StandardScaler()\n",
    "# data = sc.fit_transform(data)\n",
    "# data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "m = dope(model)\n",
    "\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "m.fit(x_train, y_train)\n",
    "\n",
    "# score = m.score(x_test, y_test)\n",
    "\n",
    "\n",
    "### Automation script ###\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 200\n",
    "# }\n",
    "\n",
    "# experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "sklearn_pred = model.predict(x_test)\n",
    "keras_pred = m.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9989899125789394"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.correlations import concordance_correlation_coefficient as ccc\n",
    "\n",
    "ccc(sklearn_pred, keras_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22704f6fb38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.scatter(sklearn_pred, keras_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x261561d9358>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing CCC ###\n",
    "\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imly import dope\n",
    "\n",
    "dataset_name = \"uci_auto_mpg\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_auto_mpg.csv\", delimiter=\",\", header=0, index_col='car name')\n",
    "data = data[data.horsepower != '?']\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:,1]\n",
    "X = data.iloc[:,2:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "m = dope(model)\n",
    "\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "m.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "sklearn_pred = model.predict(x_test)\n",
    "keras_pred = m.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9894374129958334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.correlations import concordance_correlation_coefficient as ccc\n",
    "\n",
    "ccc(sklearn_pred, keras_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x26156618e10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(sklearn_pred, keras_pred)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../data/test2_pdf.pdf\", bbox_inches='tight') # write pdf to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../data/test_pdf.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    }
   ],
   "source": [
    "### Test upload to AWS S3 ###\n",
    "import boto\n",
    "import sys\n",
    "from boto.s3.key import Key\n",
    "# from boto.s3.key import Key\n",
    "bucket_name = 'mlsquare-datasets'\n",
    "AWS_ACCESS_KEY_ID = 'AKIAJXRNK62PGFLPIJTA'\n",
    "AWS_SECRET_ACCESS_KEY = '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'\n",
    "REGION_HOST = 's3.ap-south-1.amazonaws.com'\n",
    "\n",
    "# bucket_name = AWS_ACCESS_KEY_ID.lower() + '-dump'\n",
    "conn = boto.connect_s3(AWS_ACCESS_KEY_ID,\n",
    "        AWS_SECRET_ACCESS_KEY, host=REGION_HOST)\n",
    "bucket = conn.get_bucket('mlsquare-pdf', validate=False)\n",
    "\n",
    "# bucket = conn.create_bucket(bucket_name,\n",
    "#     location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "testfile = \"../data/test_pdf.pdf\"\n",
    "print ('Uploading %s to Amazon S3 bucket %s' % (testfile, bucket_name))\n",
    "\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "k = Key(bucket)\n",
    "k.key = 'my test file'\n",
    "k.set_contents_from_filename(testfile,\n",
    "    cb=percent_cb, num_cb=10) # upload file\n",
    "url = k.generate_url(expires_in=0, query_auth=False) # get url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/uci_abalone_logistic.pdf'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'uci_abalone'\n",
    "algo = 'logistic'\n",
    "'../data/' + ('_').join([name,algo]) + '.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets #\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "df = pd.concat([X, y], axis=1)\n",
    "# testData3 = np.concatenate((X,y), axis=1)\n",
    "# testData3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1001)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/testData4.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y type --  <class 'theano.tensor.var.TensorVariable'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s 449us/step - loss: -6.6398\n",
      "60/60 [==============================] - 0s 50us/step\n",
      "-5.562779839833578\n"
     ]
    }
   ],
   "source": [
    "### Testing Deep LDA implementation with Theano loss ###\n",
    "\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 170\n",
    "# }\n",
    "\n",
    "# automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "\n",
    "## Create model ##\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model(input_dim, reg_par):\n",
    "    \"\"\"\n",
    "    Builds the model\n",
    "    The structure of the model can get easily substituted with a more efficient and powerful network like CNN\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1, input_shape=(input_dim,), activation='sigmoid', kernel_regularizer=l2(reg_par)))\n",
    "#     model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_par)))\n",
    "#     model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_par)))\n",
    "#     model.add(Dense(2, activation='linear', kernel_regularizer=l2(reg_par))) \n",
    "#     outdim_size is passed via arguments\n",
    "\n",
    "    return model\n",
    "\n",
    "## Define loss function ##\n",
    "\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from theano.compile.ops import as_op\n",
    "\n",
    "\n",
    "@as_op(itypes=[theano.tensor.ivector],  # Why? What is the need for such an op?\n",
    "       otypes=[theano.tensor.ivector])\n",
    "def numpy_unique(a):\n",
    "    return np.unique(a)\n",
    "\n",
    "\n",
    "def lda_loss(n_components, margin):\n",
    "    \"\"\"\n",
    "    The main loss function (inner_lda_objective) is wrapped in this function due to\n",
    "    the constraints imposed by Keras on objective functions\n",
    "    \"\"\"\n",
    "    def inner_lda_objective(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        It is the loss function of LDA as introduced in the original paper. \n",
    "        It is adopted from the the original implementation in the following link:\n",
    "        https://github.com/CPJKU/deep_lda\n",
    "        Note: it is implemented by Theano tensor operations, and does not work on Tensorflow backend\n",
    "        \"\"\"\n",
    "        r = 1e-4\n",
    "\n",
    "        # init groups\n",
    "        print('y type -- ',type(y_true))\n",
    "        yt = T.cast(y_true.flatten(), \"int32\")\n",
    "        groups = numpy_unique(yt)\n",
    "\n",
    "        def compute_cov(group, Xt, yt):\n",
    "            Xgt = Xt[T.eq(yt, group).nonzero()[0], :]\n",
    "            Xgt_bar = Xgt - T.mean(Xgt, axis=0)\n",
    "            m = T.cast(Xgt_bar.shape[0], 'float32')\n",
    "            return (1.0 / (m - 1)) * T.dot(Xgt_bar.T, Xgt_bar)\n",
    "\n",
    "        # scan over groups\n",
    "        covs_t, updates = theano.scan(fn=compute_cov, outputs_info=None,\n",
    "                                      sequences=[groups], non_sequences=[y_pred, yt])\n",
    "\n",
    "        # compute average covariance matrix (within scatter)\n",
    "        Sw_t = T.mean(covs_t, axis=0)\n",
    "\n",
    "        # compute total scatter\n",
    "        Xt_bar = y_pred - T.mean(y_pred, axis=0)\n",
    "        m = T.cast(Xt_bar.shape[0], 'float32')\n",
    "        St_t = (1.0 / (m - 1)) * T.dot(Xt_bar.T, Xt_bar)\n",
    "\n",
    "        # compute between scatter\n",
    "        Sb_t = St_t - Sw_t\n",
    "\n",
    "        # cope for numerical instability (regularize)\n",
    "        Sw_t += T.identity_like(Sw_t) * r\n",
    "\n",
    "        # return T.cast(T.neq(yt[0], -1), 'float32')*T.nlinalg.trace(T.dot(T.nlinalg.matrix_inverse(St_t), Sb_t))\n",
    "\n",
    "        # compute eigenvalues\n",
    "        evals_t = T.slinalg.eigvalsh(Sb_t, Sw_t)\n",
    "\n",
    "        # get eigenvalues\n",
    "        top_k_evals = evals_t[-n_components:]\n",
    "\n",
    "        # maximize variance between classes\n",
    "        # (k smallest eigenvalues below threshold)\n",
    "        thresh = T.min(top_k_evals) + margin\n",
    "        top_k_evals = top_k_evals[(top_k_evals <= thresh).nonzero()]\n",
    "        costs = T.mean(top_k_evals)\n",
    "\n",
    "        return -costs\n",
    "\n",
    "    return inner_lda_objective\n",
    "\n",
    "## Fit the model ##\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = create_model(x_train.shape[-1], reg_par=1e-5)\n",
    "\n",
    "model_optimizer = Adam()\n",
    "model.compile(loss=lda_loss(n_components=1, margin=1), optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lda_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c4fa72ce1ce2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlda_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_loss' is not defined"
     ]
    }
   ],
   "source": [
    "### Testing Keras with Theano backend ###\n",
    "\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 170\n",
    "# }\n",
    "\n",
    "# automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "\n",
    "## Create model ##\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Builds the model\n",
    "    The structure of the model can get easily substituted with a more efficient and powerful network like CNN\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1, input_shape=(4,), activation='sigmoid', kernel_regularizer=l2(1e-5))) \n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.compile(loss=lda_loss(n_components=1, margin=1), optimizer='adam')\n",
    "model.fit(x_train, y_train)\n",
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3719358722368876"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
