{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments using IMLY ###\n",
    "\n",
    "This notebook contains experimental runs of IMLY with different datasets.  \n",
    "The readings of these experiments can be referred to in this [sheet](https://docs.google.com/spreadsheets/d/1E5jcq2w42gN8bMIaeaRJpAdhgSVN-2XDJ_YTHe4qfwY/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #1\n",
    "\n",
    "#### Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "266/266 [==============================] - 0s 85us/step\n",
      "Uploading ../data/diabetes_linear_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "..."
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# X = preprocessing.scale(X)\n",
    "# Y = preprocessing.normalize(Y)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #2\n",
    "\n",
    "#### UCI Abalone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "794/794 [==============================] - ETA:  - 0s 77us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_abalone_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"uci_abalone\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\" if path.exists(\"../data/abalone.data.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #3\n",
    "\n",
    "#### UCI Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "60/60 [==============================] - ETA:  - 0s 6ms/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_iris_logistic_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "params = {\n",
    "    'epochs': 170\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 22]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "a, b = np.unique(y_train, return_counts=True)\n",
    "# len(y_test)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.509 - 0s 12ms/step - loss: 5.1482\n",
      "Epoch 2/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.240 - 0s 0us/step - loss: 5.1335\n",
      "Epoch 3/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.517 - 0s 391us/step - loss: 5.1188\n",
      "Epoch 4/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.011 - 0s 390us/step - loss: 5.1038\n",
      "Epoch 5/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.508 - 0s 391us/step - loss: 5.0873\n",
      "Epoch 6/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.215 - 0s 0us/step - loss: 5.0720\n",
      "Epoch 7/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.087 - 0s 390us/step - loss: 5.0557\n",
      "Epoch 8/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.836 - 0s 390us/step - loss: 5.0393\n",
      "Epoch 9/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.396 - 0s 391us/step - loss: 5.0238\n",
      "Epoch 10/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.395 - 0s 391us/step - loss: 5.0078\n",
      "Epoch 11/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.055 - 0s 391us/step - loss: 4.9915\n",
      "Epoch 12/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.953 - 0s 0us/step - loss: 4.9744\n",
      "Epoch 13/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.635 - 0s 391us/step - loss: 4.9605\n",
      "Epoch 14/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.623 - 0s 390us/step - loss: 4.9432\n",
      "Epoch 15/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.790 - 0s 391us/step - loss: 4.9275\n",
      "Epoch 16/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.494 - 0s 391us/step - loss: 4.9111\n",
      "Epoch 17/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.714 - 0s 390us/step - loss: 4.8952\n",
      "Epoch 18/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.580 - 0s 0us/step - loss: 4.8786\n",
      "Epoch 19/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.689 - 0s 391us/step - loss: 4.8623\n",
      "Epoch 20/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.878 - 0s 391us/step - loss: 4.8461\n",
      "Epoch 21/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.709 - 0s 0us/step - loss: 4.8294\n",
      "Epoch 22/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.673 - 0s 391us/step - loss: 4.8130\n",
      "Epoch 23/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.621 - 0s 390us/step - loss: 4.7966\n",
      "Epoch 24/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.869 - 0s 391us/step - loss: 4.7806\n",
      "Epoch 25/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.858 - 0s 0us/step - loss: 4.7642\n",
      "Epoch 26/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.628 - 0s 391us/step - loss: 4.7477\n",
      "Epoch 27/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.529 - 0s 391us/step - loss: 4.7315\n",
      "Epoch 28/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.308 - 0s 390us/step - loss: 4.7150\n",
      "Epoch 29/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.863 - 0s 0us/step - loss: 4.6993\n",
      "Epoch 30/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.463 - 0s 391us/step - loss: 4.6824\n",
      "Epoch 31/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.488 - 0s 391us/step - loss: 4.6661\n",
      "Epoch 32/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.270 - 0s 390us/step - loss: 4.6494\n",
      "Epoch 33/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.786 - 0s 391us/step - loss: 4.6336\n",
      "Epoch 34/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.702 - 0s 390us/step - loss: 4.6170\n",
      "Epoch 35/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.659 - 0s 781us/step - loss: 4.6007\n",
      "Epoch 36/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.110 - 0s 390us/step - loss: 4.5838\n",
      "Epoch 37/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.053 - 0s 0us/step - loss: 4.5675\n",
      "Epoch 38/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.404 - 0s 0us/step - loss: 4.5514\n",
      "Epoch 39/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.908 - 0s 391us/step - loss: 4.5354\n",
      "Epoch 40/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.359 - 0s 391us/step - loss: 4.5182\n",
      "Epoch 41/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.530 - 0s 391us/step - loss: 4.5022\n",
      "Epoch 42/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.781 - 0s 391us/step - loss: 4.4864\n",
      "Epoch 43/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.048 - 0s 390us/step - loss: 4.4707\n",
      "Epoch 44/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.526 - 0s 390us/step - loss: 4.4544\n",
      "Epoch 45/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.533 - 0s 0us/step - loss: 4.4391\n",
      "Epoch 46/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.277 - 0s 390us/step - loss: 4.4234\n",
      "Epoch 47/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.263 - 0s 391us/step - loss: 4.4079\n",
      "Epoch 48/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.745 - 0s 0us/step - loss: 4.3915\n",
      "Epoch 49/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.943 - 0s 390us/step - loss: 4.3771\n",
      "Epoch 50/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.038 - 0s 0us/step - loss: 4.3597\n",
      "Epoch 51/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.719 - 0s 391us/step - loss: 4.3433\n",
      "Epoch 52/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.439 - 0s 781us/step - loss: 4.3280\n",
      "Epoch 53/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.210 - 0s 391us/step - loss: 4.3111\n",
      "Epoch 54/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.314 - 0s 391us/step - loss: 4.2949\n",
      "Epoch 55/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.834 - 0s 391us/step - loss: 4.2779\n",
      "Epoch 56/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.171 - 0s 523us/step - loss: 4.2619\n",
      "Epoch 57/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.022 - 0s 0us/step - loss: 4.2452\n",
      "Epoch 58/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 5.090 - 0s 391us/step - loss: 4.2303\n",
      "Epoch 59/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.372 - 0s 390us/step - loss: 4.2130\n",
      "Epoch 60/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.004 - 0s 391us/step - loss: 4.1970\n",
      "Epoch 61/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.203 - 0s 392us/step - loss: 4.1817\n",
      "Epoch 62/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.513 - 0s 390us/step - loss: 4.1664\n",
      "Epoch 63/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.094 - 0s 390us/step - loss: 4.1501\n",
      "Epoch 64/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.275 - 0s 391us/step - loss: 4.1349\n",
      "Epoch 65/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.851 - 0s 0us/step - loss: 4.1188\n",
      "Epoch 66/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.861 - 0s 781us/step - loss: 4.1047\n",
      "Epoch 67/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.231 - 0s 391us/step - loss: 4.0882\n",
      "Epoch 68/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.723 - 0s 391us/step - loss: 4.0724\n",
      "Epoch 69/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.415 - 0s 0us/step - loss: 4.0566\n",
      "Epoch 70/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.104 - 0s 0us/step - loss: 4.0417\n",
      "Epoch 71/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.804 - 0s 391us/step - loss: 4.0250\n",
      "Epoch 72/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.847 - 0s 391us/step - loss: 4.0088\n",
      "Epoch 73/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.039 - 0s 390us/step - loss: 3.9928\n",
      "Epoch 74/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.277 - 0s 390us/step - loss: 3.9769\n",
      "Epoch 75/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.317 - 0s 0us/step - loss: 3.9608\n",
      "Epoch 76/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.784 - 0s 390us/step - loss: 3.9443\n",
      "Epoch 77/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.753 - 0s 390us/step - loss: 3.9287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.743 - 0s 391us/step - loss: 3.9128\n",
      "Epoch 79/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.752 - 0s 390us/step - loss: 3.8968\n",
      "Epoch 80/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.988 - 0s 391us/step - loss: 3.8810\n",
      "Epoch 81/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.013 - 0s 391us/step - loss: 3.8649\n",
      "Epoch 82/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.802 - 0s 390us/step - loss: 3.8486\n",
      "Epoch 83/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.492 - 0s 390us/step - loss: 3.8322\n",
      "Epoch 84/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.399 - 0s 391us/step - loss: 3.8160\n",
      "Epoch 85/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.101 - 0s 391us/step - loss: 3.8007\n",
      "Epoch 86/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 4.168 - 0s 391us/step - loss: 3.7843\n",
      "Epoch 87/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.587 - 0s 391us/step - loss: 3.7674\n",
      "Epoch 88/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.915 - 0s 0us/step - loss: 3.7522\n",
      "Epoch 89/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.781 - 0s 0us/step - loss: 3.7361\n",
      "Epoch 90/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.762 - 0s 0us/step - loss: 3.7204\n",
      "Epoch 91/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.485 - 0s 0us/step - loss: 3.7043\n",
      "Epoch 92/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.315 - 0s 391us/step - loss: 3.6881\n",
      "Epoch 93/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.937 - 0s 390us/step - loss: 3.6731\n",
      "Epoch 94/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.400 - 0s 0us/step - loss: 3.6560\n",
      "Epoch 95/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.745 - 0s 0us/step - loss: 3.6405\n",
      "Epoch 96/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.762 - 0s 390us/step - loss: 3.6244\n",
      "Epoch 97/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.713 - 0s 390us/step - loss: 3.6084\n",
      "Epoch 98/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.460 - 0s 391us/step - loss: 3.5922\n",
      "Epoch 99/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.474 - 0s 391us/step - loss: 3.5764\n",
      "Epoch 100/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.676 - 0s 391us/step - loss: 3.5608\n",
      "Epoch 101/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.412 - 0s 391us/step - loss: 3.5444\n",
      "Epoch 102/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.191 - 0s 391us/step - loss: 3.5281\n",
      "Epoch 103/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.802 - 0s 391us/step - loss: 3.5130\n",
      "Epoch 104/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.355 - 0s 0us/step - loss: 3.4960\n",
      "Epoch 105/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.545 - 0s 0us/step - loss: 3.4804\n",
      "Epoch 106/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.301 - 0s 0us/step - loss: 3.4639\n",
      "Epoch 107/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.530 - 0s 390us/step - loss: 3.4483\n",
      "Epoch 108/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.769 - 0s 391us/step - loss: 3.4327\n",
      "Epoch 109/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.130 - 0s 391us/step - loss: 3.4158\n",
      "Epoch 110/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.464 - 0s 391us/step - loss: 3.4007\n",
      "Epoch 111/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.222 - 0s 0us/step - loss: 3.3844\n",
      "Epoch 112/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.709 - 0s 0us/step - loss: 3.3694\n",
      "Epoch 113/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.640 - 0s 391us/step - loss: 3.3534\n",
      "Epoch 114/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.449 - 0s 391us/step - loss: 3.3377\n",
      "Epoch 115/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.202 - 0s 390us/step - loss: 3.3220\n",
      "Epoch 116/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.037 - 0s 0us/step - loss: 3.3064\n",
      "Epoch 117/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.792 - 0s 0us/step - loss: 3.2902\n",
      "Epoch 118/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.338 - 0s 0us/step - loss: 3.2751\n",
      "Epoch 119/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.517 - 0s 391us/step - loss: 3.2591\n",
      "Epoch 120/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.152 - 0s 391us/step - loss: 3.2422\n",
      "Epoch 121/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.443 - 0s 0us/step - loss: 3.2270\n",
      "Epoch 122/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.158 - 0s 0us/step - loss: 3.2106\n",
      "Epoch 123/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.128 - 0s 391us/step - loss: 3.1949\n",
      "Epoch 124/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.026 - 0s 391us/step - loss: 3.1790\n",
      "Epoch 125/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.434 - 0s 0us/step - loss: 3.1639\n",
      "Epoch 126/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.406 - 0s 0us/step - loss: 3.1480\n",
      "Epoch 127/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.360 - 0s 391us/step - loss: 3.1325\n",
      "Epoch 128/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.351 - 0s 0us/step - loss: 3.1174\n",
      "Epoch 129/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.308 - 0s 0us/step - loss: 3.1024\n",
      "Epoch 130/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.160 - 0s 391us/step - loss: 3.0874\n",
      "Epoch 131/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.276 - 0s 391us/step - loss: 3.0729\n",
      "Epoch 132/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.797 - 0s 0us/step - loss: 3.0573\n",
      "Epoch 133/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.941 - 0s 0us/step - loss: 3.0426\n",
      "Epoch 134/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.097 - 0s 391us/step - loss: 3.0276\n",
      "Epoch 135/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.317 - 0s 391us/step - loss: 3.0126\n",
      "Epoch 136/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.871 - 0s 0us/step - loss: 2.9965\n",
      "Epoch 137/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.615 - 0s 0us/step - loss: 2.9808\n",
      "Epoch 138/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.624 - 0s 391us/step - loss: 2.9652\n",
      "Epoch 139/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.206 - 0s 391us/step - loss: 2.9504\n",
      "Epoch 140/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.321 - 0s 0us/step - loss: 2.9345\n",
      "Epoch 141/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.950 - 0s 0us/step - loss: 2.9182\n",
      "Epoch 142/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.817 - 0s 0us/step - loss: 2.9027\n",
      "Epoch 143/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.303 - 0s 0us/step - loss: 2.8864\n",
      "Epoch 144/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.756 - 0s 0us/step - loss: 2.8715\n",
      "Epoch 145/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.132 - 0s 391us/step - loss: 2.8561\n",
      "Epoch 146/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.777 - 0s 390us/step - loss: 2.8392\n",
      "Epoch 147/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.689 - 0s 391us/step - loss: 2.8233\n",
      "Epoch 148/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.030 - 0s 0us/step - loss: 2.8082\n",
      "Epoch 149/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.840 - 0s 391us/step - loss: 2.7921\n",
      "Epoch 150/150\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.820 - 0s 391us/step - loss: 2.7766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imly import dope\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1,\n",
    "                input_dim=4,\n",
    "                activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy')\n",
    "\n",
    "# m = dope(LogisticRegression())\n",
    "# m.fit(x_train.values, y_train.values)\n",
    "# m.predict(x_test)\n",
    "model.fit(x_train.values, y_train.values, epochs=150)\n",
    "test = model.predict(x_test)\n",
    "test.argmax(axis=-1)\n",
    "# value, count = np.unique(test, return_counts=True)\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../data/uci_iris_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://mlsquare-pdf.s3.ap-south-1.amazonaws.com:443/uci_iris_logistic_regression.pdf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto\n",
    "import sys\n",
    "from boto.s3.key import Key\n",
    "# from boto.s3.key import Key\n",
    "fig_path = '../data/uci_iris_logistic_regression.pdf'\n",
    "fig_name = 'uci_iris_logistic_regression.pdf'\n",
    "bucket_name = 'mlsquare-datasets'\n",
    "credentials_json = json.load(open('../data/aws_credentials.json'))\n",
    "AWS_ACCESS_KEY_ID = credentials_json['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = credentials_json['AWS_SECRET_ACCESS_KEY']\n",
    "REGION_HOST = 's3.ap-south-1.amazonaws.com'\n",
    "\n",
    "# bucket_name = AWS_ACCESS_KEY_ID.lower() + '-dump'\n",
    "conn = boto.connect_s3(AWS_ACCESS_KEY_ID,\n",
    "                       AWS_SECRET_ACCESS_KEY, host=REGION_HOST)\n",
    "bucket = conn.get_bucket('mlsquare-pdf', validate=False)\n",
    "\n",
    "# bucket = conn.create_bucket(bucket_name,\n",
    "#     location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "print('Uploading %s to Amazon S3 bucket %s' % (fig_path, bucket_name))\n",
    "\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "k = Key(bucket)\n",
    "k.key = fig_name\n",
    "k.set_contents_from_filename(fig_path,\n",
    "                             cb=percent_cb, num_cb=10)  # upload file\n",
    "url = k.generate_url(expires_in=0, query_auth=False)\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #4\n",
    "\n",
    "#### UCI Adult salary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:29<00:00, 29.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "27134/27134 [==============================] - ETA: 26 - ETA: 1 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 25us/step\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_adult_salary_logistic_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_adult_salary\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\" \", header=None, names=names)\n",
    "\n",
    "\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)\n",
    "\n",
    "# Split the dataset into test and train datasets\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #5\n",
    "\n",
    "#### UCI Ad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "1416/1416 [==============================] - ETA:  - ETA:  - 0s 67us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_ad_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_name = \"uci_ad\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/ad.data.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "data = data.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Label encoding #\n",
    "\n",
    "lb = LabelEncoder()\n",
    "Y = lb.fit_transform(data.iloc[:, -1])\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #6\n",
    "\n",
    "#### UCI Mushroom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info #\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_mushroom\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields with missing values\n",
      "stalk-root\n",
      "2480\n",
      "30.53%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = ['classes', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n",
    "        'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "        'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "        'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color',\n",
    "        'population', 'habitat']\n",
    "url = \"../data/mushroom.data.csv\" if path.exists(\"../data/dataset.csv.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "print(\"Fields with missing values\")\n",
    "col_names = data.columns\n",
    "num_data = data.shape[0]\n",
    "for c in col_names:\n",
    "    num_non = data[c].isin([\"?\"]).sum()\n",
    "    if num_non > 0:\n",
    "        print (c)\n",
    "        print (num_non)\n",
    "        print (\"{0:.2f}%\".format(float(num_non) / num_data * 100))\n",
    "        print (\"\\n\")\n",
    "\n",
    "data = data[data[\"stalk-root\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "\n",
    "for col in names:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "# Split the dataset into test and train datasets #\n",
    "feature_list = names[1:23]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['classes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "3387/3387 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 63us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/uci_mushroom_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #7\n",
    "\n",
    "#### Covertype dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"covertype\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/covtype.data.csv\", delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "data = data[data[54].isin([1,2])]\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [10:42<00:00, 642.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "297085/297085 [==============================] - ETA: 3: - ETA: 13s - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 8s 28us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/covertype_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAEYCAYAAADcRnS9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XecVcX5x/HPl12aIoIgRUBBxYLEAhbUaFAUQVGIUYNBREOCGjUa488WI7GQoDFqSCxBQbBEJDZQUSTYDUVQFLEAYltZhHVpFtry/P44s3hZ7t6+LLv3efs6r71nzpw5c1l8mDlzzozMDOecc+mpU90VcM65msiDp3POZcCDp3POZcCDp3POZcCDp3POZcCDp3POZcCDZy0jqaGkpyWtlPSfLMoZIOmFXNatukg6StJH1V0PV7vIn/OsHpJ+AVwG7AOsBuYAw8zs9SzLHQhcDBxhZhuyrug2TpIBHc1sYXXXxeUXb3lWA0mXAXcAfwZaArsCdwF9c1D8bsD8fAicqZBUWN11cLWUmfm2FTdgR+Ab4PQEeeoTBdfFYbsDqB+OdQeKgN8DS4Fi4Nxw7HpgHbA+XGMw8CfgoZiy2wMGFIb9c4BFRK3fT4ABMemvx5x3BPAmsDL8PCLm2MvAjcAboZwXgOaVfLfy+l8RU/9+wInAfKAUuCYm/6HANGBFyPtPoF449mr4Lt+G7/vzmPKvBJYAD5anhXP2CNfoEvZ3AUqA7tX9d8O3mrV5y3PrOxxoADyZIM8fgG7AgcABRAHk2pjjrYiCcBuiAHmnpKZmNpSoNfuomTUys1GJKiJpe2AE0NvMdiAKkHPi5NsJeDbkbQbcBjwrqVlMtl8A5wItgHrA5Qku3Yroz6ANcB1wL3AW0BU4CrhO0u4hbxnwO6A50Z9dD+A3AGZ2dMhzQPi+j8aUvxNRK3xI7IXN7GOiwPqwpO2A+4ExZvZygvo6twUPnltfM6DEEnerBwA3mNlSM1tG1KIcGHN8fTi+3swmEbW69s6wPhuBzpIamlmxmc2Lk+ckYIGZPWhmG8zsEeBD4OSYPPeb2Xwz+x4YTxT4K7Oe6P7uemAcUWD8u5mtDtefB+wPYGazzWx6uO6nwL+An6TwnYaa2dpQn82Y2b3AAmAG0JroHyvn0uLBc+v7Gmie5F7cLsBnMfufhbRNZVQIvt8BjdKtiJl9S9TVPR8olvSspH1SqE95ndrE7C9Joz5fm1lZ+Fwe3L6KOf59+fmS9pL0jKQlklYRtaybJygbYJmZrUmS516gM/APM1ubJK9zW/DgufVNA9YQ3eerzGKiLme5XUNaJr4FtovZbxV70Mwmm9nxRC2wD4mCSrL6lNfpywzrlI67ierV0cwaA9cASnJOwkdIJDUiuo88CvhTuC3hXFo8eG5lZraS6D7fnZL6SdpOUl1JvSXdErI9AlwraWdJzUP+hzK85BzgaEm7StoRuLr8gKSWkk4J9z7XEnX/y+KUMQnYS9IvJBVK+jnQCXgmwzqlYwdgFfBNaBVfUOH4V8DuW5yV2N+B2Wb2K6J7ufdkXUuXdzx4VgMzu43oGc9rgWXAF8BFwFMhy03ALOBdYC7wVkjL5FpTgEdDWbPZPODVIRq1X0w0Av0TwmBMhTK+BvqEvF8TjZT3MbOSTOqUpsuJBqNWE7WKH61w/E/AWEkrJJ2RrDBJfYFeRLcqIPo9dJE0IGc1dnnBH5J3zrkMeMvTOecy4MHTOecy4MHTOecy4MHTOecysE1NmrDdjk2tScs2yTO6bcaO9etWdxVcGoq//JwVpV8ne042LQWNdzPbsMWLXHHZ98smm1mvXF6/umxTwbNJyzb86h9PVHc1XBpO2rNFdVfBpeGcfsfkvEzb8D319076lBgAa+bcmeztsBpjmwqezrmaSKD8uwPowdM5lx0BdQqquxZbnQdP51z2lNPbqDWCB0/nXJa82+6cc5nxlqdzzqVJeMvTOefSJ295OudcRny03Tnn0uUDRs45lz7h3XbnnMuItzydcy5d3m13zrnM1PFuu3POpSdP323Pv7a2cy7HQrc9lS1ZSdJoSUslvVch/WJJH0maF7NEN5KulrQwHDshJr1XSFso6aqY9A6SZkhaIOlRSfVCev2wvzAcb5+srh48nXPZk1LbkhtDtDR0TNE6BugL7G9m+wG3hvROQH9gv3DOXZIKJBUAdwK9gU7AmSEvwM3A7WbWEVgODA7pg4HlZrYncHvIl5AHT+dc9nLU8jSzV4HSCskXAMPNbG3IszSk9wXGmdlaM/sEWAgcGraFZrbIzNYB44C+kgQcCzwWzh8L9Ispa2z4/BjQI+SvlAdP51x2Um11RrGouaRZMduQFK6wF3BU6E6/IumQkN4G+CImX1FIqyy9GbDCzDZUSN+srHB8ZchfKR8wcs5lL/UBoxIzOzjN0guBpkA34BBgvKTdiYaqKjLiNwotQX6SHKu0Us45l4Uqf86zCHjCzAyYKWkj0Dykt4vJ1xZYHD7HSy8BmkgqDK3L2PzlZRVJKgR2ZMvbB5vxbrtzLnu5GzCK5ymie5VI2guoRxQIJwL9w0h5B6AjMBN4E+gYRtbrEQ0qTQzB9yXgtFDuIGBC+Dwx7BOOvxjyV8pbns657ORwPk9JjwDdie6NFgFDgdHA6PD40jpgUAhs8ySNB94HNgAXmllZKOciYDJQAIw2s3nhElcC4yTdBLwNjArpo4AHJS0kanH2T1ZXD57OuSzlrttuZmdWcuisSvIPA4bFSZ8ETIqTvohoNL5i+hrg9HTq6sHTOZc9n1XJOecykIevZ3rwdM5lRz6rknPOZca77c45l74kbzLWSh48nXNZiVbh8ODpnHPpEfFfbqzlPHg657Ik6tTxASPnnEubd9udcy4DHjydcy5dfs/TOefSJ+QtT+ecy4QPGDnnXAa85emcc+nye57OOZeZfGx55t+NCudcTpUPGKWyJS1LGi1paZg1vuKxyyWZpOZhX5JGSFoo6V1JXWLyDpK0IGyDYtK7SpobzhlRvrywpJ0kTQn5p0hqmqyuHjydc1nLVfAExgC94pTfDjge+DwmuTfRukUdgSHA3SHvTkTLdxxGNGv80JhgeHfIW35e+bWuAqaaWUdgathPyIOncy47AtVRSlsyZvYq8VetvB24gs2XA+4LPGCR6UQrY7YGTgCmmFmpmS0HpgC9wrHGZjYtrIH0ANAvpqyx4fPYmPRK+T1P51zW0rjn2VzSrJj9kWY2MknZpwBfmtk7Fa7TBvgiZr8opCVKL4qTDtDSzIoBzKxYUotkX8SDp3Mua2kEzxIzOziNcrcD/gD0jHc4TpplkJ4R77Y757KSywGjOPYAOgDvSPoUaAu8JakVUcuxXUzetsDiJOlt46QDfBW69YSfS5NVzIOncy57SnFLk5nNNbMWZtbezNoTBcAuZrYEmAicHUbduwErQ9d7MtBTUtMwUNQTmByOrZbULYyynw1MCJeaCJSPyg+KSa+Ud9tTVPLFIp74y+827S9f8gXdB/6W3Q7oxqQRQ1m35juatGzDT6+4lfrbN6Jsw3qeueNaihe+z8ayDezfox8/7n8eABNvu5oFM15m+ybNOP9fz2x2nZkTHuTNiQ9Rp6CQjof+hON+dcVW/Z61UVlZGef2O4adW7Xmb/c+ynWX/ZoP586hsLCQTgd05aobb6ewbl3MjNtuvIppL0+hfsOG/PHmu9in8wEAHLFXM/bYuxMALVu35daRjwBwwxW/4e2Zb9Boh8YA/PHmu9ir04+q54tWF+XuOU9JjwDdie6NFgFDzWxUJdknAScCC4HvgHMBzKxU0o3AmyHfDWZWPgh1AdGIfkPgubABDAfGSxpMNKKfdA13D54pat5ud4bcFf1jtLGsjDvOOpq9jziex276Lcf/+kp22/9Q5kx+jP89dh/HDLqU9197ng3r13H+PU+zfs333D3kJDp3P4kmrdpywPGncsjJZzHh1is3u8an70xn/rSpnHf30xTWq8e3K76ujq9a6zw65h7a77kX336zGoBep5zO9X+Lxiiu+92vmDD+AX42YDDTXpnCF59+zH+mzmbenFncMvT3jH78vwDUb9CQB59+LW75F195A8f27rt1vsw2KlfvtpvZmUmOt4/5bMCFleQbDYyOkz4L6Bwn/WugRzp19W57Bj6ZM42mrdvRpGUbvv7yE3b90SEAdOhyJB++8QIQ3Qdav+Z7NpZtYP26NRTUrUv97RsBsNuPDqHhDjtuUe6sZx7hiDOGUFivHgDbN2m2lb5R7bW0+Ev+9/ILnHLG2ZvSjujec9M9uE77d2Xpkui216v/ncSJP+2PJDofdAjfrFpJydIl1VX1mqWKuu3bMg+eGZj3yrN07t4HgBa77cX86VMB+ODV51m1rBiAfY86gboNGnL7L37MiIHHcPjPfknDHZokLLf0y0/5fN4sRl1yOmP/7ywWf/Ru1X6RPHD7Tddw0ZXXozjrim9Yv57nnnqUw4+OGhzLviqmRes2m463aLULy76Kfp/r1q7hnH7HMPhnx/PKlGc3K+ee225iwElHcsdN17Bu7doq/DbbriocMNpmVVnwTPSaVU1Wtn4d86e/yL5HRS8mnHzZMGY9/W/uvehU1n7/LQWFUatx8UfvUqdOHS59+DUuHjuVaY+PZnnxF4mKZmNZGWtWr+KXd4znuF9dweN/vpSoZ+Iy8fqLz9O0WXP26Xxg3OO3DL2cgw49ggMPOQIg7p91+f/wT706lzFPvcQNt9/L7TddTdFnnwDwm8uv49EXZnL/Ey+yauVyHhz59yr6NtuuVAOnB8/UjSHOa1Y13cJZr9J6z/1o1LQ5AM3b7cGAP4/m1/98gs7dT6Jp6+gJifdeeoY9uh5FQWFdtm/SjHb7dWHxgrkJy27cvCX7HHk8kmiz9/6oTh2+W7m8yr9TbfXu7Bm8NvV5+v1kf/546WBmTXuNoZcNAeC+ETezorSES64Ztil/i1a7sLT4y037S5cspnmLVgDs3LI1AG12bU+Xw37M/PejXkHzFq2QRL369TnpZwN4/93ZW+vrbVM8eOZQgtesarT3Xn6W/bqftGm/fFDHNm7ktUfuputJ/QFo3KI1n74zAzNj3Zrv+PLDd2jedveEZe99xHF8+s50AL4u+oSy9evZbsek8xO4Svzm/4by9BvzeOqVd7nxjlEcfPhRXH/bSCY8+gAzXpvKDXfct9lAx1E9ejPpyXGYGe+9/SaNdmhM8xatWLVyxabu+IrSr3l39gw67Lk3wKZ7ombGq/99lt077rv1v+g2IB+DZ7WPtksaQvSiPju22KWaa5PY+jXf88lb/+Ok396wKe29l59h1tP/BmCfI4/ngJ4/A+CQkwcw8W9Xc895fQDjgONPpeXu+wDwxF8u47N3Z/LdquXccdbR/OSsizmo1+kc2PNnTLztGu45rw8FhXU55fLhte4v3Lbglusuo9Uu7fj16dFLK917nszgi6/giO49+d/LUzjt2C40aNiQa2++E4BPP/6Im6/9HapTB9u4kbPPu5QOHaPf5dDLhrCitAQzo+O+P+LKG2+rtu9VnVJ5b722UVXeU5PUHnjGzLZ4NCCeXfbqbL/6xxNVVh+XeyftmfQVYLcNOaffMXww9+2cRrr6rTpa2wEjUsq76LYTZ6fzeua2rNpbns65mk1APnaQPHg657JU++5npqIqH1V6BJgG7C2pKLz25JyrhaTUttqkylqeyV6zcs7VEoI6eThg5N1251xWhAdP55zLSG3rkqfCg6dzLmv5OGDkwdM5l51aOBiUCg+ezrmsRM955l/09CnpnHNZEnXqpLYlLSnObGyS/irpQ0nvSnpSUpOYY1dLWijpI0knxKT3CmkLJV0Vk95B0gxJCyQ9KqleSK8f9heG4+2T1dWDp3MuazmcGGQMW87GNgXobGb7A/OBq8M1OwH9gf3COXdJKpBUANwJ9AY6AWeGvAA3A7ebWUdgOVD+/PlgYLmZ7Um0RvzNySrqwdM5l50UH5BPJXbGm43NzF4wsw1hdzo/rIDZFxhnZmvN7BOitYwODdtCM1tkZuuAcUDfsOjbscBj4fyxQL+YssaGz48BPZQk2nvwdM5lpfyeZ4otz+aSZsVsQ9K83C/5YdG2NkDsDONFIa2y9GbAiphAXJ6+WVnh+MqQv1I+YOScy1oa40Ulmc6qJOkPwAbg4fKkONmM+I1CS5A/UVmV8uDpnMtaVY+2SxoE9AF62A/zaBYB7WKytQUWh8/x0kuAJpIKQ+syNn95WUWSCoEdSTKZu3fbnXPZCe+252K0PW7xUi/gSuAUM/su5tBEoH8YKe8AdARmEq3X3jGMrNcjGlSaGILuS8Bp4fxBwISYsgaFz6cBL1qSyY695emcy0ou5/MMs7F1J7o3WgQMJRpdrw9MCS3c6WZ2vpnNkzQeeJ+oO3+hmZWFci4CJgMFwGgzmxcucSUwTtJNwNvAqJA+CnhQ0kKiFmf/ZHX14Omcy1Lu5vOsZDa2UXHSyvMPA4bFSZ8ETIqTvohoNL5i+hrg9HTq6sHTOZe1PHzByIOncy57+fh6pgdP51xW5JMhO+dcZrzl6ZxzGcjD2OnB0zmXPW95OudcunwyZOecS5/ydN12D57OuawV+Gi7c86lLw8bnh48nXPZiSY6zr/oWWnwlNQ40Ylmtir31XHO1UR52GtP2PKcx5YTiJbvG7BrFdbLOVeDeMszhpm1q+yYc87FysPYmdpkyJL6S7omfG4rqWvVVss5V1MIKJBS2mqTpMFT0j+BY4CBIek74J6qrJRzrgZJcfG32ta1T6XleYSZnQesATCzUqBeldbKOVej5GrpYUmjJS2V9F5M2k6SpkhaEH42DemSNELSQknvSuoSc86gkH9BWP+oPL2rpLnhnBHlywtXdo1EUgme6yXVIawkJ6kZsDGF85xzeUBAHSmlLQVjgF4V0q4CpppZR2Bq2AfoTbRuUUdgCHA3RIGQaPmOw4hmjR8aEwzvDnnLz+uV5BqVSiV43gk8Duws6XrgdeDmFM5zzuWJXLU8zexVtly1si8wNnweC/SLSX/AItOJVsZsDZwATDGzUjNbDkwBeoVjjc1sWljc7YEKZcW7RqWSPiRvZg9Img0cF5JON7P3Ep3jnMsfaU6G3FzSrJj9kWY2Msk5Lc2sGMDMiiW1COltgC9i8hWFtETpRXHSE12jUqm+YVQArKfyBeWdc3ksxS45QImZHZyjy8a7aMVn01NJz0gqo+1/AB4BdiFaJP7fkq7O9ILOudpHKW4Z+ip0uQk/l4b0IiD2efS2wOIk6W3jpCe6RqVSaUWeBRxiZtea2R+IbsCencJ5zrk8UcWPKk0EykfMBwETYtLPDqPu3YCVoes9GegpqWkYKOoJTA7HVkvqFkbZz65QVrxrVCqVbvtnFfIVAotSOM85lwei0fYclSU9AnQnujdaRDRqPhwYL2kw8Dk/rK8+CTgRWEj0/Pm5ED1OKelG4M2Q74bwiCXABUQj+g2B58JGgmtUKtHEILcT3Q/4DpgnaXLY70k04u6cc5seks8FMzuzkkM94uQ14MJKyhkNjI6TPgvoHCf963jXSCRRy7N8RH0e8GxM+vR0LuCcq/186eEYZjZqa1bEOVcz5bLbXpMkvecpaQ9gGNAJaFCebmZ7VWG9nHM1SG17bz0VqYy2jwHuJ/oHpjcwHhhXhXVyztUwVfyo0jYpleC5nZlNBjCzj83sWqJZlpxzLnrDKHfvttcYqTyqtDY8E/WxpPOBL4Gkry455/JHLYuLKUkleP4OaAT8luje547AL6uyUs65msVH2+Mwsxnh42p+mBDZOecAELWvS56KRA/JP0mCl+bN7NQqqZFzrmZJcbq52iZRy/OfW60WQesdGnBND38CqiZpeshF1V0Fl4a1H39ZJeXm46NKiR6Sn7o1K+Kcq7nycZ7KVOfzdM65uIS3PJ1zLiOFedj0TDl4SqpvZmursjLOuZonWp8o/1qeqcwkf6ikucCCsH+ApH9Uec2cczVGHaW21SapNLZHAH2ArwHM7B389UznXIxcrZ5Zk6QSPOuY2WcV0sqqojLOuZonl+u2S/qdpHmS3pP0iKQGkjpImiFpgaRHJdULeeuH/YXhePuYcq4O6R9JOiEmvVdIWygp6drsiaQSPL+QdChgkgokXQrMz+aizrnapUCpbYlIakP0GvjBZtaZaNXe/sDNwO1m1hFYDgwOpwwGlpvZnsDtIR+SOoXz9gN6AXeF2FUA3Ek0O1wn4MyQNyOpBM8LgMuAXYGvgG4hzTnnUIqtzhRf4SwEGkoqBLYDioFjgcfC8bFAv/C5b9gnHO8RJjHqC4wzs7Vm9gnRGkeHhm2hmS0ys3VEU2v2zfR7p/Ju+1KiKO6cc3GlcT+zuaRZMfsjzWwkgJl9KelWogXYvgdeAGYDK8xsQ8hfBLQJn9sAX4RzN0haCTQL6bHLBcWe80WF9MNSrnkFqcwkfy9x3nE3syGZXtQ5V7ukMZJeYmYHxzsQlgnuC3QAVgD/IepiV1Qej+Jd1RKkx+tpVzp/RzKpPOf535jPDYCfsnn0ds7lsfIBoxw4DvjEzJYBSHoCOAJoIqkwtD7bAotD/iKgHVAUuvk7AqUx6eViz6ksPW2pdNsfjd2X9CAwJdMLOudqnxw9hvQ50E3SdkTd9h7ALOAl4DSie5SDgAkh/8SwPy0cf9HMTNJE4N+SbgN2AToCM4nifEdJHYgmde8P/CLTymbyemYHYLdML+icq2UEBTmInmY2Q9JjwFvABuBtYCTR0ufjJN0U0spX9h0FPChpIVGLs38oZ56k8cD7oZwLzawMQNJFwGSikfzRZjYv0/qmcs9zOT/cF6gTKpnV81HOudojl0sPm9lQYGiF5EVEI+UV864BTq+knGFEK19UTJ8ETMq+pkmCZxj2P4CoiQuw0cwyvsHqnKudaturl6lI+JxnCJRPmllZ2DxwOue2ICmlrTZJ5SH5mZK6VHlNnHM1Unm3Pd8mBkm0hlH5owE/Bn4t6WPgW6I/KzMzD6jOOV/DKI6ZQBd+eBXKOee2IKCwtjUrU5AoeArAzD7eSnVxztVQ3vLc3M6SLqvsoJndVgX1cc7VOKJO3Dcia7dEwbMAaET890Sdcw4oXwCuumux9SUKnsVmdsNWq4lzrmaqhSPpqUh6z9M55xIRUJCH0TNR8Oyx1WrhnKvRcjSrUo1SafA0s9KtWRHnXM2Vh7Ezo1mVnHNuE5Haq4q1jQdP51x2RK17bz0VHjydc1nLv9CZn61t51wOiWgy5FS2pGVJTSQ9JulDSR9IOlzSTpKmhHXbp4S1jlBkRFiD/d3YCYwkDQr5F0gaFJPeVdLccM4IZdFk9uDpnMualNqWgr8Dz5vZPkRzCX9ANPn61LBu+1R+mIy9N9ESGx2BIcDdUV20E9GEyocRTaI8tDzghjxDYs7rlel39uDpnMtSanN5JmvkSWoMHE1YZsPM1pnZCjZfn73iuu0PWGQ60UJxrYETgClmVmpmy4nWXOsVjjU2s2lhbuIHyGLiIw+ezrmslI+2p7IlsTuwDLhf0tuS7pO0PdDSzIoBws8WIf+mdduD8vXZE6UXxUnPiAdP51zW0mh5Npc0K2YbElNMIdE0mHeb2UFE8wcnWi8t3XXbK0vPiI+2O+eylsaoS4mZHVzJsSKgyMxmhP3HiILnV5Jam1lx6Hovjckfbx32IqB7hfSXQ3rbOPkz4i1P51xWpNyMtpvZEuALSXuHpB5EyweXr88OW67bfnYYde8GrAzd+slAT0lNw0BRT2ByOLZaUrcwyn52TFlp85ancy5rOXxI/mLgYUn1iJYcPpeokTde0mDgc35YbngScCKwEPgu5MXMSiXdCLwZ8t0Q87r5BcAYoCHwXNgy4sHTOZe1XIVOM5sDxOvWbzFRURgxv7CSckYDo+OkzwI6Z1lNwIOncy4H8vDtTA+ezrnsRI8q5V/09ODpnMuatzydcy5t8smQnXMuXd5td865TKQ+6Uet4sHTOZc1D57OOZcBebfdZeK8X/2S5yY9w84tWjB7znsAlJaWMvAXP+ezzz5lt93a89Aj42natCmvvvIyp5/al/btOwDQ96encs2111Vn9WuVe4YOoPfRnVlWupqDT/8zAA8OP5eO7VsC0GSHhqxY/T3d+g9n19Y7MeeJa5n/WfSq9My5n/LbYeMA+NOFJzOgz6E0abwdOx/5+03l16tbyKgbB3LQvrtSuvJbzrpyNJ8Xl1K3sIB/XnsmXTrtykbbyOW3PM5rsxds5W9fPconQ843/m57DgwcdA4Tnnl+s7RbbxlO92N78N4HC+h+bA9uvWX4pmNH/vgoZsyew4zZczxw5tiDT0+n74V3bpY28Kr76dZ/ON36D+epqXOY8OKcTccWFZVsOlYeOAEmvTqXowb+dYvyz+l3OMtXf0/nvtfzj4dfYtglfQH45alHAnDIGX+mz/n/ZPhlP82rdX1yOBlyjeHBMwd+fNTR7LTTTpulPfP0BM4aGM1lcNbAQTw98anqqFreeeOtjyld+V2lx392fBfGPz87aTkz537KkpJVW6T36b4/Dz8dTfrzxH/fpvuh0RwW++zeipdmfgTAsuXfsHL193TttGsmX6FGUor/1SYePKvI0q++onXr1gC0bt2aZUuXbjo2Y/o0Du1yAH379Ob9efOqq4p558gue/BV6Wo+/nzZprT2bZox7ZEreeG+SzjyoD2SlrFLix0pWrIcgLKyjaz65nuaNdmeufO/5OTuP6KgoA677dKMgzq1o22rpklKqx0E1FFqW21SZfc8JbUjmua+FbARGGlmf6+q69UUBx7UhY8+/oxGjRrx/HOTOOO0frz3QX7cG6tuZ/Q6mP88P2vT/pKSVezV+zpKV37LQfu2Y/xtQ+hy2jBWf7um0jLidcXNYOyEaezToSVvPHwFnxeXMv2dT9hQVlYl32PbU/talamoypbnBuD3ZrYv0A24UFKnKrzeNqVFy5YUFxcDUFxczM4topUDGjduTKNGjQDo1ftE1q9fT0lJSbXVM18UFNSh77EH8NjktzalrVu/gdKV3wLw9gdfsKiohI67taisCAC+/GrFphZlQUEdGjdqSOnKbykr28gVf3uCbv2Hc8bvRtJkh4ads4HKAAALzklEQVQsjGnh1mop3u/0e54pMrNiM3srfF5NtApexuuF1DQn9TmFhx6M1qx66MGx9Dk5GlhYsmQJ0Uxa8ObMmWzcuJFmzZpVWz3zxbGH7c38T7/iy6UrNqU1b9qIOqEv2b5NM/bcdWc+KUr8D9mzr8xlwMmHAXDqcQfxypvzAWjYoC7bNagXrrUPG8o28uGiJVXxVbY5uVx6uCbZKo8qSWoPHATMiHNsCNFSoLTbtWbeYD/7rDN57ZWXKSkpYY/2bfnjdddz+RVXcdaZZzD2/lG0a7crD4/7DwBPPv4Y9468m8KCQho0bMgDD43Lq1HZqjb2L+dwVNeONG/SiIXP38iN90xi7FPTOP2ErlsMFP24y5788YKT2FBWRlmZcfGwcSxfFQ02DbukLz/vfTDbNajLwudv5P4npzHsX5MY89T/GH3T2bw3YSjLV33LwKvuB2Dnpjvw9F0XsnGjsXjZCgZfO3aLutVm+fg3WOWtoCq7gNQIeAUYZmZPJMrbtevB9saMWYmyuG1M00Muqu4quDSs/Wg8G79bmtNYt++PDrL7n3oppbyH79l0doI1jACQVADMAr40sz6SOgDjgJ2At4CBZrZOUn2icZWuwNfAz83s01DG1cBgoAz4rZlNDum9iNaGLwDuM7PhZKhKR9sl1QUeBx5OFjidczVXjh9VuoToNl+5m4HbzawjsJwoKBJ+LjezPYHbQz7C2Ep/YD+gF3CXpIIQlO8EegOdgDOzGYepsuAZFlgaBXxgZrdV1XWcc9UvVwNGktoCJwH3hX0BxxKtpAkwFugXPvcN+4TjPUL+vsA4M1trZp8QrXF0aNgWmtkiM1tH1Jrtm+l3rsqW55HAQOBYSXPCdmIVXs85V02U4paCO4AriB5vBGgGrDCzDWG/iB8GntsAXwCE4ytD/k3pFc6pLD0jVTZgZGavk5/3kZ3LKyKt1TObS4od2BhpZiOJyugDLDWz2ZK6xxRfkSU5Vll6vMZixoM+PjGIcy476T3DWZJgwOhI4JTQQ20ANCZqiTaRVBhal22BxSF/EdAOKJJUCOwIlMakl4s9p7L0tPnrmc65rOWi225mV5tZWzNrTzTg86KZDQBeAk4L2QYBE8LniWGfcPzFsBzxRKC/pPphpL4jMJNoHfeOkjqEdeH7h7wZ8Zancy57VXuD7kpgnKSbgLeJBqIJPx+UtJCoxdkfwMzmSRoPvE/0puOFZlYGIOkiYDLRo0qjzSzjySU8eDrnspT7d9vN7GXg5fB5EdFIecU8a4DTKzl/GDAsTvokYFIu6ujB0zmXlfJZlfKNB0/nXPY8eDrnXPrycUo6D57Ouazl49w2Hjydc1nLw9jpwdM5l6U03r2sTTx4OueyEo2251/09ODpnMta/oVOD57OuVzIw+jpwdM5lzV/VMk55zKQh7c8PXg657KXh7HTg6dzLjtpToZca3jwdM5lJ73JkGsND57OuazlYez04Omcy4E8jJ6+DIdzLkuprtqeOMJKaifpJUkfSJon6ZKQvpOkKZIWhJ9NQ7okjZC0UNK7krrElDUo5F8gaVBMeldJc8M5I5TFzVoPns65rJRPhpzKlsQG4Pdmti/QDbhQUifgKmCqmXUEpoZ9gN5E6xN1BIYAd0MUbIGhwGFEM9APLQ+4Ic+QmPN6Zfq9PXg657KXgxXgzKzYzN4Kn1cDHxCtq94XGBuyjQX6hc99gQcsMp1olc3WwAnAFDMrNbPlwBSgVzjW2MymhYXiHogpK21+z9M5l7U03jCqdN32zcqT2gMHATOAlmZWDFGAldQiZGsDfBFzWlFIS5ReFCc9Ix48nXNZy9G67aEsNQIeBy41s1UJbkvGO2AZpGfEu+3OuazlYt12AEl1iQLnw2b2REj+KnS5CT+XhvQioF3M6W2BxUnS28ZJz4gHT+dcdsJD8qlsCYuJmpijgA/M7LaYQxOB8hHzQcCEmPSzw6h7N2Bl6N5PBnpKahoGinoCk8Ox1ZK6hWudHVNW2rzb7pzLSg5fzzwSGAjMlTQnpF0DDAfGSxoMfM4Pa7VPAk4EFgLfAecCmFmppBuBN0O+G8ysNHy+ABgDNASeC1tGPHg657KWi9BpZq8nKKpHnPwGXFhJWaOB0XHSZwGds6jmJh48nXNZ83fbnXMuAz4ZsnPOZSL/YqcHT+dc9vIwdnrwdM5lR/Klh51zLjP5Fzs9eDrnspeHsdODp3Mue3nYa/fg6ZzLVvKJjmsjD57OuaxEr2dWdy22Pg+ezrmsefB0zrkMeLfdOefS5eu2O+dc+lKd6Li28eDpnMteHkZPD57Ouaz565nOOZeB/AudHjydc7mQh9HTg6dzLmv5+KiSomVAtg2SlgGfVXc9qkBzoKS6K+HSUlt/Z7uZ2c65LFDS80R/XqkoMbNeubx+ddmmgmdtJWmWmR1c3fVwqfPfmUvG1213zrkMePB0zrkMePDcOkZWdwVc2vx35hLye57OOZcBb3k651wGPHg651wGPHg651wGPHhWIUkF1V0HlzpJe0o6WFL96q6L2/Z58KwCkvYCMLMyD6A1g6Q+wBPAX4Ex5b9D5yrjwTPHwv+EcyT9GzyA1gSSjgBuBQaZ2THAcuCq6q2V29Z58MwhSdsDFwGXAuskPQQeQGuI4Wb2dvg8FNjJu+8uEX/OM8ck7QKsAhoA9wBrzOys6q2VSyT8w7a9ma0Kn1sDTwM9zWyZpGZm9nX11tJta7zlmWNmttjMvjGzEuA8oGF5C1RSF0n7VG8NXUVmVmZmq8KugBVAaQicA4CbJDWsvhq6bZG3PKuYpOZEgxCHAwXAMWZWVL21cslIGgMUAz2Bc8xsbvXWyG1rfDLkKmZmJZLeBXoDx3vg3LZJElAXOCr87GFmC6q3Vm5b5MGziklqCpxIdP/MWy/bOIu6Yusk3Qi86YHTVca77VuBpAZmtqa66+FSJ0nm/3O4BDx4OudcBny03TnnMuDB0znnMuDB0znnMuDB0znnMuDBswaRVCZpjqT3JP1H0nZZlNVd0jPh8ymSKp0IQ1ITSb/J4Bp/knR5qukV8oyRdFoa12ov6b106+hcpjx41izfm9mBZtYZWAecH3tQkbR/p2Y20cyGJ8jSBEg7eDpXm3nwrLleA/YMLa4PJN0FvAW0k9RT0jRJb4UWaiMASb0kfSjpdeDU8oIknSPpn+FzS0lPSnonbEcAw4E9Qqv3ryHf/0l6U9K7kq6PKesPkj6S9F9g72RfQtKvQznvSHq8Qmv6OEmvSZofpvpDUoGkv8Zc+7xs/yCdy4QHzxpIUiHR657lbyztDTxgZgcB3wLXAseZWRdgFnCZpAbAvcDJRK8etqqk+BHAK2Z2ANAFmEc0t+XHodX7f5J6Ah2BQ4EDga6SjpbUFegPHEQUnA9J4es8YWaHhOt9AAyOOdYe+AlwEnBP+A6DgZVmdkgo/9eSOqRwHedyyl/PrFkaSpoTPr8GjAJ2AT4zs+khvRvQCXgjek2besA0YB/gk/LXDcNMT0PiXONY4GyIZhsCVoZXTGP1DFv5/JeNiILpDsCTZvZduMbEFL5TZ0k3Ed0aaARMjjk23sw2AgskLQrfoSewf8z90B3DteencC3ncsaDZ83yvZkdGJsQAuS3sUnAFDM7s0K+A4FcvU4m4C9m9q8K17g0g2uMAfqZ2TuSzgG6xxyrWJaFa19sZrFBFknt07yuc1nxbnvtMx04UtKeAJK2C+vxfAh0kLRHyHdmJedPBS4I5xZIagysJmpVlpsM/DLmXmobSS2AV4GfSmooaQeiWwTJ7AAUS6oLDKhw7HRJdUKddwc+Cte+IORH0l5hBn/ntipvedYyYQLfc4BHYpaRuNbM5ksaAjwrqQR4Hegcp4hLgJGSBgNlwAVmNk3SG+FRoOfCfc99gWmh5fsNcJaZvSXpUWAO8BnRrYVk/gjMCPnnsnmQ/gh4BWgJnG9mayTdR3Qv9K0wfdwyoF9qfzrO5Y5PDOKccxnwbrtzzmXAg6dzzmXAg6dzzmXAg6dzzmXAg6dzzmXAg6dzzmXAg6dzzmXg/wEjE2yjsW9VAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #8\n",
    "\n",
    "#### TestData1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"test_data_1\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData1.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "600/600 [==============================] - ETA:  - 0s 105us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/test_data_1_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\":10\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #9\n",
    "\n",
    "#### TestData2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"test_data_2\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData2.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique,count = np.unique(Y,return_counts=True)\n",
    "class1=count[0]/X.shape[0]*100\n",
    "class2=count[1]/X.shape[0]*100\n",
    "class_distribution = round(class1, 2)\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "480/480 [==============================] - ETA:  - 0s 98us/step\n",
      "Confusion matrix, without normalization\n",
      "Uploading ../data/test_data_2_logistic_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #10\n",
    "\n",
    "#### UCI Airfoil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"uci_airfoil\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_airfoil_self_noise.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "902/902 [==============================] - ETA:  - 0s 44us/step\n",
      "Uploading ../data/uci_airfoil_linear_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "....."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #11\n",
    "\n",
    "#### UCI Auto-mpg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"uci_auto_mpg\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_auto_mpg.csv\", delimiter=\",\", header=0, index_col='car name')\n",
    "data = data[data.horsepower != '?']\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:,1]\n",
    "X = data.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "236/236 [==============================] - ETA:  - 0s 101us/step\n",
      "Uploading ../data/uci_auto_mpg_linear_regression.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2f5bc21b2f8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((int(1e12),int(1e12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #12\n",
    "\n",
    "#### Testdata 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"test_data_3\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData3.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "600/600 [==============================] - ETA:  - 0s 100us/step\n",
      "Uploading ../data/test_data_3_linear_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "...."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #13\n",
    "\n",
    "#### Testdata 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset_name = \"test_data_4\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/testData4.csv\", delimiter=\",\", header=0, index_col=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:, -1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:27<00:00, 27.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n",
      "600/600 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 843us/step\n",
      "Uploading ../data/test_data_4_linear_regression.pdf to Amazon S3 bucket mlsquare-pdf\n",
      "...."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\":100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_regression', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #14\n",
    "\n",
    "#### Iris LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "Epoch 1/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -29.1046 - acc: 0.9750\n",
      "Epoch 2/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -29.3852 - acc: 0.9750\n",
      "Epoch 3/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -29.6683 - acc: 0.9750\n",
      "Epoch 4/500\n",
      "40/40 [==============================] - 0s 376us/step - loss: -29.9539 - acc: 0.9750\n",
      "Epoch 5/500\n",
      "40/40 [==============================] - 0s 400us/step - loss: -30.2420 - acc: 0.9750\n",
      "Epoch 6/500\n",
      "40/40 [==============================] - 0s 549us/step - loss: -30.5325 - acc: 0.9750\n",
      "Epoch 7/500\n",
      "40/40 [==============================] - 0s 398us/step - loss: -30.8257 - acc: 1.0000\n",
      "Epoch 8/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -31.1213 - acc: 1.0000\n",
      "Epoch 9/500\n",
      "40/40 [==============================] - 0s 449us/step - loss: -31.4195 - acc: 1.0000\n",
      "Epoch 10/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -31.7202 - acc: 1.0000\n",
      "Epoch 11/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -32.0236 - acc: 1.0000\n",
      "Epoch 12/500\n",
      "40/40 [==============================] - 0s 474us/step - loss: -32.3296 - acc: 1.0000\n",
      "Epoch 13/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -32.6382 - acc: 1.0000\n",
      "Epoch 14/500\n",
      "40/40 [==============================] - 0s 474us/step - loss: -32.9497 - acc: 1.0000\n",
      "Epoch 15/500\n",
      "40/40 [==============================] - 0s 573us/step - loss: -33.2640 - acc: 1.0000\n",
      "Epoch 16/500\n",
      "40/40 [==============================] - 0s 474us/step - loss: -33.5812 - acc: 1.0000\n",
      "Epoch 17/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -33.9014 - acc: 1.0000\n",
      "Epoch 18/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -34.2246 - acc: 1.0000\n",
      "Epoch 19/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -34.5508 - acc: 1.0000\n",
      "Epoch 20/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -34.8801 - acc: 1.0000\n",
      "Epoch 21/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -35.2124 - acc: 1.0000\n",
      "Epoch 22/500\n",
      "40/40 [==============================] - 0s 449us/step - loss: -35.5478 - acc: 1.0000\n",
      "Epoch 23/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -35.8861 - acc: 1.0000\n",
      "Epoch 24/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -36.2275 - acc: 1.0000\n",
      "Epoch 25/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -36.5717 - acc: 1.0000\n",
      "Epoch 26/500\n",
      "40/40 [==============================] - 0s 548us/step - loss: -36.9189 - acc: 1.0000\n",
      "Epoch 27/500\n",
      "40/40 [==============================] - 0s 449us/step - loss: -37.2688 - acc: 1.0000\n",
      "Epoch 28/500\n",
      "40/40 [==============================] - 0s 275us/step - loss: -37.6216 - acc: 1.0000\n",
      "Epoch 29/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -37.9770 - acc: 1.0000\n",
      "Epoch 30/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -38.3350 - acc: 1.0000\n",
      "Epoch 31/500\n",
      "40/40 [==============================] - 0s 499us/step - loss: -38.6956 - acc: 1.0000\n",
      "Epoch 32/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -39.0585 - acc: 1.0000\n",
      "Epoch 33/500\n",
      "40/40 [==============================] - 0s 449us/step - loss: -39.4238 - acc: 1.0000\n",
      "Epoch 34/500\n",
      "40/40 [==============================] - 0s 474us/step - loss: -39.7913 - acc: 1.0000\n",
      "Epoch 35/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -40.1608 - acc: 1.0000\n",
      "Epoch 36/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -40.5323 - acc: 1.0000\n",
      "Epoch 37/500\n",
      "40/40 [==============================] - 0s 474us/step - loss: -40.9055 - acc: 1.0000\n",
      "Epoch 38/500\n",
      "40/40 [==============================] - 0s 424us/step - loss: -41.2804 - acc: 1.0000\n",
      "Epoch 39/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -41.6568 - acc: 1.0000\n",
      "Epoch 40/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -42.0344 - acc: 1.0000\n",
      "Epoch 41/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -42.4132 - acc: 1.0000\n",
      "Epoch 42/500\n",
      "40/40 [==============================] - 0s 424us/step - loss: -42.7929 - acc: 1.0000\n",
      "Epoch 43/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -43.1733 - acc: 1.0000\n",
      "Epoch 44/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -43.5543 - acc: 1.0000\n",
      "Epoch 45/500\n",
      "40/40 [==============================] - 0s 424us/step - loss: -43.9356 - acc: 1.0000\n",
      "Epoch 46/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -44.3169 - acc: 1.0000\n",
      "Epoch 47/500\n",
      "40/40 [==============================] - 0s 423us/step - loss: -44.6982 - acc: 1.0000\n",
      "Epoch 48/500\n",
      "40/40 [==============================] - 0s 474us/step - loss: -45.0790 - acc: 1.0000\n",
      "Epoch 49/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -45.4593 - acc: 1.0000\n",
      "Epoch 50/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -45.8387 - acc: 1.0000\n",
      "Epoch 51/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -46.2169 - acc: 1.0000\n",
      "Epoch 52/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -46.5939 - acc: 1.0000\n",
      "Epoch 53/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -46.9691 - acc: 1.0000\n",
      "Epoch 54/500\n",
      "40/40 [==============================] - 0s 424us/step - loss: -47.3426 - acc: 1.0000\n",
      "Epoch 55/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -47.7139 - acc: 1.0000\n",
      "Epoch 56/500\n",
      "40/40 [==============================] - 0s 424us/step - loss: -48.0828 - acc: 1.0000\n",
      "Epoch 57/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -48.4491 - acc: 1.0000\n",
      "Epoch 58/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -48.8125 - acc: 1.0000\n",
      "Epoch 59/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -49.1728 - acc: 1.0000\n",
      "Epoch 60/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -49.5298 - acc: 1.0000\n",
      "Epoch 61/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -49.8832 - acc: 1.0000\n",
      "Epoch 62/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -50.2328 - acc: 1.0000\n",
      "Epoch 63/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -50.5784 - acc: 1.0000\n",
      "Epoch 64/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -50.9197 - acc: 1.0000\n",
      "Epoch 65/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -51.2567 - acc: 1.0000\n",
      "Epoch 66/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -51.5892 - acc: 1.0000\n",
      "Epoch 67/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -51.9169 - acc: 1.0000\n",
      "Epoch 68/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -52.2399 - acc: 1.0000\n",
      "Epoch 69/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -52.5578 - acc: 1.0000\n",
      "Epoch 70/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -52.8707 - acc: 1.0000\n",
      "Epoch 71/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -53.1785 - acc: 1.0000\n",
      "Epoch 72/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -53.4810 - acc: 1.0000\n",
      "Epoch 73/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -53.7784 - acc: 1.0000\n",
      "Epoch 74/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -54.0705 - acc: 1.0000\n",
      "Epoch 75/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -54.3574 - acc: 1.0000\n",
      "Epoch 76/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -54.6391 - acc: 1.0000\n",
      "Epoch 77/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -54.9156 - acc: 1.0000\n",
      "Epoch 78/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -55.1870 - acc: 1.0000\n",
      "Epoch 79/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -55.4534 - acc: 1.0000\n",
      "Epoch 80/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -55.7149 - acc: 1.0000\n",
      "Epoch 81/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -55.9716 - acc: 1.0000\n",
      "Epoch 82/500\n",
      "40/40 [==============================] - 0s 424us/step - loss: -56.2238 - acc: 1.0000\n",
      "Epoch 83/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -56.4713 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -56.7146 - acc: 1.0000\n",
      "Epoch 85/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -56.9538 - acc: 1.0000\n",
      "Epoch 86/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -57.1890 - acc: 1.0000\n",
      "Epoch 87/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -57.4205 - acc: 1.0000\n",
      "Epoch 88/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -57.6484 - acc: 1.0000\n",
      "Epoch 89/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -57.8730 - acc: 1.0000\n",
      "Epoch 90/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -58.0945 - acc: 1.0000\n",
      "Epoch 91/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -58.3131 - acc: 1.0000\n",
      "Epoch 92/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -58.5290 - acc: 1.0000\n",
      "Epoch 93/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -58.7424 - acc: 1.0000\n",
      "Epoch 94/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -58.9536 - acc: 1.0000\n",
      "Epoch 95/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -59.1627 - acc: 1.0000\n",
      "Epoch 96/500\n",
      "40/40 [==============================] - 0s 213us/step - loss: -59.3699 - acc: 1.0000\n",
      "Epoch 97/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -59.5755 - acc: 1.0000\n",
      "Epoch 98/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -59.7796 - acc: 1.0000\n",
      "Epoch 99/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -59.9823 - acc: 1.0000\n",
      "Epoch 100/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -60.1840 - acc: 1.0000\n",
      "Epoch 101/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -60.3847 - acc: 1.0000\n",
      "Epoch 102/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -60.5845 - acc: 1.0000\n",
      "Epoch 103/500\n",
      "40/40 [==============================] - 0s 238us/step - loss: -60.7837 - acc: 1.0000\n",
      "Epoch 104/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -60.9824 - acc: 1.0000\n",
      "Epoch 105/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -61.1805 - acc: 1.0000\n",
      "Epoch 106/500\n",
      "40/40 [==============================] - 0s 275us/step - loss: -61.3784 - acc: 1.0000\n",
      "Epoch 107/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -61.5761 - acc: 1.0000\n",
      "Epoch 108/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -61.7736 - acc: 1.0000\n",
      "Epoch 109/500\n",
      "40/40 [==============================] - 0s 300us/step - loss: -61.9711 - acc: 1.0000\n",
      "Epoch 110/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -62.1686 - acc: 1.0000\n",
      "Epoch 111/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -62.3663 - acc: 1.0000\n",
      "Epoch 112/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -62.5641 - acc: 1.0000\n",
      "Epoch 113/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -62.7622 - acc: 1.0000\n",
      "Epoch 114/500\n",
      "40/40 [==============================] - 0s 300us/step - loss: -62.9605 - acc: 1.0000\n",
      "Epoch 115/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -63.1592 - acc: 1.0000\n",
      "Epoch 116/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -63.3583 - acc: 1.0000\n",
      "Epoch 117/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -63.5578 - acc: 1.0000\n",
      "Epoch 118/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -63.7577 - acc: 1.0000\n",
      "Epoch 119/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -63.9581 - acc: 1.0000\n",
      "Epoch 120/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -64.1591 - acc: 1.0000\n",
      "Epoch 121/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -64.3605 - acc: 1.0000\n",
      "Epoch 122/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -64.5625 - acc: 1.0000\n",
      "Epoch 123/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -64.7651 - acc: 1.0000\n",
      "Epoch 124/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -64.9683 - acc: 1.0000\n",
      "Epoch 125/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -65.1720 - acc: 1.0000\n",
      "Epoch 126/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -65.3764 - acc: 1.0000\n",
      "Epoch 127/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -65.5814 - acc: 1.0000\n",
      "Epoch 128/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -65.7870 - acc: 1.0000\n",
      "Epoch 129/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -65.9934 - acc: 1.0000\n",
      "Epoch 130/500\n",
      "40/40 [==============================] - 0s 275us/step - loss: -66.2003 - acc: 1.0000\n",
      "Epoch 131/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -66.4079 - acc: 1.0000\n",
      "Epoch 132/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -66.6163 - acc: 1.0000\n",
      "Epoch 133/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -66.8253 - acc: 1.0000\n",
      "Epoch 134/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -67.0350 - acc: 1.0000\n",
      "Epoch 135/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -67.2454 - acc: 1.0000\n",
      "Epoch 136/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -67.4566 - acc: 1.0000\n",
      "Epoch 137/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -67.6684 - acc: 1.0000\n",
      "Epoch 138/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -67.8810 - acc: 1.0000\n",
      "Epoch 139/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -68.0944 - acc: 1.0000\n",
      "Epoch 140/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -68.3084 - acc: 1.0000\n",
      "Epoch 141/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -68.5232 - acc: 1.0000\n",
      "Epoch 142/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -68.7389 - acc: 1.0000\n",
      "Epoch 143/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -68.9552 - acc: 1.0000\n",
      "Epoch 144/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -69.1724 - acc: 1.0000\n",
      "Epoch 145/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -69.3903 - acc: 1.0000\n",
      "Epoch 146/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -69.6090 - acc: 1.0000\n",
      "Epoch 147/500\n",
      "40/40 [==============================] - 0s 325us/step - loss: -69.8286 - acc: 1.0000\n",
      "Epoch 148/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -70.0489 - acc: 1.0000\n",
      "Epoch 149/500\n",
      "40/40 [==============================] - 0s 300us/step - loss: -70.2700 - acc: 1.0000\n",
      "Epoch 150/500\n",
      "40/40 [==============================] - 0s 250us/step - loss: -70.4920 - acc: 1.0000\n",
      "Epoch 151/500\n",
      "40/40 [==============================] - 0s 300us/step - loss: -70.7147 - acc: 1.0000\n",
      "Epoch 152/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -70.9384 - acc: 1.0000\n",
      "Epoch 153/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -71.1628 - acc: 1.0000\n",
      "Epoch 154/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -71.3881 - acc: 1.0000\n",
      "Epoch 155/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -71.6142 - acc: 1.0000\n",
      "Epoch 156/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -71.8412 - acc: 1.0000\n",
      "Epoch 157/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -72.0691 - acc: 1.0000\n",
      "Epoch 158/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -72.2978 - acc: 1.0000\n",
      "Epoch 159/500\n",
      "40/40 [==============================] - 0s 300us/step - loss: -72.5273 - acc: 1.0000\n",
      "Epoch 160/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -72.7578 - acc: 1.0000\n",
      "Epoch 161/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -72.9891 - acc: 1.0000\n",
      "Epoch 162/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -73.2214 - acc: 1.0000\n",
      "Epoch 163/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -73.4545 - acc: 1.0000\n",
      "Epoch 164/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -73.6885 - acc: 1.0000\n",
      "Epoch 165/500\n",
      "40/40 [==============================] - 0s 187us/step - loss: -73.9234 - acc: 1.0000\n",
      "Epoch 166/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -74.1593 - acc: 1.0000\n",
      "Epoch 167/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -74.3960 - acc: 1.0000\n",
      "Epoch 168/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -74.6337 - acc: 1.0000\n",
      "Epoch 169/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -74.8723 - acc: 1.0000\n",
      "Epoch 170/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -75.1117 - acc: 1.0000\n",
      "Epoch 171/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -75.3522 - acc: 1.0000\n",
      "Epoch 172/500\n",
      "40/40 [==============================] - 0s 300us/step - loss: -75.5936 - acc: 1.0000\n",
      "Epoch 173/500\n",
      "40/40 [==============================] - 0s 287us/step - loss: -75.8359 - acc: 1.0000\n",
      "Epoch 174/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -76.0792 - acc: 1.0000\n",
      "Epoch 175/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -76.3234 - acc: 1.0000\n",
      "Epoch 176/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -76.5686 - acc: 1.0000\n",
      "Epoch 177/500\n",
      "40/40 [==============================] - 0s 250us/step - loss: -76.8147 - acc: 1.0000\n",
      "Epoch 178/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -77.0618 - acc: 1.0000\n",
      "Epoch 179/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -77.3099 - acc: 1.0000\n",
      "Epoch 180/500\n",
      "40/40 [==============================] - 0s 201us/step - loss: -77.5589 - acc: 1.0000\n",
      "Epoch 181/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -77.8090 - acc: 1.0000\n",
      "Epoch 182/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -78.0600 - acc: 1.0000\n",
      "Epoch 183/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -78.3120 - acc: 1.0000\n",
      "Epoch 184/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -78.5650 - acc: 1.0000\n",
      "Epoch 185/500\n",
      "40/40 [==============================] - 0s 236us/step - loss: -78.8190 - acc: 1.0000\n",
      "Epoch 186/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -79.0740 - acc: 1.0000\n",
      "Epoch 187/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -79.3300 - acc: 1.0000\n",
      "Epoch 188/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -79.5871 - acc: 1.0000\n",
      "Epoch 189/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -79.8451 - acc: 1.0000\n",
      "Epoch 190/500\n",
      "40/40 [==============================] - 0s 223us/step - loss: -80.1042 - acc: 1.0000\n",
      "Epoch 191/500\n",
      "40/40 [==============================] - 0s 300us/step - loss: -80.3643 - acc: 1.0000\n",
      "Epoch 192/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -80.6255 - acc: 1.0000\n",
      "Epoch 193/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -80.8877 - acc: 1.0000\n",
      "Epoch 194/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -81.1509 - acc: 1.0000\n",
      "Epoch 195/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -81.4152 - acc: 1.0000\n",
      "Epoch 196/500\n",
      "40/40 [==============================] - 0s 474us/step - loss: -81.6806 - acc: 1.0000\n",
      "Epoch 197/500\n",
      "40/40 [==============================] - 0s 673us/step - loss: -81.9470 - acc: 1.0000\n",
      "Epoch 198/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -82.2145 - acc: 1.0000\n",
      "Epoch 199/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -82.4830 - acc: 1.0000\n",
      "Epoch 200/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -82.7527 - acc: 1.0000\n",
      "Epoch 201/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -83.0234 - acc: 1.0000\n",
      "Epoch 202/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -83.2952 - acc: 1.0000\n",
      "Epoch 203/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -83.5681 - acc: 1.0000\n",
      "Epoch 204/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -83.8421 - acc: 1.0000\n",
      "Epoch 205/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -84.1173 - acc: 1.0000\n",
      "Epoch 206/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -84.3935 - acc: 1.0000\n",
      "Epoch 207/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -84.6709 - acc: 1.0000\n",
      "Epoch 208/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -84.9493 - acc: 1.0000\n",
      "Epoch 209/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -85.2289 - acc: 1.0000\n",
      "Epoch 210/500\n",
      "40/40 [==============================] - 0s 524us/step - loss: -85.5097 - acc: 1.0000\n",
      "Epoch 211/500\n",
      "40/40 [==============================] - 0s 312us/step - loss: -85.7916 - acc: 1.0000\n",
      "Epoch 212/500\n",
      "40/40 [==============================] - 0s 549us/step - loss: -86.0745 - acc: 1.0000\n",
      "Epoch 213/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -86.3588 - acc: 1.0000\n",
      "Epoch 214/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -86.6441 - acc: 1.0000\n",
      "Epoch 215/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -86.9306 - acc: 1.0000\n",
      "Epoch 216/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -87.2183 - acc: 1.0000\n",
      "Epoch 217/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -87.5072 - acc: 1.0000\n",
      "Epoch 218/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -87.7972 - acc: 1.0000\n",
      "Epoch 219/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -88.0884 - acc: 1.0000\n",
      "Epoch 220/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -88.3808 - acc: 1.0000\n",
      "Epoch 221/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -88.6745 - acc: 1.0000\n",
      "Epoch 222/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -88.9693 - acc: 1.0000\n",
      "Epoch 223/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -89.2654 - acc: 1.0000\n",
      "Epoch 224/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -89.5627 - acc: 1.0000\n",
      "Epoch 225/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -89.8612 - acc: 1.0000\n",
      "Epoch 226/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -90.1610 - acc: 1.0000\n",
      "Epoch 227/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -90.4620 - acc: 1.0000\n",
      "Epoch 228/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -90.7642 - acc: 1.0000\n",
      "Epoch 229/500\n",
      "40/40 [==============================] - 0s 250us/step - loss: -91.0677 - acc: 1.0000\n",
      "Epoch 230/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -91.3724 - acc: 1.0000\n",
      "Epoch 231/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -91.6785 - acc: 1.0000\n",
      "Epoch 232/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -91.9857 - acc: 1.0000\n",
      "Epoch 233/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -92.2943 - acc: 1.0000\n",
      "Epoch 234/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -92.6042 - acc: 1.0000\n",
      "Epoch 235/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -92.9153 - acc: 1.0000\n",
      "Epoch 236/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -93.2278 - acc: 1.0000\n",
      "Epoch 237/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -93.5416 - acc: 1.0000\n",
      "Epoch 238/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -93.8567 - acc: 1.0000\n",
      "Epoch 239/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -94.1731 - acc: 1.0000\n",
      "Epoch 240/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -94.4908 - acc: 1.0000\n",
      "Epoch 241/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -94.8099 - acc: 1.0000\n",
      "Epoch 242/500\n",
      "40/40 [==============================] - 0s 523us/step - loss: -95.1303 - acc: 1.0000\n",
      "Epoch 243/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -95.4521 - acc: 1.0000\n",
      "Epoch 244/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -95.7751 - acc: 1.0000\n",
      "Epoch 245/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -96.0996 - acc: 1.0000\n",
      "Epoch 246/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -96.4255 - acc: 1.0000\n",
      "Epoch 247/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -96.7528 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -97.0813 - acc: 1.0000\n",
      "Epoch 249/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -97.4114 - acc: 1.0000\n",
      "Epoch 250/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -97.7428 - acc: 1.0000\n",
      "Epoch 251/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -98.0756 - acc: 1.0000\n",
      "Epoch 252/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -98.4098 - acc: 1.0000\n",
      "Epoch 253/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -98.7455 - acc: 1.0000\n",
      "Epoch 254/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -99.0826 - acc: 1.0000\n",
      "Epoch 255/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -99.4211 - acc: 1.0000\n",
      "Epoch 256/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -99.7611 - acc: 1.0000\n",
      "Epoch 257/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -100.1025 - acc: 1.0000\n",
      "Epoch 258/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -100.4454 - acc: 1.0000\n",
      "Epoch 259/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -100.7897 - acc: 1.0000\n",
      "Epoch 260/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -101.1356 - acc: 1.0000\n",
      "Epoch 261/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -101.4829 - acc: 1.0000\n",
      "Epoch 262/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -101.8317 - acc: 1.0000\n",
      "Epoch 263/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -102.1821 - acc: 1.0000\n",
      "Epoch 264/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -102.5339 - acc: 1.0000\n",
      "Epoch 265/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -102.8872 - acc: 1.0000\n",
      "Epoch 266/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -103.2421 - acc: 1.0000\n",
      "Epoch 267/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -103.5985 - acc: 1.0000\n",
      "Epoch 268/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -103.9564 - acc: 1.0000\n",
      "Epoch 269/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -104.3159 - acc: 1.0000\n",
      "Epoch 270/500\n",
      "40/40 [==============================] - 0s 399us/step - loss: -104.6769 - acc: 1.0000\n",
      "Epoch 271/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -105.0395 - acc: 1.0000\n",
      "Epoch 272/500\n",
      "40/40 [==============================] - 0s 424us/step - loss: -105.4037 - acc: 1.0000\n",
      "Epoch 273/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -105.7695 - acc: 1.0000\n",
      "Epoch 274/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -106.1368 - acc: 1.0000\n",
      "Epoch 275/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -106.5058 - acc: 1.0000\n",
      "Epoch 276/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -106.8764 - acc: 1.0000\n",
      "Epoch 277/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -107.2486 - acc: 1.0000\n",
      "Epoch 278/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -107.6224 - acc: 1.0000\n",
      "Epoch 279/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -107.9978 - acc: 1.0000\n",
      "Epoch 280/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -108.3749 - acc: 1.0000\n",
      "Epoch 281/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -108.7536 - acc: 1.0000\n",
      "Epoch 282/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -109.1340 - acc: 1.0000\n",
      "Epoch 283/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -109.5161 - acc: 1.0000\n",
      "Epoch 284/500\n",
      "40/40 [==============================] - 0s 237us/step - loss: -109.8998 - acc: 1.0000\n",
      "Epoch 285/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -110.2853 - acc: 1.0000\n",
      "Epoch 286/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -110.6724 - acc: 1.0000\n",
      "Epoch 287/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -111.0612 - acc: 1.0000\n",
      "Epoch 288/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -111.4518 - acc: 1.0000\n",
      "Epoch 289/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -111.8440 - acc: 1.0000\n",
      "Epoch 290/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -112.2381 - acc: 1.0000\n",
      "Epoch 291/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -112.6338 - acc: 1.0000\n",
      "Epoch 292/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -113.0313 - acc: 1.0000\n",
      "Epoch 293/500\n",
      "40/40 [==============================] - 0s 312us/step - loss: -113.4305 - acc: 1.0000\n",
      "Epoch 294/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -113.8315 - acc: 1.0000\n",
      "Epoch 295/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -114.2344 - acc: 1.0000\n",
      "Epoch 296/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -114.6390 - acc: 1.0000\n",
      "Epoch 297/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -115.0454 - acc: 1.0000\n",
      "Epoch 298/500\n",
      "40/40 [==============================] - 0s 403us/step - loss: -115.4536 - acc: 1.0000\n",
      "Epoch 299/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -115.8636 - acc: 1.0000\n",
      "Epoch 300/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -116.2755 - acc: 1.0000\n",
      "Epoch 301/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -116.6892 - acc: 1.0000\n",
      "Epoch 302/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -117.1048 - acc: 1.0000\n",
      "Epoch 303/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -117.5222 - acc: 1.0000\n",
      "Epoch 304/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -117.9415 - acc: 1.0000\n",
      "Epoch 305/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -118.3626 - acc: 1.0000\n",
      "Epoch 306/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -118.7856 - acc: 1.0000\n",
      "Epoch 307/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -119.2106 - acc: 1.0000\n",
      "Epoch 308/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -119.6375 - acc: 1.0000\n",
      "Epoch 309/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -120.0663 - acc: 1.0000\n",
      "Epoch 310/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -120.4969 - acc: 1.0000\n",
      "Epoch 311/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -120.9296 - acc: 1.0000\n",
      "Epoch 312/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -121.3641 - acc: 1.0000\n",
      "Epoch 313/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -121.8008 - acc: 1.0000\n",
      "Epoch 314/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -122.2393 - acc: 1.0000\n",
      "Epoch 315/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -122.6797 - acc: 1.0000\n",
      "Epoch 316/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -123.1223 - acc: 1.0000\n",
      "Epoch 317/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -123.5668 - acc: 1.0000\n",
      "Epoch 318/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -124.0134 - acc: 1.0000\n",
      "Epoch 319/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -124.4619 - acc: 1.0000\n",
      "Epoch 320/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -124.9125 - acc: 1.0000\n",
      "Epoch 321/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -125.3652 - acc: 1.0000\n",
      "Epoch 322/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -125.8198 - acc: 1.0000\n",
      "Epoch 323/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -126.2766 - acc: 1.0000\n",
      "Epoch 324/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -126.7355 - acc: 1.0000\n",
      "Epoch 325/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -127.1964 - acc: 1.0000\n",
      "Epoch 326/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -127.6594 - acc: 1.0000\n",
      "Epoch 327/500\n",
      "40/40 [==============================] - 0s 250us/step - loss: -128.1246 - acc: 1.0000\n",
      "Epoch 328/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -128.5919 - acc: 1.0000\n",
      "Epoch 329/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -129.0613 - acc: 1.0000\n",
      "Epoch 330/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -129.5328 - acc: 1.0000\n",
      "Epoch 331/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -130.0065 - acc: 1.0000\n",
      "Epoch 332/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -130.4825 - acc: 1.0000\n",
      "Epoch 333/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -130.9606 - acc: 1.0000\n",
      "Epoch 334/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -131.4409 - acc: 1.0000\n",
      "Epoch 335/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -131.9234 - acc: 1.0000\n",
      "Epoch 336/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -132.4081 - acc: 1.0000\n",
      "Epoch 337/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -132.8950 - acc: 1.0000\n",
      "Epoch 338/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -133.3843 - acc: 1.0000\n",
      "Epoch 339/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -133.8757 - acc: 1.0000\n",
      "Epoch 340/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -134.3694 - acc: 1.0000\n",
      "Epoch 341/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -134.8653 - acc: 1.0000\n",
      "Epoch 342/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -135.3637 - acc: 1.0000\n",
      "Epoch 343/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -135.8642 - acc: 1.0000\n",
      "Epoch 344/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -136.3672 - acc: 1.0000\n",
      "Epoch 345/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -136.8724 - acc: 1.0000\n",
      "Epoch 346/500\n",
      "40/40 [==============================] - 0s 275us/step - loss: -137.3800 - acc: 1.0000\n",
      "Epoch 347/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -137.8900 - acc: 1.0000\n",
      "Epoch 348/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -138.4023 - acc: 1.0000\n",
      "Epoch 349/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -138.9170 - acc: 1.0000\n",
      "Epoch 350/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -139.4340 - acc: 1.0000\n",
      "Epoch 351/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -139.9535 - acc: 1.0000\n",
      "Epoch 352/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -140.4754 - acc: 1.0000\n",
      "Epoch 353/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -140.9997 - acc: 1.0000\n",
      "Epoch 354/500\n",
      "40/40 [==============================] - 0s 275us/step - loss: -141.5265 - acc: 1.0000\n",
      "Epoch 355/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -142.0558 - acc: 1.0000\n",
      "Epoch 356/500\n",
      "40/40 [==============================] - 0s 349us/step - loss: -142.5874 - acc: 1.0000\n",
      "Epoch 357/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -143.1217 - acc: 1.0000\n",
      "Epoch 358/500\n",
      "40/40 [==============================] - 0s 324us/step - loss: -143.6584 - acc: 1.0000\n",
      "Epoch 359/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -144.1975 - acc: 1.0000\n",
      "Epoch 360/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -144.7393 - acc: 1.0000\n",
      "Epoch 361/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -145.2835 - acc: 1.0000\n",
      "Epoch 362/500\n",
      "40/40 [==============================] - 0s 374us/step - loss: -145.8304 - acc: 1.0000\n",
      "Epoch 363/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -146.3797 - acc: 1.0000\n",
      "Epoch 364/500\n",
      "40/40 [==============================] - 0s 250us/step - loss: -146.9317 - acc: 1.0000\n",
      "Epoch 365/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -147.4863 - acc: 1.0000\n",
      "Epoch 366/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -148.0434 - acc: 1.0000\n",
      "Epoch 367/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -148.6032 - acc: 1.0000\n",
      "Epoch 368/500\n",
      "40/40 [==============================] - 0s 299us/step - loss: -149.1657 - acc: 1.0000\n",
      "Epoch 369/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -149.7307 - acc: 1.0000\n",
      "Epoch 370/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -150.2984 - acc: 1.0000\n",
      "Epoch 371/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -150.8688 - acc: 1.0000\n",
      "Epoch 372/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -151.4420 - acc: 1.0000\n",
      "Epoch 373/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -152.0177 - acc: 1.0000\n",
      "Epoch 374/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -152.5962 - acc: 1.0000\n",
      "Epoch 375/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -153.1775 - acc: 1.0000\n",
      "Epoch 376/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -153.7615 - acc: 1.0000\n",
      "Epoch 377/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -154.3483 - acc: 1.0000\n",
      "Epoch 378/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -154.9379 - acc: 1.0000\n",
      "Epoch 379/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -155.5301 - acc: 1.0000\n",
      "Epoch 380/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -156.1253 - acc: 1.0000\n",
      "Epoch 381/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -156.7233 - acc: 1.0000\n",
      "Epoch 382/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -157.3240 - acc: 1.0000\n",
      "Epoch 383/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -157.9277 - acc: 1.0000\n",
      "Epoch 384/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -158.5342 - acc: 1.0000\n",
      "Epoch 385/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -159.1436 - acc: 1.0000\n",
      "Epoch 386/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -159.7558 - acc: 1.0000\n",
      "Epoch 387/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -160.3712 - acc: 1.0000\n",
      "Epoch 388/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -160.9893 - acc: 1.0000\n",
      "Epoch 389/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -161.6104 - acc: 1.0000\n",
      "Epoch 390/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -162.2344 - acc: 1.0000\n",
      "Epoch 391/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -162.8614 - acc: 1.0000\n",
      "Epoch 392/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -163.4914 - acc: 1.0000\n",
      "Epoch 393/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -164.1244 - acc: 1.0000\n",
      "Epoch 394/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -164.7606 - acc: 1.0000\n",
      "Epoch 395/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -165.3996 - acc: 1.0000\n",
      "Epoch 396/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -166.0417 - acc: 1.0000\n",
      "Epoch 397/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -166.6870 - acc: 1.0000\n",
      "Epoch 398/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -167.3352 - acc: 1.0000\n",
      "Epoch 399/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -167.9866 - acc: 1.0000\n",
      "Epoch 400/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -168.6412 - acc: 1.0000\n",
      "Epoch 401/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -169.2988 - acc: 1.0000\n",
      "Epoch 402/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -169.9596 - acc: 1.0000\n",
      "Epoch 403/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -170.6235 - acc: 1.0000\n",
      "Epoch 404/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -171.2906 - acc: 1.0000\n",
      "Epoch 405/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -171.9610 - acc: 1.0000\n",
      "Epoch 406/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -172.6346 - acc: 1.0000\n",
      "Epoch 407/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -173.3113 - acc: 1.0000\n",
      "Epoch 408/500\n",
      "40/40 [==============================] - 0s 162us/step - loss: -173.9914 - acc: 1.0000\n",
      "Epoch 409/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -174.6748 - acc: 1.0000\n",
      "Epoch 410/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 175us/step - loss: -175.3613 - acc: 1.0000\n",
      "Epoch 411/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -176.0513 - acc: 1.0000\n",
      "Epoch 412/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -176.7445 - acc: 1.0000\n",
      "Epoch 413/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -177.4410 - acc: 1.0000\n",
      "Epoch 414/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -178.1409 - acc: 1.0000\n",
      "Epoch 415/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -178.8442 - acc: 1.0000\n",
      "Epoch 416/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -179.5508 - acc: 1.0000\n",
      "Epoch 417/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -180.2608 - acc: 1.0000\n",
      "Epoch 418/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -180.9743 - acc: 1.0000\n",
      "Epoch 419/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -181.6912 - acc: 1.0000\n",
      "Epoch 420/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -182.4115 - acc: 1.0000\n",
      "Epoch 421/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -183.1354 - acc: 1.0000\n",
      "Epoch 422/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -183.8628 - acc: 1.0000\n",
      "Epoch 423/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -184.5936 - acc: 1.0000\n",
      "Epoch 424/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -185.3280 - acc: 1.0000\n",
      "Epoch 425/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -186.0659 - acc: 1.0000\n",
      "Epoch 426/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -186.8073 - acc: 1.0000\n",
      "Epoch 427/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -187.5523 - acc: 1.0000\n",
      "Epoch 428/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -188.3012 - acc: 1.0000\n",
      "Epoch 429/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -189.0534 - acc: 1.0000\n",
      "Epoch 430/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -189.8093 - acc: 1.0000\n",
      "Epoch 431/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -190.5689 - acc: 1.0000\n",
      "Epoch 432/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -191.3321 - acc: 1.0000\n",
      "Epoch 433/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -192.0990 - acc: 1.0000\n",
      "Epoch 434/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -192.8697 - acc: 1.0000\n",
      "Epoch 435/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -193.6440 - acc: 1.0000\n",
      "Epoch 436/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -194.4222 - acc: 1.0000\n",
      "Epoch 437/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -195.2041 - acc: 1.0000\n",
      "Epoch 438/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -195.9898 - acc: 1.0000\n",
      "Epoch 439/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -196.7793 - acc: 1.0000\n",
      "Epoch 440/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -197.5726 - acc: 1.0000\n",
      "Epoch 441/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -198.3697 - acc: 1.0000\n",
      "Epoch 442/500\n",
      "40/40 [==============================] - 0s 250us/step - loss: -199.1708 - acc: 1.0000\n",
      "Epoch 443/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -199.9756 - acc: 1.0000\n",
      "Epoch 444/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -200.7843 - acc: 1.0000\n",
      "Epoch 445/500\n",
      "40/40 [==============================] - 0s 192us/step - loss: -201.5970 - acc: 1.0000\n",
      "Epoch 446/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -202.4137 - acc: 1.0000\n",
      "Epoch 447/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -203.2342 - acc: 1.0000\n",
      "Epoch 448/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -204.0589 - acc: 1.0000\n",
      "Epoch 449/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -204.8874 - acc: 1.0000\n",
      "Epoch 450/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -205.7201 - acc: 1.0000\n",
      "Epoch 451/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -206.5566 - acc: 1.0000\n",
      "Epoch 452/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -207.3972 - acc: 1.0000\n",
      "Epoch 453/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -208.2419 - acc: 1.0000\n",
      "Epoch 454/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -209.0907 - acc: 1.0000\n",
      "Epoch 455/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -209.9437 - acc: 1.0000\n",
      "Epoch 456/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -210.8008 - acc: 1.0000\n",
      "Epoch 457/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -211.6620 - acc: 1.0000\n",
      "Epoch 458/500\n",
      "40/40 [==============================] - 0s 200us/step - loss: -212.5274 - acc: 1.0000\n",
      "Epoch 459/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -213.3969 - acc: 1.0000\n",
      "Epoch 460/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -214.2708 - acc: 1.0000\n",
      "Epoch 461/500\n",
      "40/40 [==============================] - 0s 259us/step - loss: -215.1489 - acc: 1.0000\n",
      "Epoch 462/500\n",
      "40/40 [==============================] - 0s 225us/step - loss: -216.0311 - acc: 1.0000\n",
      "Epoch 463/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -216.9179 - acc: 1.0000\n",
      "Epoch 464/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -217.8087 - acc: 1.0000\n",
      "Epoch 465/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -218.7038 - acc: 1.0000\n",
      "Epoch 466/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -219.6032 - acc: 1.0000\n",
      "Epoch 467/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -220.5072 - acc: 1.0000\n",
      "Epoch 468/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -221.4154 - acc: 1.0000\n",
      "Epoch 469/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -222.3281 - acc: 1.0000\n",
      "Epoch 470/500\n",
      "40/40 [==============================] - 0s 212us/step - loss: -223.2452 - acc: 1.0000\n",
      "Epoch 471/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -224.1666 - acc: 1.0000\n",
      "Epoch 472/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -225.0926 - acc: 1.0000\n",
      "Epoch 473/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -226.0232 - acc: 1.0000\n",
      "Epoch 474/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -226.9580 - acc: 1.0000\n",
      "Epoch 475/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -227.8976 - acc: 1.0000\n",
      "Epoch 476/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -228.8415 - acc: 1.0000\n",
      "Epoch 477/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -229.7902 - acc: 1.0000\n",
      "Epoch 478/500\n",
      "40/40 [==============================] - 0s 275us/step - loss: -230.7433 - acc: 1.0000\n",
      "Epoch 479/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -231.7010 - acc: 1.0000\n",
      "Epoch 480/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -232.6635 - acc: 1.0000\n",
      "Epoch 481/500\n",
      "40/40 [==============================] - 0s 249us/step - loss: -233.6306 - acc: 1.0000\n",
      "Epoch 482/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -234.6024 - acc: 1.0000\n",
      "Epoch 483/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -235.5786 - acc: 1.0000\n",
      "Epoch 484/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -236.5596 - acc: 1.0000\n",
      "Epoch 485/500\n",
      "40/40 [==============================] - 0s 250us/step - loss: -237.5456 - acc: 1.0000\n",
      "Epoch 486/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -238.5364 - acc: 1.0000\n",
      "Epoch 487/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -239.5317 - acc: 1.0000\n",
      "Epoch 488/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -240.5318 - acc: 1.0000\n",
      "Epoch 489/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -241.5368 - acc: 1.0000\n",
      "Epoch 490/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -242.5468 - acc: 1.0000\n",
      "Epoch 491/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -243.5613 - acc: 1.0000\n",
      "Epoch 492/500\n",
      "40/40 [==============================] - 0s 274us/step - loss: -244.5810 - acc: 1.0000\n",
      "Epoch 493/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -245.6055 - acc: 1.0000\n",
      "Epoch 494/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -246.6349 - acc: 1.0000\n",
      "Epoch 495/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -247.6693 - acc: 1.0000\n",
      "Epoch 496/500\n",
      "40/40 [==============================] - 0s 224us/step - loss: -248.7086 - acc: 1.0000\n",
      "Epoch 497/500\n",
      "40/40 [==============================] - 0s 150us/step - loss: -249.7530 - acc: 1.0000\n",
      "Epoch 498/500\n",
      "40/40 [==============================] - 0s 175us/step - loss: -250.8023 - acc: 1.0000\n",
      "Epoch 499/500\n",
      "40/40 [==============================] - 0s 199us/step - loss: -251.8567 - acc: 1.0000\n",
      "Epoch 500/500\n",
      "40/40 [==============================] - 0s 174us/step - loss: -252.9162 - acc: 1.0000\n",
      "60/60 [==============================] - ETA:  - 0s 116us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_iris_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 170\n",
    "# }\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #15\n",
    "\n",
    "#### Adult salary LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "Epoch 1/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 4us/step - loss: 0.2370 - acc: 0.7631\n",
      "Epoch 2/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - ETA: 0s - loss: 0.2356 - acc: 0.764 - 0s 5us/step - loss: 0.2370 - acc: 0.7631\n",
      "Epoch 3/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3000 - acc: 0.700 - ETA: 0s - loss: 0.2358 - acc: 0.764 - ETA: 0s - loss: 0.2382 - acc: 0.761 - 0s 10us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 4/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - ETA: 0s - loss: 0.2357 - acc: 0.764 - 0s 6us/step - loss: 0.2388 - acc: 0.7609\n",
      "Epoch 5/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - ETA: 0s - loss: 0.2364 - acc: 0.763 - 0s 7us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 6/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - ETA: 0s - loss: 0.2375 - acc: 0.762 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 7/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - ETA: 0s - loss: 0.2374 - acc: 0.762 - ETA: 0s - loss: 0.2364 - acc: 0.763 - 0s 9us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 8/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - ETA: 0s - loss: 0.2368 - acc: 0.763 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 9/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - ETA: 0s - loss: 0.2473 - acc: 0.752 - ETA: 0s - loss: 0.2373 - acc: 0.762 - 0s 8us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 10/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - ETA: 0s - loss: 0.2368 - acc: 0.763 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 11/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - ETA: 0s - loss: 0.2331 - acc: 0.766 - 0s 7us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 12/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - ETA: 0s - loss: 0.2420 - acc: 0.758 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 13/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - ETA: 0s - loss: 0.2360 - acc: 0.764 - ETA: 0s - loss: 0.2364 - acc: 0.763 - 0s 9us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 14/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - ETA: 0s - loss: 0.2340 - acc: 0.766 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 15/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - ETA: 0s - loss: 0.2383 - acc: 0.761 - 0s 6us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 16/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1700 - acc: 0.830 - ETA: 0s - loss: 0.2423 - acc: 0.757 - 0s 8us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 17/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - ETA: 0s - loss: 0.2365 - acc: 0.763 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 18/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - ETA: 0s - loss: 0.2422 - acc: 0.757 - 0s 6us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 19/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - ETA: 0s - loss: 0.2315 - acc: 0.768 - 0s 7us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 20/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - ETA: 0s - loss: 0.2399 - acc: 0.760 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 21/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - ETA: 0s - loss: 0.2347 - acc: 0.765 - 0s 7us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 22/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - ETA: 0s - loss: 0.2388 - acc: 0.761 - 0s 6us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 23/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - ETA: 0s - loss: 0.2373 - acc: 0.762 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 24/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - ETA: 0s - loss: 0.2356 - acc: 0.764 - ETA: 0s - loss: 0.2357 - acc: 0.764 - 0s 11us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 25/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - ETA: 0s - loss: 0.2316 - acc: 0.768 - 0s 6us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 26/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - ETA: 0s - loss: 0.2392 - acc: 0.760 - ETA: 0s - loss: 0.2364 - acc: 0.763 - 0s 10us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 27/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1500 - acc: 0.850 - ETA: 0s - loss: 0.2389 - acc: 0.761 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 28/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - ETA: 0s - loss: 0.2358 - acc: 0.764 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 29/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - ETA: 0s - loss: 0.2337 - acc: 0.766 - ETA: 0s - loss: 0.2364 - acc: 0.763 - 0s 8us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 30/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - ETA: 0s - loss: 0.2451 - acc: 0.754 - 0s 7us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 31/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3100 - acc: 0.690 - ETA: 0s - loss: 0.2396 - acc: 0.760 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 32/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 33/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - ETA: 0s - loss: 0.2375 - acc: 0.762 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 34/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - ETA: 0s - loss: 0.2328 - acc: 0.767 - ETA: 0s - loss: 0.2375 - acc: 0.762 - 0s 9us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 35/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - ETA: 0s - loss: 0.2387 - acc: 0.761 - 0s 6us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 36/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - ETA: 0s - loss: 0.2370 - acc: 0.763 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 37/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - ETA: 0s - loss: 0.2381 - acc: 0.761 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 38/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 39/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 40/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 41/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 42/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.840 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 43/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 44/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 45/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 46/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 47/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 48/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 49/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 50/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 51/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 52/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1700 - acc: 0.830 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 53/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 54/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 55/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 56/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - ETA: 0s - loss: 0.2371 - acc: 0.762 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 57/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 58/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3000 - acc: 0.700 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 59/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 60/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 61/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 62/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 63/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3000 - acc: 0.700 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 64/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 65/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1700 - acc: 0.830 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 66/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 67/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 68/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 69/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 70/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 71/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 72/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 73/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 74/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 75/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 76/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 77/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.840 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 78/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 79/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 80/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 81/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3100 - acc: 0.690 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 82/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 83/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 84/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1400 - acc: 0.860 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 85/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 86/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 87/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 88/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 89/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 90/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 91/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - ETA: 0s - loss: 0.2348 - acc: 0.765 - 0s 6us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 92/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - ETA: 0s - loss: 0.2354 - acc: 0.764 - 0s 7us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 93/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - ETA: 0s - loss: 0.2361 - acc: 0.763 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 94/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - ETA: 0s - loss: 0.2351 - acc: 0.764 - 0s 6us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 95/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3000 - acc: 0.700 - ETA: 0s - loss: 0.2373 - acc: 0.762 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 96/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - ETA: 0s - loss: 0.2364 - acc: 0.763 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 97/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - ETA: 0s - loss: 0.2343 - acc: 0.765 - 0s 6us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 98/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - ETA: 0s - loss: 0.2369 - acc: 0.763 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 99/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 100/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 101/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 102/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 103/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.840 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 105/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 106/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 107/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 108/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 109/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 110/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 111/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3400 - acc: 0.660 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 112/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 113/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 114/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 115/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 116/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 117/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 118/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 119/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 120/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 121/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 122/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 123/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1700 - acc: 0.830 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 124/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 125/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 126/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 127/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 128/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 129/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 130/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 131/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.840 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 132/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 133/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 134/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3000 - acc: 0.700 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 135/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 136/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 137/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 138/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 139/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3300 - acc: 0.670 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 140/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 141/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 142/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 143/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 144/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 145/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 146/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 147/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 148/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 149/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 150/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1500 - acc: 0.850 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 151/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 152/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.840 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 153/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 154/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 155/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 156/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 157/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 158/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.840 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 159/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3100 - acc: 0.690 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 160/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 161/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 162/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1700 - acc: 0.830 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 163/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 164/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 165/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 166/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 167/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 168/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 169/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 170/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 171/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 172/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 173/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 174/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 175/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 176/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 177/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3000 - acc: 0.700 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 178/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 179/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 180/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 181/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3100 - acc: 0.690 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 182/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 183/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 184/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 185/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 186/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 187/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 188/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 189/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 190/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1700 - acc: 0.830 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 191/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 192/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 193/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 194/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 195/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 196/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 197/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 198/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 199/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3400 - acc: 0.660 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 200/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 201/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 202/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 203/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 204/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 205/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 206/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 207/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 208/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 209/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 210/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 211/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 212/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 213/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 214/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 215/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 216/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3500 - acc: 0.650 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 217/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 218/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 219/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 220/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3000 - acc: 0.700 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 221/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 222/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - ETA: 0s - loss: 0.2371 - acc: 0.762 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 223/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - ETA: 0s - loss: 0.2363 - acc: 0.763 - 0s 7us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 224/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 225/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 226/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 227/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 228/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 229/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 230/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 231/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 232/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - ETA: 0s - loss: 0.2375 - acc: 0.762 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 233/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - ETA: 0s - loss: 0.2372 - acc: 0.762 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 234/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - ETA: 0s - loss: 0.2362 - acc: 0.763 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 235/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 236/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 237/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 238/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3100 - acc: 0.690 - ETA: 0s - loss: 0.2377 - acc: 0.762 - 0s 5us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 239/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 240/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 4us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 241/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3500 - acc: 0.650 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 242/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 243/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1700 - acc: 0.830 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 244/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.840 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 245/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 246/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1200 - acc: 0.880 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 247/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2369 - acc: 0.7631\n",
      "Epoch 248/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2397 - acc: 0.7598\n",
      "Epoch 249/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 250/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 251/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 252/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3100 - acc: 0.690 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 253/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 254/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 255/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 256/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3200 - acc: 0.680 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 257/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1400 - acc: 0.860 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 258/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 259/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3000 - acc: 0.700 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 260/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3100 - acc: 0.690 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 261/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 262/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 263/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 264/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 265/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 266/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 267/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 268/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 269/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 270/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 271/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 272/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1200 - acc: 0.880 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 273/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 274/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.710 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 275/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 276/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3100 - acc: 0.690 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 278/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 279/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1700 - acc: 0.830 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 280/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 281/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 282/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 283/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 284/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 285/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 286/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 287/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 288/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 289/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 290/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 291/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1400 - acc: 0.860 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 292/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 293/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2700 - acc: 0.730 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 294/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 295/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 296/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 297/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 298/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 299/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.740 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 300/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 301/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 302/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 303/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1700 - acc: 0.830 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 304/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 305/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 306/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2800 - acc: 0.720 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 307/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 308/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 309/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.840 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 310/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 311/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.790 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 312/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 313/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.760 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 314/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 315/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1800 - acc: 0.820 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 316/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2300 - acc: 0.770 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 317/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2000 - acc: 0.800 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 318/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 319/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2200 - acc: 0.780 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 320/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.840 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 321/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.810 - 0s 3us/step - loss: 0.2364 - acc: 0.7636\n",
      "Epoch 322/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.2500 - acc: 0.750 - 0s 3us/step - loss: 0.2365 - acc: 0.7635\n",
      "Epoch 323/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.3500 - acc: 0.650 - 0s 3us/step - loss: 0.6394 - acc: 0.3606\n",
      "Epoch 324/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 325/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 326/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 327/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8500 - acc: 0.150 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 328/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 329/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 330/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8300 - acc: 0.170 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 331/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 332/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 333/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 334/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 335/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 336/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8200 - acc: 0.180 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 337/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 338/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 339/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 340/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 341/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 342/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 343/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 344/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 345/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 346/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 347/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 348/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 349/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7100 - acc: 0.290 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 350/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 351/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 352/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 353/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 354/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 355/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.6900 - acc: 0.310 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 356/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7100 - acc: 0.290 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 357/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 358/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 359/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 360/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 361/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 362/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 363/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8200 - acc: 0.180 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 364/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 365/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 366/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7100 - acc: 0.290 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 367/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8400 - acc: 0.160 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 368/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 369/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 370/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 371/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 372/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 373/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 374/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 375/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 376/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 377/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 378/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8300 - acc: 0.170 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 379/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 380/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.6800 - acc: 0.320 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 381/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 382/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 383/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 384/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 385/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 386/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 387/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 388/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.6800 - acc: 0.320 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 389/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 390/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 391/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 392/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7100 - acc: 0.290 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 393/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8400 - acc: 0.160 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 394/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 395/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 396/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 397/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 398/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 399/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 400/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 401/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 402/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 403/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 404/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 405/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7100 - acc: 0.290 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 406/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 407/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 408/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 409/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 410/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8600 - acc: 0.140 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 411/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 412/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 413/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 414/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7000 - acc: 0.300 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 415/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 416/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 417/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 418/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 419/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 420/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 421/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 422/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8500 - acc: 0.150 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 423/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 424/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 425/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8300 - acc: 0.170 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 426/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 427/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 428/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 429/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 430/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 431/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 432/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 433/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 434/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 435/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 436/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8200 - acc: 0.180 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 437/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 438/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 439/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 440/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 441/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 442/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 443/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 444/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 445/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 446/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 447/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 448/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 449/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 450/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 451/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 452/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 453/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 454/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 455/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 456/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 457/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 458/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 459/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 460/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 461/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 462/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 463/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8500 - acc: 0.150 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 464/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 465/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 466/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 467/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 468/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.6800 - acc: 0.320 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 469/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 470/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 471/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.6900 - acc: 0.310 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 472/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 473/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 474/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 475/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8000 - acc: 0.200 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 476/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7900 - acc: 0.210 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 477/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 478/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 479/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 480/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 481/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 482/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8200 - acc: 0.180 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 483/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8100 - acc: 0.190 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 484/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 485/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8500 - acc: 0.150 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 486/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 487/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.6500 - acc: 0.350 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 488/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7400 - acc: 0.260 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 489/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 490/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 491/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7500 - acc: 0.250 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 492/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7300 - acc: 0.270 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 493/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 494/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 495/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7600 - acc: 0.240 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 496/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.220 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 497/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 498/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7700 - acc: 0.230 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 499/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.7200 - acc: 0.280 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "Epoch 500/500\n",
      "13024/13024 [==============================] - ETA: 0s - loss: 0.8500 - acc: 0.150 - 0s 3us/step - loss: 0.7631 - acc: 0.2369\n",
      "19537/19537 [==============================] - ETA:  - ETA:  - 0s 3us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "\n",
    "dataset_name = \"uci_adult_salary_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "         'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "         'hours-per-week', 'native-country', 'target']\n",
    "url = \"../data/adult.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names)\n",
    "\n",
    "\n",
    "data = data[data[\"workclass\"] != \"?\"]\n",
    "data = data[data[\"occupation\"] != \"?\"]\n",
    "data = data[data[\"native-country\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "categorical_col = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country', 'target']\n",
    "\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "feature_list = names[:14]\n",
    "# Test train split #\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['target']]\n",
    "# Y = column_or_1d(Y, warn=True)\n",
    "\n",
    "params = {\n",
    "    'batch_size': 1000,\n",
    "    'epochs': 100\n",
    "}\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing tune by ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.tune as tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 1.6259 - acc: 0.5500\n",
      "60/60 [==============================] - 0s 630us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8434850215911864, 0.4666666626930237]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose iris with logistic regression as a test dataset #\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"../data/iris.csv\"\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "\n",
    "def make_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1,\n",
    "                    input_dim=4,\n",
    "                    activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_iris(args):\n",
    "    model = make_model()\n",
    "    model.fit(x_train,y_train)\n",
    "    model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-16 19:59:52,761\tWARNING worker.py:1354 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-02-16 19:59:52,763\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-16_19-59-52_11337/logs.\n",
      "2019-02-16 19:59:52,876\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:44284 to respond...\n",
      "2019-02-16 19:59:52,989\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:38375 to respond...\n",
      "2019-02-16 19:59:52,997\tINFO services.py:798 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-02-16 19:59:53,049\tINFO services.py:1360 -- Starting the Plasma object store with 3.2850935800000003 GB memory using /dev/shm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8888/notebooks/ray_ui.ipynb?token=28905882e7b4006a0c688e21e69424958e0f2e952cc53040\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': None,\n",
       " 'redis_address': '192.168.1.4:44284',\n",
       " 'object_store_address': '/tmp/ray/session_2019-02-16_19-59-52_11337/sockets/plasma_store',\n",
       " 'webui_url': 'http://localhost:8888/notebooks/ray_ui.ipynb?token=28905882e7b4006a0c688e21e69424958e0f2e952cc53040',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-02-16_19-59-52_11337/sockets/raylet'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the signature of train function to accomodate Tune #\n",
    "def train_iris_tune(config, reporter):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        config (dict): Parameters provided from the search algorithm\n",
    "            or variant generation.\n",
    "        reporter (Reporter): Handle to report intermediate metrics to Tune.\n",
    "    \"\"\"\n",
    "    model = make_model()\n",
    "    model.fit(x_train,y_train)\n",
    "    accuracy = model.evaluate(x_test, y_test)[1]\n",
    "    reporter(mean_accuracy=accuracy, metric2=1, metric3=0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'test_reporter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a0aacb736dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_reporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtest_reporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iris_tune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'test_reporter'"
     ]
    }
   ],
   "source": [
    "from ray import test_reporter\n",
    "assert test_reporter(train_iris_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a Tune experiment #\n",
    "# 1) Search space\n",
    "# 2) Stopping criteria\n",
    "\n",
    "configuration = tune.Experiment(\n",
    "    \"experiment_name\",\n",
    "    run=train_iris_tune,\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    stop={\"mean_accuracy\": 95},  # TODO: Part 1\n",
    "    config={\n",
    "        \"optimizer\": tune.grid_search(['adam', 'nadam'])\n",
    "    }  # TODO: Part 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-17 11:28:02,985\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-17 11:28:02,986\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/8.2 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/8.2 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_iris_tune_1_optimizer=nadam:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_iris_tune_0_optimizer=adam:\tRUNNING\n",
      "\n",
      "Result for train_iris_tune_1_optimizer=nadam:\n",
      "  date: 2019-02-17_11-28-05\n",
      "  done: false\n",
      "  experiment_id: 49523f8d5a164a08a23bfbd2b5e5800c\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.4666666626930237\n",
      "  metric2: 1\n",
      "  metric3: 0.3\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 23323\n",
      "  time_since_restore: 1.0010333061218262\n",
      "  time_this_iter_s: 1.0010333061218262\n",
      "  time_total_s: 1.0010333061218262\n",
      "  timestamp: 1550383085\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_0_optimizer=adam:\n",
      "  date: 2019-02-17_11-28-05\n",
      "  done: false\n",
      "  experiment_id: cf285405c95e47e19c57db48b29f9ea7\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.4666666626930237\n",
      "  metric2: 1\n",
      "  metric3: 0.3\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 23321\n",
      "  time_since_restore: 1.0004656314849854\n",
      "  time_this_iter_s: 1.0004656314849854\n",
      "  time_total_s: 1.0004656314849854\n",
      "  timestamp: 1550383085\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_1_optimizer=nadam:\n",
      "  date: 2019-02-17_11-28-06\n",
      "  done: true\n",
      "  experiment_id: 49523f8d5a164a08a23bfbd2b5e5800c\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.4666666626930237\n",
      "  metric2: 1\n",
      "  metric3: 0.3\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 23323\n",
      "  time_since_restore: 2.0022518634796143\n",
      "  time_this_iter_s: 1.001218557357788\n",
      "  time_total_s: 2.0022518634796143\n",
      "  timestamp: 1550383086\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_iris_tune_0_optimizer=adam:\n",
      "  date: 2019-02-17_11-28-06\n",
      "  done: true\n",
      "  experiment_id: cf285405c95e47e19c57db48b29f9ea7\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.4666666626930237\n",
      "  metric2: 1\n",
      "  metric3: 0.3\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 23321\n",
      "  time_since_restore: 2.001732110977173\n",
      "  time_this_iter_s: 1.0012664794921875\n",
      "  time_total_s: 2.001732110977173\n",
      "  timestamp: 1550383086\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/8.2 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_iris_tune_0_optimizer=adam:\tTERMINATED [pid=23321], 2 s, 2 iter, 0.467 acc\n",
      " - train_iris_tune_1_optimizer=nadam:\tTERMINATED [pid=23323], 2 s, 2 iter, 0.467 acc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the experiment #\n",
    "trials = tune.run_experiments(configuration, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_best_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3f9cda0cf31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_best_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The best result is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_best_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_best_result'"
     ]
    }
   ],
   "source": [
    "from ray.tune.util import \n",
    "# print(\"The best result is\", get_best_result(trials, metric=\"mean_accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               False\n",
       "workclass         False\n",
       "fnlwgt            False\n",
       "education         False\n",
       "education-num     False\n",
       "marital-status    False\n",
       "occupation        False\n",
       "relationship      False\n",
       "race              False\n",
       "sex               False\n",
       "capital-gain      False\n",
       "capital-loss      False\n",
       "hours-per-week    False\n",
       "native-country    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isinf(X).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #16\n",
    "\n",
    "#### Abalone LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n",
      "Epoch 1/100\n",
      "529/529 [==============================] - 1s 1ms/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 2/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 3/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 4/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0051 - acc: 0.4726\n",
      "Epoch 5/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 6/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 7/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 8/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 9/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 10/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 11/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 12/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0050 - acc: 0.4726\n",
      "Epoch 13/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 14/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 15/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 16/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 17/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 18/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 19/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 20/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0049 - acc: 0.4726\n",
      "Epoch 21/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 22/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 23/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 24/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 25/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 26/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0048 - acc: 0.4726\n",
      "Epoch 27/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 28/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 29/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 30/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 31/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 32/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0047 - acc: 0.4726\n",
      "Epoch 33/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 34/100\n",
      "529/529 [==============================] - 0s 15us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 35/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 36/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 37/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0046 - acc: 0.4726\n",
      "Epoch 38/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 39/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 40/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 41/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 42/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0045 - acc: 0.4726\n",
      "Epoch 43/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 44/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 45/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 46/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0044 - acc: 0.4726\n",
      "Epoch 47/100\n",
      "529/529 [==============================] - 0s 17us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 48/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 49/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 50/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0043 - acc: 0.4726\n",
      "Epoch 51/100\n",
      "529/529 [==============================] - 0s 15us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 52/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 53/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 54/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0042 - acc: 0.4726\n",
      "Epoch 55/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0041 - acc: 0.4726\n",
      "Epoch 56/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0041 - acc: 0.4726\n",
      "Epoch 57/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0041 - acc: 0.4726\n",
      "Epoch 58/100\n",
      "529/529 [==============================] - 0s 15us/step - loss: 0.0040 - acc: 0.4726\n",
      "Epoch 59/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0040 - acc: 0.4726\n",
      "Epoch 60/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0040 - acc: 0.4726\n",
      "Epoch 61/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 62/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 63/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 64/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0039 - acc: 0.4726\n",
      "Epoch 65/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0038 - acc: 0.4726\n",
      "Epoch 66/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0038 - acc: 0.4726\n",
      "Epoch 67/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0037 - acc: 0.4726\n",
      "Epoch 68/100\n",
      "529/529 [==============================] - 0s 11us/step - loss: 0.0037 - acc: 0.4726\n",
      "Epoch 69/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0037 - acc: 0.4726\n",
      "Epoch 70/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0036 - acc: 0.4726\n",
      "Epoch 71/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0036 - acc: 0.4726\n",
      "Epoch 72/100\n",
      "529/529 [==============================] - 0s 17us/step - loss: 0.0036 - acc: 0.4726\n",
      "Epoch 73/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0035 - acc: 0.4726\n",
      "Epoch 74/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0035 - acc: 0.4726\n",
      "Epoch 75/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0034 - acc: 0.4726\n",
      "Epoch 76/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0034 - acc: 0.4726\n",
      "Epoch 77/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0034 - acc: 0.4726\n",
      "Epoch 78/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0033 - acc: 0.4726\n",
      "Epoch 79/100\n",
      "529/529 [==============================] - 0s 17us/step - loss: 0.0033 - acc: 0.4726\n",
      "Epoch 80/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0032 - acc: 0.4726\n",
      "Epoch 81/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0032 - acc: 0.4726\n",
      "Epoch 82/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0031 - acc: 0.4726\n",
      "Epoch 83/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0031 - acc: 0.4726\n",
      "Epoch 84/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0030 - acc: 0.4726\n",
      "Epoch 85/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0030 - acc: 0.4726\n",
      "Epoch 86/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0030 - acc: 0.4726\n",
      "Epoch 87/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0029 - acc: 0.4726\n",
      "Epoch 88/100\n",
      "529/529 [==============================] - 0s 9us/step - loss: 0.0029 - acc: 0.4726\n",
      "Epoch 89/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0028 - acc: 0.4726\n",
      "Epoch 90/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0027 - acc: 0.4726\n",
      "Epoch 91/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0027 - acc: 0.4726\n",
      "Epoch 92/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0026 - acc: 0.4726\n",
      "Epoch 93/100\n",
      "529/529 [==============================] - 0s 13us/step - loss: 0.0026 - acc: 0.4726\n",
      "Epoch 94/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0025 - acc: 0.4726\n",
      "Epoch 95/100\n",
      "529/529 [==============================] - 0s 6us/step - loss: 0.0025 - acc: 0.4726\n",
      "Epoch 96/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0024 - acc: 0.4726\n",
      "Epoch 97/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0024 - acc: 0.4726\n",
      "Epoch 98/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0023 - acc: 0.4726\n",
      "Epoch 99/100\n",
      "529/529 [==============================] - 0s 8us/step - loss: 0.0022 - acc: 0.4726\n",
      "Epoch 100/100\n",
      "529/529 [==============================] - 0s 4us/step - loss: 0.0022 - acc: 0.4726\n",
      "794/794 [==============================] - ETA:  - 0s 14us/step\n"
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "dataset_info = automation_script.get_dataset_info(\"uci_abalone_lda\")\n",
    "\n",
    "names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\",\n",
    "        \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "url = \"../data/abalone.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "data.head()\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "col_names = data.columns\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "categorical_col = ['sex']\n",
    "for col in categorical_col:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "    \n",
    "# Filter dataset to contain 'rings' 9 and 10 #\n",
    "data = data[data['rings'].isin([9,10])]\n",
    "data['rings'] = data['rings'].map({9: 0, 10: 1})\n",
    "\n",
    "\n",
    "feature_list = names[:7]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['rings']]\n",
    "\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #17\n",
    "\n",
    "#### Mushroom LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields with missing values\n",
      "stalk-root\n",
      "2480\n",
      "30.53%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0564 - acc: 0.5432\n",
      "Epoch 2/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0572 - acc: 0.5445\n",
      "Epoch 3/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0581 - acc: 0.5467\n",
      "Epoch 4/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0590 - acc: 0.5476\n",
      "Epoch 5/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0599 - acc: 0.5498\n",
      "Epoch 6/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0608 - acc: 0.5516\n",
      "Epoch 7/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0617 - acc: 0.5516\n",
      "Epoch 8/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0627 - acc: 0.5521\n",
      "Epoch 9/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0636 - acc: 0.5529\n",
      "Epoch 10/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0646 - acc: 0.5525\n",
      "Epoch 11/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0656 - acc: 0.5543\n",
      "Epoch 12/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0666 - acc: 0.5552\n",
      "Epoch 13/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0676 - acc: 0.5556\n",
      "Epoch 14/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0687 - acc: 0.5565\n",
      "Epoch 15/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0697 - acc: 0.5569\n",
      "Epoch 16/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0708 - acc: 0.5583\n",
      "Epoch 17/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0719 - acc: 0.5583\n",
      "Epoch 18/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0730 - acc: 0.5591\n",
      "Epoch 19/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0741 - acc: 0.5605\n",
      "Epoch 20/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0752 - acc: 0.5614\n",
      "Epoch 21/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.0764 - acc: 0.5631\n",
      "Epoch 22/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0775 - acc: 0.5662\n",
      "Epoch 23/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0787 - acc: 0.5667\n",
      "Epoch 24/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0800 - acc: 0.5702\n",
      "Epoch 25/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0812 - acc: 0.5698\n",
      "Epoch 26/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.0825 - acc: 0.5711\n",
      "Epoch 27/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0837 - acc: 0.5742\n",
      "Epoch 28/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0850 - acc: 0.5742\n",
      "Epoch 29/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0864 - acc: 0.5755\n",
      "Epoch 30/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0877 - acc: 0.5778\n",
      "Epoch 31/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.0891 - acc: 0.5778\n",
      "Epoch 32/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.0904 - acc: 0.5804\n",
      "Epoch 33/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0918 - acc: 0.5813\n",
      "Epoch 34/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0933 - acc: 0.5813\n",
      "Epoch 35/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0947 - acc: 0.5826\n",
      "Epoch 36/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.0962 - acc: 0.5840\n",
      "Epoch 37/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.0977 - acc: 0.5857\n",
      "Epoch 38/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.0992 - acc: 0.5866\n",
      "Epoch 39/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1007 - acc: 0.5888\n",
      "Epoch 40/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1022 - acc: 0.5911\n",
      "Epoch 41/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1038 - acc: 0.5919\n",
      "Epoch 42/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.1054 - acc: 0.5942\n",
      "Epoch 43/100\n",
      "2257/2257 [==============================] - 0s 6us/step - loss: -0.1070 - acc: 0.5937\n",
      "Epoch 44/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1086 - acc: 0.5942\n",
      "Epoch 45/100\n",
      "2257/2257 [==============================] - 0s 6us/step - loss: -0.1103 - acc: 0.5955\n",
      "Epoch 46/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1119 - acc: 0.5959\n",
      "Epoch 47/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1136 - acc: 0.5981\n",
      "Epoch 48/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.1153 - acc: 0.5964\n",
      "Epoch 49/100\n",
      "2257/2257 [==============================] - 0s 5us/step - loss: -0.1170 - acc: 0.5977\n",
      "Epoch 50/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1187 - acc: 0.5995\n",
      "Epoch 51/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1205 - acc: 0.6008\n",
      "Epoch 52/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1223 - acc: 0.5999\n",
      "Epoch 53/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1241 - acc: 0.5990\n",
      "Epoch 54/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1259 - acc: 0.6008\n",
      "Epoch 55/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1277 - acc: 0.6008\n",
      "Epoch 56/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1295 - acc: 0.6008\n",
      "Epoch 57/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1314 - acc: 0.6026\n",
      "Epoch 58/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1333 - acc: 0.6043\n",
      "Epoch 59/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1352 - acc: 0.6061\n",
      "Epoch 60/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1371 - acc: 0.6061\n",
      "Epoch 61/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1390 - acc: 0.6079\n",
      "Epoch 62/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1409 - acc: 0.6079\n",
      "Epoch 63/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1429 - acc: 0.6088\n",
      "Epoch 64/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1449 - acc: 0.6097\n",
      "Epoch 65/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.1469 - acc: 0.6097\n",
      "Epoch 66/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1489 - acc: 0.6114\n",
      "Epoch 67/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1509 - acc: 0.6123\n",
      "Epoch 68/100\n",
      "2257/2257 [==============================] - 0s 4us/step - loss: -0.1529 - acc: 0.6128\n",
      "Epoch 69/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1549 - acc: 0.6128\n",
      "Epoch 70/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1570 - acc: 0.6145\n",
      "Epoch 71/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1591 - acc: 0.6154\n",
      "Epoch 72/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1611 - acc: 0.6154\n",
      "Epoch 73/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1632 - acc: 0.6167\n",
      "Epoch 74/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1653 - acc: 0.6167\n",
      "Epoch 75/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1674 - acc: 0.6176\n",
      "Epoch 76/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1696 - acc: 0.6185\n",
      "Epoch 77/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1717 - acc: 0.6194\n",
      "Epoch 78/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.1738 - acc: 0.6207\n",
      "Epoch 79/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1760 - acc: 0.6221\n",
      "Epoch 80/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1781 - acc: 0.6234\n",
      "Epoch 81/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1803 - acc: 0.6238\n",
      "Epoch 82/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1825 - acc: 0.6243\n",
      "Epoch 83/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1847 - acc: 0.6256\n",
      "Epoch 84/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1868 - acc: 0.6269\n",
      "Epoch 85/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1890 - acc: 0.6269\n",
      "Epoch 86/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1912 - acc: 0.6287\n",
      "Epoch 87/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.1934 - acc: 0.6296\n",
      "Epoch 88/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1957 - acc: 0.6296\n",
      "Epoch 89/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.1979 - acc: 0.6305\n",
      "Epoch 90/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.2001 - acc: 0.6318\n",
      "Epoch 91/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.2023 - acc: 0.6323\n",
      "Epoch 92/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.2046 - acc: 0.6331\n",
      "Epoch 93/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2068 - acc: 0.6336\n",
      "Epoch 94/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2090 - acc: 0.6336\n",
      "Epoch 95/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2113 - acc: 0.6340\n",
      "Epoch 96/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.2135 - acc: 0.6336\n",
      "Epoch 97/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2158 - acc: 0.6349\n",
      "Epoch 98/100\n",
      "2257/2257 [==============================] - 0s 2us/step - loss: -0.2180 - acc: 0.6358\n",
      "Epoch 99/100\n",
      "2257/2257 [==============================] - 0s 3us/step - loss: -0.2202 - acc: 0.6380\n",
      "Epoch 100/100\n",
      "2257/2257 [==============================] - 0s 1us/step - loss: -0.2225 - acc: 0.6389\n",
      "3387/3387 [==============================] - ETA:  - 0s 13us/step\n"
     ]
    }
   ],
   "source": [
    "# Load dataset info #\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "dataset_name = \"uci_mushroom_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "names = ['classes', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n",
    "        'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "        'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "        'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color',\n",
    "        'population', 'habitat']\n",
    "url = \"../data/mushroom.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, names=names, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "print(\"Fields with missing values\")\n",
    "col_names = data.columns\n",
    "num_data = data.shape[0]\n",
    "for c in col_names:\n",
    "    num_non = data[c].isin([\"?\"]).sum()\n",
    "    if num_non > 0:\n",
    "        print (c)\n",
    "        print (num_non)\n",
    "        print (\"{0:.2f}%\".format(float(num_non) / num_data * 100))\n",
    "        print (\"\\n\")\n",
    "\n",
    "data = data[data[\"stalk-root\"] != \"?\"]\n",
    "\n",
    "# Convert categorical fields #\n",
    "\n",
    "for col in names:\n",
    "    b, c = np.unique(data[col], return_inverse=True)\n",
    "    data[col] = c\n",
    "\n",
    "# Split the dataset into test and train datasets #\n",
    "feature_list = names[1:23]\n",
    "X = data.loc[:, feature_list]\n",
    "Y = data[['classes']]\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset  #18\n",
    "\n",
    "#### Ad dataset LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "943/943 [==============================] - ETA: 5s - loss: 0.0160 - acc: 0.370 - ETA: 4s - loss: 0.0439 - acc: 0.455 - ETA: 4s - loss: 0.0825 - acc: 0.466 - ETA: 3s - loss: 0.0449 - acc: 0.480 - ETA: 2s - loss: 0.0451 - acc: 0.484 - ETA: 2s - loss: 0.0312 - acc: 0.470 - ETA: 1s - loss: 0.0100 - acc: 0.462 - ETA: 0s - loss: -0.0063 - acc: 0.45 - ETA: 0s - loss: -0.0262 - acc: 0.46 - 6s 6ms/step - loss: -0.0298 - acc: 0.4581\n",
      "Epoch 2/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3217 - acc: 0.38 - ETA: 4s - loss: -0.2402 - acc: 0.43 - ETA: 3s - loss: -0.2131 - acc: 0.45 - ETA: 2s - loss: -0.2276 - acc: 0.46 - ETA: 2s - loss: -0.2561 - acc: 0.46 - ETA: 1s - loss: -0.2503 - acc: 0.47 - ETA: 1s - loss: -0.3107 - acc: 0.47 - ETA: 0s - loss: -0.3132 - acc: 0.48 - ETA: 0s - loss: -0.3395 - acc: 0.48 - 5s 6ms/step - loss: -0.3187 - acc: 0.4878\n",
      "Epoch 3/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1564 - acc: 0.47 - ETA: 3s - loss: -0.3541 - acc: 0.47 - ETA: 3s - loss: -0.3828 - acc: 0.46 - ETA: 2s - loss: -0.3830 - acc: 0.47 - ETA: 2s - loss: -0.4156 - acc: 0.47 - ETA: 1s - loss: -0.4448 - acc: 0.48 - ETA: 1s - loss: -0.4328 - acc: 0.49 - ETA: 0s - loss: -0.4359 - acc: 0.49 - ETA: 0s - loss: -0.4410 - acc: 0.49 - 5s 6ms/step - loss: -0.4407 - acc: 0.4910\n",
      "Epoch 4/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4342 - acc: 0.57 - ETA: 4s - loss: -0.5157 - acc: 0.54 - ETA: 3s - loss: -0.4810 - acc: 0.53 - ETA: 3s - loss: -0.5172 - acc: 0.50 - ETA: 2s - loss: -0.5215 - acc: 0.51 - ETA: 1s - loss: -0.5480 - acc: 0.50 - ETA: 1s - loss: -0.5572 - acc: 0.50 - ETA: 0s - loss: -0.5600 - acc: 0.49 - ETA: 0s - loss: -0.5808 - acc: 0.49 - 5s 6ms/step - loss: -0.5869 - acc: 0.4889\n",
      "Epoch 5/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7736 - acc: 0.56 - ETA: 3s - loss: -0.7013 - acc: 0.54 - ETA: 3s - loss: -0.7459 - acc: 0.51 - ETA: 2s - loss: -0.7248 - acc: 0.49 - ETA: 2s - loss: -0.7558 - acc: 0.47 - ETA: 1s - loss: -0.7678 - acc: 0.46 - ETA: 1s - loss: -0.7710 - acc: 0.47 - ETA: 0s - loss: -0.7515 - acc: 0.48 - ETA: 0s - loss: -0.7559 - acc: 0.49 - 5s 6ms/step - loss: -0.7629 - acc: 0.4899\n",
      "Epoch 6/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8415 - acc: 0.42 - ETA: 3s - loss: -0.8272 - acc: 0.46 - ETA: 3s - loss: -0.8596 - acc: 0.45 - ETA: 2s - loss: -0.8581 - acc: 0.46 - ETA: 2s - loss: -0.8643 - acc: 0.49 - ETA: 1s - loss: -0.8808 - acc: 0.48 - ETA: 1s - loss: -0.8600 - acc: 0.48 - ETA: 0s - loss: -0.8665 - acc: 0.47 - ETA: 0s - loss: -0.8640 - acc: 0.48 - 5s 6ms/step - loss: -0.8745 - acc: 0.4772\n",
      "Epoch 7/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9354 - acc: 0.53 - ETA: 4s - loss: -0.8707 - acc: 0.47 - ETA: 3s - loss: -0.9284 - acc: 0.48 - ETA: 2s - loss: -0.9676 - acc: 0.47 - ETA: 2s - loss: -0.9602 - acc: 0.48 - ETA: 1s - loss: -0.9570 - acc: 0.49 - ETA: 1s - loss: -0.9885 - acc: 0.47 - ETA: 0s - loss: -0.9971 - acc: 0.46 - ETA: 0s - loss: -0.9982 - acc: 0.46 - 5s 6ms/step - loss: -1.0014 - acc: 0.4624\n",
      "Epoch 8/500\n",
      "943/943 [==============================] - ETA: 5s - loss: -0.9578 - acc: 0.48 - ETA: 4s - loss: -0.9893 - acc: 0.47 - ETA: 3s - loss: -0.9935 - acc: 0.46 - ETA: 2s - loss: -1.0052 - acc: 0.46 - ETA: 2s - loss: -1.0208 - acc: 0.47 - ETA: 1s - loss: -1.0369 - acc: 0.46 - ETA: 1s - loss: -1.0928 - acc: 0.45 - ETA: 0s - loss: -1.0877 - acc: 0.44 - ETA: 0s - loss: -1.0968 - acc: 0.43 - 5s 6ms/step - loss: -1.1179 - acc: 0.4305\n",
      "Epoch 9/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1992 - acc: 0.45 - ETA: 3s - loss: -1.1744 - acc: 0.42 - ETA: 3s - loss: -1.1121 - acc: 0.44 - ETA: 2s - loss: -1.1507 - acc: 0.42 - ETA: 2s - loss: -1.1815 - acc: 0.40 - ETA: 1s - loss: -1.1978 - acc: 0.39 - ETA: 1s - loss: -1.2149 - acc: 0.39 - ETA: 0s - loss: -1.2576 - acc: 0.38 - ETA: 0s - loss: -1.2534 - acc: 0.38 - 5s 6ms/step - loss: -1.2466 - acc: 0.3796\n",
      "Epoch 10/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3155 - acc: 0.34 - ETA: 4s - loss: -1.3431 - acc: 0.33 - ETA: 3s - loss: -1.5020 - acc: 0.30 - ETA: 2s - loss: -1.4455 - acc: 0.32 - ETA: 2s - loss: -1.4224 - acc: 0.32 - ETA: 1s - loss: -1.4044 - acc: 0.33 - ETA: 1s - loss: -1.4756 - acc: 0.31 - ETA: 0s - loss: -1.5248 - acc: 0.31 - ETA: 0s - loss: -1.5189 - acc: 0.31 - 5s 6ms/step - loss: -1.5969 - acc: 0.3065\n",
      "Epoch 11/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8700 - acc: 0.20 - ETA: 3s - loss: -2.0790 - acc: 0.22 - ETA: 3s - loss: -1.9102 - acc: 0.25 - ETA: 2s - loss: -1.8087 - acc: 0.25 - ETA: 2s - loss: -1.8741 - acc: 0.24 - ETA: 1s - loss: -1.8377 - acc: 0.24 - ETA: 1s - loss: -1.8553 - acc: 0.25 - ETA: 0s - loss: -1.9868 - acc: 0.24 - ETA: 0s - loss: -1.9661 - acc: 0.24 - 5s 5ms/step - loss: -1.9035 - acc: 0.2460\n",
      "Epoch 12/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.2500 - acc: 0.16 - ETA: 3s - loss: -2.6354 - acc: 0.20 - ETA: 3s - loss: -2.4525 - acc: 0.19 - ETA: 2s - loss: -2.4763 - acc: 0.18 - ETA: 2s - loss: -2.6246 - acc: 0.18 - ETA: 1s - loss: -2.4844 - acc: 0.18 - ETA: 1s - loss: -2.5068 - acc: 0.18 - ETA: 0s - loss: -2.4707 - acc: 0.19 - ETA: 0s - loss: -2.5405 - acc: 0.19 - 5s 6ms/step - loss: -2.5071 - acc: 0.1941\n",
      "Epoch 13/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.4867 - acc: 0.18 - ETA: 4s - loss: -3.0096 - acc: 0.15 - ETA: 3s - loss: -2.7626 - acc: 0.14 - ETA: 2s - loss: -3.3102 - acc: 0.12 - ETA: 2s - loss: -3.7300 - acc: 0.10 - ETA: 1s - loss: -3.3260 - acc: 0.10 - ETA: 1s - loss: -3.1592 - acc: 0.10 - ETA: 0s - loss: -3.2059 - acc: 0.10 - ETA: 0s - loss: -3.1367 - acc: 0.10 - 6s 6ms/step - loss: -3.1375 - acc: 0.0986\n",
      "Epoch 14/500\n",
      "943/943 [==============================] - ETA: 5s - loss: -6.5389 - acc: 0.04 - ETA: 4s - loss: -4.3411 - acc: 0.09 - ETA: 3s - loss: -3.6538 - acc: 0.08 - ETA: 3s - loss: -3.5941 - acc: 0.07 - ETA: 2s - loss: -3.4739 - acc: 0.07 - ETA: 1s - loss: -4.0584 - acc: 0.06 - ETA: 1s - loss: -3.7578 - acc: 0.07 - ETA: 0s - loss: -4.0786 - acc: 0.07 - ETA: 0s - loss: -4.1159 - acc: 0.06 - 6s 6ms/step - loss: -4.0747 - acc: 0.0668\n",
      "Epoch 15/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.9294 - acc: 0.03 - ETA: 3s - loss: -4.9509 - acc: 0.03 - ETA: 3s - loss: -4.0062 - acc: 0.03 - ETA: 2s - loss: -4.4747 - acc: 0.03 - ETA: 2s - loss: -4.3081 - acc: 0.03 - ETA: 1s - loss: -5.4833 - acc: 0.03 - ETA: 1s - loss: -5.0112 - acc: 0.03 - ETA: 0s - loss: -4.9100 - acc: 0.03 - ETA: 0s - loss: -4.8345 - acc: 0.03 - 5s 6ms/step - loss: -4.7120 - acc: 0.0339\n",
      "Epoch 16/500\n",
      "943/943 [==============================] - ETA: 5s - loss: -7.5979 - acc: 0.02 - ETA: 4s - loss: -4.9143 - acc: 0.03 - ETA: 3s - loss: -6.6042 - acc: 0.02 - ETA: 3s - loss: -6.3261 - acc: 0.02 - ETA: 2s - loss: -7.0113 - acc: 0.02 - ETA: 1s - loss: -6.3156 - acc: 0.02 - ETA: 1s - loss: -5.7874 - acc: 0.02 - ETA: 0s - loss: -5.4849 - acc: 0.02 - ETA: 0s - loss: -5.2796 - acc: 0.02 - 5s 6ms/step - loss: -5.5278 - acc: 0.0223\n",
      "Epoch 17/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -7.4313 - acc: 0.0000e+ - ETA: 4s - loss: -8.8041 - acc: 0.0050   - ETA: 3s - loss: -7.6933 - acc: 0.01 - ETA: 3s - loss: -6.5857 - acc: 0.01 - ETA: 2s - loss: -5.5223 - acc: 0.01 - ETA: 1s - loss: -5.3908 - acc: 0.01 - ETA: 1s - loss: -5.5671 - acc: 0.01 - ETA: 0s - loss: -5.4532 - acc: 0.01 - ETA: 0s - loss: -6.3227 - acc: 0.01 - 5s 6ms/step - loss: -6.4098 - acc: 0.0138\n",
      "Epoch 18/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -9.8372 - acc: 0.0000e+ - ETA: 3s - loss: -6.1016 - acc: 0.0100   - ETA: 3s - loss: -5.1387 - acc: 0.01 - ETA: 2s - loss: -5.4604 - acc: 0.01 - ETA: 2s - loss: -5.8865 - acc: 0.01 - ETA: 1s - loss: -5.5401 - acc: 0.01 - ETA: 1s - loss: -5.0050 - acc: 0.01 - ETA: 0s - loss: -4.6309 - acc: 0.01 - ETA: 0s - loss: -6.8430 - acc: 0.01 - 5s 6ms/step - loss: -7.4547 - acc: 0.0138\n",
      "Epoch 19/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -21.7013 - acc: 0.0000e+0 - ETA: 4s - loss: -12.9301 - acc: 0.0050    - ETA: 3s - loss: -9.3100 - acc: 0.010 - ETA: 2s - loss: -7.7657 - acc: 0.01 - ETA: 2s - loss: -7.3192 - acc: 0.01 - ETA: 1s - loss: -7.0943 - acc: 0.01 - ETA: 1s - loss: -6.8573 - acc: 0.01 - ETA: 0s - loss: -8.5374 - acc: 0.01 - ETA: 0s - loss: -7.8992 - acc: 0.01 - 5s 6ms/step - loss: -8.4697 - acc: 0.0117\n",
      "Epoch 20/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1180 - acc: 0.03 - ETA: 3s - loss: -1.9603 - acc: 0.02 - ETA: 3s - loss: -6.0405 - acc: 0.01 - ETA: 2s - loss: -5.1471 - acc: 0.02 - ETA: 2s - loss: -5.6669 - acc: 0.02 - ETA: 1s - loss: -10.0366 - acc: 0.016 - ETA: 1s - loss: -9.5850 - acc: 0.014 - ETA: 0s - loss: -8.6051 - acc: 0.01 - ETA: 0s - loss: -8.0274 - acc: 0.01 - 5s 5ms/step - loss: -8.7430 - acc: 0.0138\n",
      "Epoch 21/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -10.9605 - acc: 0.020 - ETA: 3s - loss: -7.8338 - acc: 0.020 - ETA: 3s - loss: -9.8828 - acc: 0.01 - ETA: 2s - loss: -9.0310 - acc: 0.01 - ETA: 2s - loss: -10.4831 - acc: 0.010 - ETA: 1s - loss: -8.9954 - acc: 0.011 - ETA: 1s - loss: -7.8493 - acc: 0.01 - ETA: 0s - loss: -7.5603 - acc: 0.01 - ETA: 0s - loss: -7.1663 - acc: 0.01 - 5s 5ms/step - loss: -7.1718 - acc: 0.0127\n",
      "Epoch 22/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -8.7781 - acc: 0.01 - ETA: 4s - loss: -5.0755 - acc: 0.01 - ETA: 3s - loss: -5.1448 - acc: 0.01 - ETA: 3s - loss: -6.2847 - acc: 0.01 - ETA: 2s - loss: -5.6560 - acc: 0.01 - ETA: 1s - loss: -7.9994 - acc: 0.01 - ETA: 1s - loss: -8.3657 - acc: 0.01 - ETA: 0s - loss: -7.8766 - acc: 0.01 - ETA: 0s - loss: -7.5530 - acc: 0.01 - 5s 6ms/step - loss: -10.4166 - acc: 0.0117\n",
      "Epoch 23/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -12.2975 - acc: 0.010 - ETA: 3s - loss: -9.1960 - acc: 0.015 - ETA: 3s - loss: -16.9202 - acc: 0.010 - ETA: 2s - loss: -17.3017 - acc: 0.010 - ETA: 2s - loss: -14.3680 - acc: 0.010 - ETA: 1s - loss: -12.1361 - acc: 0.015 - ETA: 1s - loss: -10.9005 - acc: 0.014 - ETA: 0s - loss: -16.5870 - acc: 0.012 - ETA: 0s - loss: -14.9335 - acc: 0.013 - 5s 5ms/step - loss: -14.4280 - acc: 0.0127\n",
      "Epoch 24/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8066 - acc: 0.03 - ETA: 3s - loss: -16.5788 - acc: 0.015 - ETA: 3s - loss: -14.3017 - acc: 0.013 - ETA: 2s - loss: -11.8080 - acc: 0.012 - ETA: 2s - loss: -10.1618 - acc: 0.014 - ETA: 1s - loss: -10.1670 - acc: 0.011 - ETA: 1s - loss: -8.9789 - acc: 0.014 - ETA: 0s - loss: -8.1729 - acc: 0.01 - ETA: 0s - loss: -7.6781 - acc: 0.01 - 5s 6ms/step - loss: -8.6400 - acc: 0.0127\n",
      "Epoch 25/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.0200 - acc: 0.02 - ETA: 4s - loss: -4.4845 - acc: 0.01 - ETA: 3s - loss: -3.5998 - acc: 0.01 - ETA: 2s - loss: -3.6341 - acc: 0.01 - ETA: 2s - loss: -23.1304 - acc: 0.012 - ETA: 1s - loss: -19.6279 - acc: 0.011 - ETA: 1s - loss: -21.7612 - acc: 0.011 - ETA: 0s - loss: -19.4288 - acc: 0.011 - ETA: 0s - loss: -17.4927 - acc: 0.013 - 5s 6ms/step - loss: -17.0126 - acc: 0.0138\n",
      "Epoch 26/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.2522 - acc: 0.01 - ETA: 3s - loss: -6.2811 - acc: 0.01 - ETA: 3s - loss: -5.0968 - acc: 0.01 - ETA: 2s - loss: -4.1746 - acc: 0.01 - ETA: 2s - loss: -3.3650 - acc: 0.01 - ETA: 1s - loss: -4.6124 - acc: 0.01 - ETA: 1s - loss: -4.5239 - acc: 0.01 - ETA: 0s - loss: -4.5166 - acc: 0.01 - ETA: 0s - loss: -4.1410 - acc: 0.01 - 5s 6ms/step - loss: -5.5312 - acc: 0.0159\n",
      "Epoch 27/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9162 - acc: 0.03 - ETA: 3s - loss: -1.2845 - acc: 0.03 - ETA: 3s - loss: -2.6622 - acc: 0.02 - ETA: 2s - loss: -2.2533 - acc: 0.02 - ETA: 2s - loss: -3.5334 - acc: 0.02 - ETA: 1s - loss: -9.6285 - acc: 0.01 - ETA: 1s - loss: -16.7014 - acc: 0.014 - ETA: 0s - loss: -14.7134 - acc: 0.013 - ETA: 0s - loss: -13.3458 - acc: 0.015 - 5s 6ms/step - loss: -15.7985 - acc: 0.0148\n",
      "Epoch 28/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -6.1332 - acc: 0.01 - ETA: 4s - loss: -4.9791 - acc: 0.01 - ETA: 3s - loss: -3.7097 - acc: 0.01 - ETA: 2s - loss: -2.9244 - acc: 0.01 - ETA: 2s - loss: -8.5549 - acc: 0.01 - ETA: 1s - loss: -8.1134 - acc: 0.01 - ETA: 1s - loss: -8.1042 - acc: 0.01 - ETA: 0s - loss: -7.4352 - acc: 0.01 - ETA: 0s - loss: -7.4161 - acc: 0.01 - 5s 6ms/step - loss: -7.2284 - acc: 0.0148\n",
      "Epoch 29/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.3563 - acc: 0.01 - ETA: 3s - loss: -2.4171 - acc: 0.01 - ETA: 3s - loss: -4.3384 - acc: 0.01 - ETA: 2s - loss: -4.3269 - acc: 0.01 - ETA: 2s - loss: -4.7456 - acc: 0.01 - ETA: 1s - loss: -4.2619 - acc: 0.01 - ETA: 1s - loss: -4.5535 - acc: 0.01 - ETA: 0s - loss: -4.1682 - acc: 0.01 - ETA: 0s - loss: -3.9431 - acc: 0.01 - 5s 6ms/step - loss: -4.0399 - acc: 0.0127\n",
      "Epoch 30/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -8.5149 - acc: 0.0000e+ - ETA: 3s - loss: -6.8436 - acc: 0.0050   - ETA: 3s - loss: -5.1648 - acc: 0.01 - ETA: 2s - loss: -4.3680 - acc: 0.01 - ETA: 2s - loss: -5.2355 - acc: 0.01 - ETA: 1s - loss: -22.7792 - acc: 0.011 - ETA: 1s - loss: -30.5894 - acc: 0.010 - ETA: 0s - loss: -27.0327 - acc: 0.012 - ETA: 0s - loss: -24.2767 - acc: 0.013 - 5s 6ms/step - loss: -25.8527 - acc: 0.0127\n",
      "Epoch 31/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.1131 - acc: 0.01 - ETA: 4s - loss: -3.0090 - acc: 0.01 - ETA: 3s - loss: -3.2144 - acc: 0.01 - ETA: 2s - loss: -5.5050 - acc: 0.01 - ETA: 2s - loss: -4.5880 - acc: 0.01 - ETA: 1s - loss: -4.2865 - acc: 0.01 - ETA: 1s - loss: -4.2571 - acc: 0.01 - ETA: 0s - loss: -4.0629 - acc: 0.01 - ETA: 0s - loss: -3.8147 - acc: 0.01 - 5s 6ms/step - loss: -3.7255 - acc: 0.0180\n",
      "Epoch 32/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -7.2520 - acc: 0.02 - ETA: 3s - loss: -5.2259 - acc: 0.02 - ETA: 3s - loss: -3.5359 - acc: 0.02 - ETA: 2s - loss: -3.3229 - acc: 0.02 - ETA: 2s - loss: -34.9371 - acc: 0.020 - ETA: 1s - loss: -29.4299 - acc: 0.018 - ETA: 1s - loss: -25.7813 - acc: 0.017 - ETA: 0s - loss: -22.8503 - acc: 0.017 - ETA: 0s - loss: -20.7443 - acc: 0.017 - 5s 5ms/step - loss: -19.7847 - acc: 0.0191\n",
      "Epoch 33/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.8989 - acc: 0.02 - ETA: 3s - loss: -2.6889 - acc: 0.02 - ETA: 3s - loss: -3.7064 - acc: 0.01 - ETA: 2s - loss: -4.5130 - acc: 0.01 - ETA: 2s - loss: -5.0194 - acc: 0.01 - ETA: 1s - loss: -4.2843 - acc: 0.01 - ETA: 1s - loss: -3.6579 - acc: 0.01 - ETA: 0s - loss: -3.1774 - acc: 0.01 - ETA: 0s - loss: -3.1312 - acc: 0.02 - 5s 5ms/step - loss: -3.2203 - acc: 0.0201\n",
      "Epoch 34/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.9751 - acc: 0.01 - ETA: 3s - loss: -5.2439 - acc: 0.01 - ETA: 3s - loss: -3.4539 - acc: 0.01 - ETA: 2s - loss: -3.2704 - acc: 0.01 - ETA: 2s - loss: -3.0099 - acc: 0.02 - ETA: 1s - loss: -2.7595 - acc: 0.02 - ETA: 1s - loss: -4.8299 - acc: 0.02 - ETA: 0s - loss: -4.7550 - acc: 0.02 - ETA: 0s - loss: -4.3866 - acc: 0.02 - 5s 6ms/step - loss: -4.2672 - acc: 0.0201\n",
      "Epoch 35/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.5796 - acc: 0.01 - ETA: 3s - loss: -9.7295 - acc: 0.01 - ETA: 3s - loss: -6.9403 - acc: 0.01 - ETA: 2s - loss: -7.9108 - acc: 0.01 - ETA: 2s - loss: -7.4523 - acc: 0.01 - ETA: 1s - loss: -6.2150 - acc: 0.01 - ETA: 1s - loss: -5.6824 - acc: 0.01 - ETA: 0s - loss: -5.2384 - acc: 0.01 - ETA: 0s - loss: -4.9872 - acc: 0.02 - 5s 6ms/step - loss: -5.3438 - acc: 0.0191\n",
      "Epoch 36/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.2135 - acc: 0.03 - ETA: 3s - loss: -15.2372 - acc: 0.015 - ETA: 3s - loss: -12.1715 - acc: 0.016 - ETA: 2s - loss: -9.5749 - acc: 0.015 - ETA: 2s - loss: -8.7926 - acc: 0.01 - ETA: 1s - loss: -7.5070 - acc: 0.01 - ETA: 1s - loss: -6.6461 - acc: 0.01 - ETA: 0s - loss: -6.0334 - acc: 0.01 - ETA: 0s - loss: -5.9046 - acc: 0.01 - 5s 5ms/step - loss: -5.9244 - acc: 0.0170\n",
      "Epoch 37/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -4.4333 - acc: 0.02 - ETA: 3s - loss: -3.9014 - acc: 0.02 - ETA: 3s - loss: -32.5291 - acc: 0.013 - ETA: 2s - loss: -26.8281 - acc: 0.010 - ETA: 2s - loss: -21.4554 - acc: 0.012 - ETA: 1s - loss: -18.1256 - acc: 0.015 - ETA: 1s - loss: -37.9917 - acc: 0.012 - ETA: 0s - loss: -33.4224 - acc: 0.013 - ETA: 0s - loss: -29.9369 - acc: 0.013 - 5s 6ms/step - loss: -28.6668 - acc: 0.0138\n",
      "Epoch 38/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2858 - acc: 0.02 - ETA: 3s - loss: -0.2684 - acc: 0.04 - ETA: 3s - loss: -1.0444 - acc: 0.03 - ETA: 2s - loss: -1.6699 - acc: 0.02 - ETA: 2s - loss: -25.8803 - acc: 0.022 - ETA: 1s - loss: -79.7069 - acc: 0.018 - ETA: 1s - loss: -71.9223 - acc: 0.015 - ETA: 0s - loss: -63.0185 - acc: 0.017 - ETA: 0s - loss: -56.6148 - acc: 0.016 - 5s 6ms/step - loss: -55.5169 - acc: 0.0159\n",
      "Epoch 39/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -15.1496 - acc: 0.0000e+0 - ETA: 3s - loss: -223.7301 - acc: 0.0000e+ - ETA: 3s - loss: -149.8890 - acc: 0.0033   - ETA: 2s - loss: -122.1642 - acc: 0.00 - ETA: 2s - loss: -99.0364 - acc: 0.0040 - ETA: 1s - loss: -82.6993 - acc: 0.010 - ETA: 1s - loss: -71.0194 - acc: 0.014 - ETA: 0s - loss: -62.1810 - acc: 0.016 - ETA: 0s - loss: -55.5163 - acc: 0.016 - 5s 6ms/step - loss: -53.0574 - acc: 0.0180\n",
      "Epoch 40/500\n",
      "943/943 [==============================] - ETA: 5s - loss: 0.1357 - acc: 0.040 - ETA: 4s - loss: -0.8818 - acc: 0.03 - ETA: 3s - loss: -0.5678 - acc: 0.02 - ETA: 3s - loss: -1.3607 - acc: 0.02 - ETA: 2s - loss: -1.4117 - acc: 0.02 - ETA: 1s - loss: -1.4274 - acc: 0.02 - ETA: 1s - loss: -41.2266 - acc: 0.021 - ETA: 0s - loss: -36.6557 - acc: 0.021 - ETA: 0s - loss: -40.3986 - acc: 0.018 - 5s 6ms/step - loss: -38.6666 - acc: 0.0191\n",
      "Epoch 41/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.9608 - acc: 0.01 - ETA: 3s - loss: -7.1916 - acc: 0.01 - ETA: 3s - loss: -5.2912 - acc: 0.01 - ETA: 2s - loss: -4.0844 - acc: 0.01 - ETA: 2s - loss: -3.8029 - acc: 0.01 - ETA: 1s - loss: -3.4524 - acc: 0.01 - ETA: 1s - loss: -3.1354 - acc: 0.01 - ETA: 0s - loss: -3.1093 - acc: 0.01 - ETA: 0s - loss: -2.8704 - acc: 0.02 - 5s 6ms/step - loss: -26.0831 - acc: 0.0191\n",
      "Epoch 42/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8772 - acc: 0.01 - ETA: 3s - loss: -1.7396 - acc: 0.02 - ETA: 3s - loss: -2.1016 - acc: 0.01 - ETA: 2s - loss: -1.8093 - acc: 0.02 - ETA: 2s - loss: -108.0231 - acc: 0.02 - ETA: 1s - loss: -90.0907 - acc: 0.0217 - ETA: 1s - loss: -77.7579 - acc: 0.020 - ETA: 0s - loss: -68.2143 - acc: 0.021 - ETA: 0s - loss: -60.9279 - acc: 0.021 - 5s 5ms/step - loss: -58.2243 - acc: 0.0212\n",
      "Epoch 43/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.2335 - acc: 0.02 - ETA: 3s - loss: -1.5607 - acc: 0.02 - ETA: 3s - loss: -7.6164 - acc: 0.01 - ETA: 2s - loss: -5.9397 - acc: 0.02 - ETA: 2s - loss: -4.9671 - acc: 0.03 - ETA: 1s - loss: -4.8672 - acc: 0.03 - ETA: 1s - loss: -42.4213 - acc: 0.025 - ETA: 0s - loss: -37.1979 - acc: 0.025 - ETA: 0s - loss: -35.2619 - acc: 0.022 - 5s 6ms/step - loss: -33.6856 - acc: 0.0223\n",
      "Epoch 44/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -101.5259 - acc: 0.0000e+ - ETA: 3s - loss: -51.2931 - acc: 0.0100     - ETA: 3s - loss: -34.5955 - acc: 0.016 - ETA: 2s - loss: -25.8493 - acc: 0.022 - ETA: 2s - loss: -20.9128 - acc: 0.030 - ETA: 1s - loss: -17.6860 - acc: 0.030 - ETA: 1s - loss: -30.8585 - acc: 0.025 - ETA: 0s - loss: -27.1701 - acc: 0.025 - ETA: 0s - loss: -24.5021 - acc: 0.023 - 5s 6ms/step - loss: -51.4647 - acc: 0.0223\n",
      "Epoch 45/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.1532 - acc: 0.02 - ETA: 3s - loss: -0.9575 - acc: 0.02 - ETA: 3s - loss: -0.7037 - acc: 0.03 - ETA: 2s - loss: -2.1680 - acc: 0.02 - ETA: 2s - loss: -1.8998 - acc: 0.02 - ETA: 1s - loss: -5.9557 - acc: 0.02 - ETA: 1s - loss: -5.2371 - acc: 0.02 - ETA: 0s - loss: -4.8088 - acc: 0.02 - ETA: 0s - loss: -4.6446 - acc: 0.02 - 5s 6ms/step - loss: -7.8893 - acc: 0.0223\n",
      "Epoch 46/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4378 - acc: 0.03 - ETA: 3s - loss: -1.1413 - acc: 0.04 - ETA: 3s - loss: -1.4248 - acc: 0.03 - ETA: 2s - loss: -1.3513 - acc: 0.03 - ETA: 2s - loss: -36.6142 - acc: 0.028 - ETA: 1s - loss: -30.6248 - acc: 0.026 - ETA: 1s - loss: -26.3956 - acc: 0.025 - ETA: 0s - loss: -23.3601 - acc: 0.026 - ETA: 0s - loss: -64.8825 - acc: 0.023 - 5s 6ms/step - loss: -61.9588 - acc: 0.0233\n",
      "Epoch 47/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1461 - acc: 0.03 - ETA: 3s - loss: -1.0481 - acc: 0.03 - ETA: 3s - loss: -1.2088 - acc: 0.02 - ETA: 2s - loss: -1.8005 - acc: 0.02 - ETA: 2s - loss: -1.5806 - acc: 0.02 - ETA: 1s - loss: -2.2508 - acc: 0.02 - ETA: 1s - loss: -1.9165 - acc: 0.02 - ETA: 0s - loss: -1.8460 - acc: 0.02 - ETA: 0s - loss: -2.2021 - acc: 0.02 - 5s 6ms/step - loss: -8.5480 - acc: 0.0223\n",
      "Epoch 48/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4362 - acc: 0.05 - ETA: 3s - loss: -2.5756 - acc: 0.03 - ETA: 3s - loss: -1.6222 - acc: 0.02 - ETA: 2s - loss: -1.5275 - acc: 0.03 - ETA: 2s - loss: -1.8513 - acc: 0.02 - ETA: 1s - loss: -2.1796 - acc: 0.02 - ETA: 1s - loss: -2.0153 - acc: 0.02 - ETA: 0s - loss: -2.0096 - acc: 0.02 - ETA: 0s - loss: -2.1409 - acc: 0.02 - 5s 5ms/step - loss: -42.3178 - acc: 0.0223\n",
      "Epoch 49/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6214 - acc: 0.02 - ETA: 3s - loss: -2.1812 - acc: 0.02 - ETA: 3s - loss: -51.9441 - acc: 0.013 - ETA: 2s - loss: -39.3287 - acc: 0.017 - ETA: 2s - loss: -31.8919 - acc: 0.018 - ETA: 1s - loss: -26.6886 - acc: 0.018 - ETA: 1s - loss: -23.5059 - acc: 0.018 - ETA: 0s - loss: -20.5666 - acc: 0.021 - ETA: 0s - loss: -18.5538 - acc: 0.021 - 5s 6ms/step - loss: -17.7347 - acc: 0.0223\n",
      "Epoch 50/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0237 - acc: 0.04 - ETA: 3s - loss: -0.2708 - acc: 0.03 - ETA: 3s - loss: -0.5955 - acc: 0.03 - ETA: 2s - loss: -1.2384 - acc: 0.03 - ETA: 2s - loss: -1.2610 - acc: 0.02 - ETA: 1s - loss: -1.4345 - acc: 0.02 - ETA: 1s - loss: -1.4122 - acc: 0.02 - ETA: 0s - loss: -1.3608 - acc: 0.02 - ETA: 0s - loss: -65.9791 - acc: 0.022 - 5s 5ms/step - loss: -63.1590 - acc: 0.0223\n",
      "Epoch 51/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1045 - acc: 0.04 - ETA: 3s - loss: -1.0590 - acc: 0.04 - ETA: 3s - loss: -1.4174 - acc: 0.03 - ETA: 2s - loss: -2.0660 - acc: 0.02 - ETA: 2s - loss: -1.6888 - acc: 0.02 - ETA: 1s - loss: -4.7084 - acc: 0.02 - ETA: 1s - loss: -4.2376 - acc: 0.02 - ETA: 0s - loss: -4.2259 - acc: 0.02 - ETA: 0s - loss: -3.8130 - acc: 0.02 - 5s 5ms/step - loss: -27.5449 - acc: 0.0223\n",
      "Epoch 52/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.1875 - acc: 0.030 - ETA: 3s - loss: -0.7817 - acc: 0.02 - ETA: 3s - loss: -2.4883 - acc: 0.02 - ETA: 2s - loss: -1.8459 - acc: 0.02 - ETA: 2s - loss: -2.0380 - acc: 0.02 - ETA: 1s - loss: -2.1309 - acc: 0.02 - ETA: 1s - loss: -2.0677 - acc: 0.02 - ETA: 0s - loss: -2.3732 - acc: 0.02 - ETA: 0s - loss: -2.1492 - acc: 0.02 - 5s 6ms/step - loss: -2.0785 - acc: 0.0233\n",
      "Epoch 53/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3466 - acc: 0.04 - ETA: 3s - loss: -0.6970 - acc: 0.03 - ETA: 3s - loss: -23.2005 - acc: 0.020 - ETA: 2s - loss: -18.4387 - acc: 0.020 - ETA: 2s - loss: -14.8144 - acc: 0.020 - ETA: 1s - loss: -12.4685 - acc: 0.021 - ETA: 1s - loss: -10.8091 - acc: 0.027 - ETA: 0s - loss: -9.7031 - acc: 0.025 - ETA: 0s - loss: -8.6236 - acc: 0.02 - 5s 5ms/step - loss: -53.2649 - acc: 0.0233\n",
      "Epoch 54/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.3563 - acc: 0.02 - ETA: 3s - loss: -3.4458 - acc: 0.01 - ETA: 3s - loss: -2.5571 - acc: 0.01 - ETA: 2s - loss: -2.3679 - acc: 0.02 - ETA: 2s - loss: -2.1787 - acc: 0.01 - ETA: 1s - loss: -1.8172 - acc: 0.02 - ETA: 1s - loss: -37.3750 - acc: 0.020 - ETA: 0s - loss: -32.7878 - acc: 0.023 - ETA: 0s - loss: -29.4924 - acc: 0.023 - 5s 5ms/step - loss: -28.1919 - acc: 0.0233\n",
      "Epoch 55/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7724 - acc: 0.02 - ETA: 3s - loss: -0.3722 - acc: 0.02 - ETA: 3s - loss: -0.5948 - acc: 0.02 - ETA: 2s - loss: -0.9834 - acc: 0.02 - ETA: 2s - loss: -0.9537 - acc: 0.02 - ETA: 1s - loss: -1.0953 - acc: 0.02 - ETA: 1s - loss: -1.2226 - acc: 0.02 - ETA: 0s - loss: -1.5588 - acc: 0.02 - ETA: 0s - loss: -1.7365 - acc: 0.02 - 5s 6ms/step - loss: -1.9818 - acc: 0.0223\n",
      "Epoch 56/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2442 - acc: 0.02 - ETA: 4s - loss: -6.9628 - acc: 0.01 - ETA: 3s - loss: -4.8345 - acc: 0.02 - ETA: 2s - loss: -3.7182 - acc: 0.02 - ETA: 2s - loss: -3.1852 - acc: 0.02 - ETA: 1s - loss: -2.9708 - acc: 0.02 - ETA: 1s - loss: -2.6656 - acc: 0.02 - ETA: 0s - loss: -2.5116 - acc: 0.02 - ETA: 0s - loss: -14.1148 - acc: 0.022 - 5s 6ms/step - loss: -13.5741 - acc: 0.0223\n",
      "Epoch 57/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3899 - acc: 0.03 - ETA: 3s - loss: -0.8711 - acc: 0.03 - ETA: 3s - loss: -6.6184 - acc: 0.02 - ETA: 2s - loss: -5.1662 - acc: 0.02 - ETA: 2s - loss: -4.8271 - acc: 0.02 - ETA: 1s - loss: -4.3499 - acc: 0.02 - ETA: 1s - loss: -6.1069 - acc: 0.02 - ETA: 0s - loss: -5.6744 - acc: 0.02 - ETA: 0s - loss: -5.2302 - acc: 0.02 - 5s 5ms/step - loss: -5.0246 - acc: 0.0223\n",
      "Epoch 58/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2075 - acc: 0.03 - ETA: 3s - loss: -0.5681 - acc: 0.02 - ETA: 3s - loss: -0.9787 - acc: 0.02 - ETA: 2s - loss: -1.0409 - acc: 0.02 - ETA: 2s - loss: -1.5663 - acc: 0.02 - ETA: 1s - loss: -48.0353 - acc: 0.021 - ETA: 1s - loss: -41.5552 - acc: 0.020 - ETA: 0s - loss: -36.3643 - acc: 0.021 - ETA: 0s - loss: -32.5382 - acc: 0.023 - 5s 6ms/step - loss: -67.9969 - acc: 0.0223\n",
      "Epoch 59/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -18.7168 - acc: 0.010 - ETA: 3s - loss: -9.8527 - acc: 0.015 - ETA: 3s - loss: -6.9487 - acc: 0.01 - ETA: 2s - loss: -5.8487 - acc: 0.01 - ETA: 2s - loss: -4.9322 - acc: 0.02 - ETA: 1s - loss: -4.1256 - acc: 0.02 - ETA: 1s - loss: -4.4443 - acc: 0.02 - ETA: 0s - loss: -4.0855 - acc: 0.02 - ETA: 0s - loss: -3.6340 - acc: 0.02 - 5s 5ms/step - loss: -37.3137 - acc: 0.0223\n",
      "Epoch 60/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9770 - acc: 0.02 - ETA: 3s - loss: -1.9494 - acc: 0.02 - ETA: 3s - loss: -1.5724 - acc: 0.02 - ETA: 2s - loss: -6.4814 - acc: 0.02 - ETA: 2s - loss: -5.1361 - acc: 0.02 - ETA: 1s - loss: -4.3887 - acc: 0.02 - ETA: 1s - loss: -5.2278 - acc: 0.02 - ETA: 0s - loss: -4.8785 - acc: 0.02 - ETA: 0s - loss: -4.4921 - acc: 0.02 - 5s 5ms/step - loss: -4.3033 - acc: 0.0233\n",
      "Epoch 61/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -7.3118 - acc: 0.01 - ETA: 3s - loss: -13.7086 - acc: 0.010 - ETA: 3s - loss: -16.9909 - acc: 0.010 - ETA: 2s - loss: -12.6395 - acc: 0.015 - ETA: 2s - loss: -10.9868 - acc: 0.016 - ETA: 1s - loss: -9.7228 - acc: 0.015 - ETA: 1s - loss: -8.3244 - acc: 0.02 - ETA: 0s - loss: -7.3120 - acc: 0.02 - ETA: 0s - loss: -6.6064 - acc: 0.02 - 5s 6ms/step - loss: -21.8749 - acc: 0.0233\n",
      "Epoch 62/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -192.6285 - acc: 0.0000e+ - ETA: 3s - loss: -96.8240 - acc: 0.0200     - ETA: 3s - loss: -65.4958 - acc: 0.026 - ETA: 2s - loss: -49.2680 - acc: 0.027 - ETA: 2s - loss: -39.8058 - acc: 0.024 - ETA: 1s - loss: -33.4627 - acc: 0.021 - ETA: 1s - loss: -29.2316 - acc: 0.020 - ETA: 0s - loss: -25.7594 - acc: 0.021 - ETA: 0s - loss: -22.9540 - acc: 0.023 - 5s 5ms/step - loss: -22.0900 - acc: 0.0233\n",
      "Epoch 63/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -15.7565 - acc: 0.0000e+0 - ETA: 3s - loss: -8.0822 - acc: 0.0250    - ETA: 3s - loss: -6.2435 - acc: 0.02 - ETA: 2s - loss: -5.7837 - acc: 0.02 - ETA: 2s - loss: -5.9928 - acc: 0.02 - ETA: 1s - loss: -5.0197 - acc: 0.02 - ETA: 1s - loss: -74.0254 - acc: 0.018 - ETA: 0s - loss: -64.8705 - acc: 0.020 - ETA: 0s - loss: -57.7220 - acc: 0.022 - 5s 5ms/step - loss: -55.3027 - acc: 0.0223\n",
      "Epoch 64/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -65.7470 - acc: 0.010 - ETA: 3s - loss: -33.5032 - acc: 0.020 - ETA: 3s - loss: -22.7206 - acc: 0.026 - ETA: 2s - loss: -17.3582 - acc: 0.030 - ETA: 2s - loss: -14.3619 - acc: 0.026 - ETA: 1s - loss: -12.4753 - acc: 0.025 - ETA: 1s - loss: -11.1882 - acc: 0.022 - ETA: 0s - loss: -9.8350 - acc: 0.023 - ETA: 0s - loss: -8.9003 - acc: 0.02 - 5s 5ms/step - loss: -49.8872 - acc: 0.0212\n",
      "Epoch 65/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8157 - acc: 0.02 - ETA: 3s - loss: -2.2365 - acc: 0.01 - ETA: 3s - loss: -1.8035 - acc: 0.01 - ETA: 2s - loss: -2.3639 - acc: 0.01 - ETA: 2s - loss: -2.2184 - acc: 0.01 - ETA: 1s - loss: -2.7427 - acc: 0.01 - ETA: 1s - loss: -2.4341 - acc: 0.02 - ETA: 0s - loss: -2.1813 - acc: 0.02 - ETA: 0s - loss: -2.0424 - acc: 0.02 - 5s 5ms/step - loss: -2.0152 - acc: 0.0201\n",
      "Epoch 66/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6241 - acc: 0.01 - ETA: 3s - loss: -2.4684 - acc: 0.02 - ETA: 3s - loss: -1.7212 - acc: 0.02 - ETA: 2s - loss: -2.2079 - acc: 0.02 - ETA: 2s - loss: -1.8752 - acc: 0.02 - ETA: 1s - loss: -1.6793 - acc: 0.02 - ETA: 1s - loss: -42.5964 - acc: 0.024 - ETA: 0s - loss: -37.6250 - acc: 0.022 - ETA: 0s - loss: -33.8747 - acc: 0.021 - 5s 6ms/step - loss: -34.4423 - acc: 0.0201\n",
      "Epoch 67/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4006 - acc: 0.03 - ETA: 3s - loss: -0.7486 - acc: 0.02 - ETA: 3s - loss: -0.8620 - acc: 0.03 - ETA: 2s - loss: -2.9528 - acc: 0.02 - ETA: 2s - loss: -2.7612 - acc: 0.02 - ETA: 1s - loss: -2.4606 - acc: 0.02 - ETA: 1s - loss: -2.5118 - acc: 0.02 - ETA: 0s - loss: -108.9390 - acc: 0.02 - ETA: 0s - loss: -100.7217 - acc: 0.02 - 5s 5ms/step - loss: -96.1926 - acc: 0.0201\n",
      "Epoch 68/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -332.2682 - acc: 0.0000e+ - ETA: 3s - loss: -167.0819 - acc: 0.0150   - ETA: 3s - loss: -111.5367 - acc: 0.02 - ETA: 2s - loss: -83.8951 - acc: 0.0200 - ETA: 2s - loss: -67.3607 - acc: 0.022 - ETA: 1s - loss: -58.1196 - acc: 0.020 - ETA: 1s - loss: -50.0929 - acc: 0.020 - ETA: 0s - loss: -43.8746 - acc: 0.021 - ETA: 0s - loss: -119.0998 - acc: 0.01 - 5s 5ms/step - loss: -113.6642 - acc: 0.0201\n",
      "Epoch 69/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -872.9194 - acc: 0.0000e+ - ETA: 3s - loss: -436.9279 - acc: 0.0150   - ETA: 3s - loss: -291.6824 - acc: 0.01 - ETA: 2s - loss: -219.0897 - acc: 0.01 - ETA: 2s - loss: -176.3892 - acc: 0.01 - ETA: 1s - loss: -288.7867 - acc: 0.01 - ETA: 1s - loss: -247.7580 - acc: 0.01 - ETA: 0s - loss: -216.8746 - acc: 0.01 - ETA: 0s - loss: -192.8377 - acc: 0.02 - 5s 5ms/step - loss: -184.0775 - acc: 0.0201\n",
      "Epoch 70/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2296 - acc: 0.03 - ETA: 3s - loss: -7.2981 - acc: 0.02 - ETA: 3s - loss: -5.3580 - acc: 0.02 - ETA: 2s - loss: -4.5672 - acc: 0.02 - ETA: 2s - loss: -3.8199 - acc: 0.02 - ETA: 1s - loss: -3.9017 - acc: 0.02 - ETA: 1s - loss: -3.8754 - acc: 0.02 - ETA: 0s - loss: -5.8420 - acc: 0.02 - ETA: 0s - loss: -6.1194 - acc: 0.01 - 5s 5ms/step - loss: -5.8362 - acc: 0.0201\n",
      "Epoch 71/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -29.7579 - acc: 0.0000e+0 - ETA: 4s - loss: -16.9525 - acc: 0.0100    - ETA: 3s - loss: -11.3270 - acc: 0.020 - ETA: 2s - loss: -9.5232 - acc: 0.020 - ETA: 2s - loss: -7.7150 - acc: 0.02 - ETA: 1s - loss: -6.8196 - acc: 0.02 - ETA: 1s - loss: -6.7746 - acc: 0.02 - ETA: 0s - loss: -5.9880 - acc: 0.02 - ETA: 0s - loss: -5.4752 - acc: 0.02 - 5s 6ms/step - loss: -5.2681 - acc: 0.0201\n",
      "Epoch 72/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.8151 - acc: 0.02 - ETA: 3s - loss: -1.4816 - acc: 0.03 - ETA: 3s - loss: -2.2801 - acc: 0.02 - ETA: 2s - loss: -1.8338 - acc: 0.03 - ETA: 2s - loss: -2.4637 - acc: 0.02 - ETA: 1s - loss: -2.1629 - acc: 0.02 - ETA: 1s - loss: -2.0067 - acc: 0.02 - ETA: 0s - loss: -2.1802 - acc: 0.02 - ETA: 0s - loss: -103.8624 - acc: 0.02 - 5s 5ms/step - loss: -99.4501 - acc: 0.0201\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -0.8304 - acc: 0.02 - ETA: 3s - loss: -2.3445 - acc: 0.02 - ETA: 3s - loss: -2.3638 - acc: 0.02 - ETA: 2s - loss: -2.0767 - acc: 0.02 - ETA: 2s - loss: -1.8877 - acc: 0.02 - ETA: 1s - loss: -1.6913 - acc: 0.02 - ETA: 1s - loss: -1.7285 - acc: 0.02 - ETA: 0s - loss: -2.2685 - acc: 0.01 - ETA: 0s - loss: -2.0647 - acc: 0.02 - 5s 5ms/step - loss: -49.3989 - acc: 0.0201\n",
      "Epoch 74/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6232 - acc: 0.02 - ETA: 3s - loss: -1.2424 - acc: 0.01 - ETA: 3s - loss: -1.6487 - acc: 0.02 - ETA: 2s - loss: -156.4290 - acc: 0.01 - ETA: 2s - loss: -125.2762 - acc: 0.02 - ETA: 1s - loss: -105.0174 - acc: 0.02 - ETA: 1s - loss: -90.4812 - acc: 0.0214 - ETA: 0s - loss: -79.4582 - acc: 0.020 - ETA: 0s - loss: -177.4953 - acc: 0.01 - 5s 5ms/step - loss: -169.3774 - acc: 0.0201\n",
      "Epoch 75/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8462 - acc: 0.02 - ETA: 3s - loss: -51.4466 - acc: 0.010 - ETA: 3s - loss: -34.2731 - acc: 0.016 - ETA: 2s - loss: -25.9492 - acc: 0.020 - ETA: 2s - loss: -20.9758 - acc: 0.024 - ETA: 1s - loss: -17.6646 - acc: 0.023 - ETA: 1s - loss: -19.1704 - acc: 0.020 - ETA: 0s - loss: -17.0162 - acc: 0.020 - ETA: 0s - loss: -15.3569 - acc: 0.021 - 5s 5ms/step - loss: -38.6671 - acc: 0.0201\n",
      "Epoch 76/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3536 - acc: 0.03 - ETA: 3s - loss: -12.0915 - acc: 0.020 - ETA: 3s - loss: -7.9845 - acc: 0.023 - ETA: 2s - loss: -7.1676 - acc: 0.02 - ETA: 2s - loss: -15.0460 - acc: 0.018 - ETA: 1s - loss: -12.8752 - acc: 0.020 - ETA: 1s - loss: -11.1838 - acc: 0.018 - ETA: 0s - loss: -9.9305 - acc: 0.018 - ETA: 0s - loss: -8.8535 - acc: 0.01 - 5s 5ms/step - loss: -8.4586 - acc: 0.0201\n",
      "Epoch 77/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -187.7766 - acc: 0.0000e+ - ETA: 3s - loss: -94.7656 - acc: 0.0050     - ETA: 3s - loss: -63.7827 - acc: 0.006 - ETA: 2s - loss: -47.8544 - acc: 0.015 - ETA: 2s - loss: -38.7933 - acc: 0.016 - ETA: 1s - loss: -32.3063 - acc: 0.018 - ETA: 1s - loss: -27.9952 - acc: 0.018 - ETA: 0s - loss: -33.2868 - acc: 0.016 - ETA: 0s - loss: -29.6093 - acc: 0.020 - 5s 6ms/step - loss: -28.3446 - acc: 0.0201\n",
      "Epoch 78/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.1837 - acc: 0.040 - ETA: 3s - loss: -0.2306 - acc: 0.03 - ETA: 3s - loss: -0.1845 - acc: 0.03 - ETA: 2s - loss: -0.8461 - acc: 0.03 - ETA: 2s - loss: -4.9720 - acc: 0.02 - ETA: 1s - loss: -4.6230 - acc: 0.02 - ETA: 1s - loss: -15.5988 - acc: 0.021 - ETA: 0s - loss: -24.4589 - acc: 0.018 - ETA: 0s - loss: -21.8450 - acc: 0.021 - 5s 5ms/step - loss: -77.5837 - acc: 0.0201\n",
      "Epoch 79/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2068 - acc: 0.04 - ETA: 3s - loss: -1.2839 - acc: 0.03 - ETA: 3s - loss: -46.0459 - acc: 0.023 - ETA: 2s - loss: -34.7876 - acc: 0.022 - ETA: 2s - loss: -28.0722 - acc: 0.024 - ETA: 1s - loss: -23.6816 - acc: 0.023 - ETA: 1s - loss: -21.0473 - acc: 0.021 - ETA: 0s - loss: -18.5188 - acc: 0.021 - ETA: 0s - loss: -16.5995 - acc: 0.021 - 5s 5ms/step - loss: -55.6937 - acc: 0.0201\n",
      "Epoch 80/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5171 - acc: 0.03 - ETA: 3s - loss: -0.3778 - acc: 0.04 - ETA: 3s - loss: -0.8591 - acc: 0.03 - ETA: 2s - loss: -145.8462 - acc: 0.02 - ETA: 2s - loss: -120.9726 - acc: 0.02 - ETA: 1s - loss: -197.8278 - acc: 0.01 - ETA: 1s - loss: -169.6804 - acc: 0.01 - ETA: 0s - loss: -148.6001 - acc: 0.01 - ETA: 0s - loss: -132.2895 - acc: 0.02 - 5s 6ms/step - loss: -126.2903 - acc: 0.0201\n",
      "Epoch 81/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.4290 - acc: 0.020 - ETA: 3s - loss: -630.5569 - acc: 0.01 - ETA: 3s - loss: -420.6155 - acc: 0.01 - ETA: 2s - loss: -316.4112 - acc: 0.01 - ETA: 2s - loss: -253.5338 - acc: 0.01 - ETA: 1s - loss: -211.3299 - acc: 0.02 - ETA: 1s - loss: -181.3258 - acc: 0.02 - ETA: 0s - loss: -158.8884 - acc: 0.02 - ETA: 0s - loss: -141.3919 - acc: 0.02 - 5s 5ms/step - loss: -153.8951 - acc: 0.0201\n",
      "Epoch 82/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7707 - acc: 0.02 - ETA: 3s - loss: -509.8636 - acc: 0.01 - ETA: 3s - loss: -342.2770 - acc: 0.01 - ETA: 2s - loss: -256.9362 - acc: 0.01 - ETA: 2s - loss: -205.6330 - acc: 0.01 - ETA: 1s - loss: -220.8952 - acc: 0.01 - ETA: 1s - loss: -189.4639 - acc: 0.01 - ETA: 0s - loss: -166.1331 - acc: 0.01 - ETA: 0s - loss: -148.0491 - acc: 0.01 - 5s 5ms/step - loss: -141.2781 - acc: 0.0201\n",
      "Epoch 83/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.9698 - acc: 0.02 - ETA: 3s - loss: -4.3038 - acc: 0.01 - ETA: 3s - loss: -4.8963 - acc: 0.01 - ETA: 2s - loss: -3.6768 - acc: 0.02 - ETA: 2s - loss: -4.2240 - acc: 0.02 - ETA: 1s - loss: -3.6383 - acc: 0.02 - ETA: 1s - loss: -3.3550 - acc: 0.02 - ETA: 0s - loss: -3.1280 - acc: 0.02 - ETA: 0s - loss: -95.2812 - acc: 0.021 - 5s 6ms/step - loss: -114.2510 - acc: 0.0201\n",
      "Epoch 84/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8988 - acc: 0.02 - ETA: 3s - loss: -0.6960 - acc: 0.02 - ETA: 3s - loss: -0.8956 - acc: 0.02 - ETA: 2s - loss: -1.0336 - acc: 0.02 - ETA: 2s - loss: -161.9733 - acc: 0.01 - ETA: 1s - loss: -135.7324 - acc: 0.01 - ETA: 1s - loss: -117.0405 - acc: 0.01 - ETA: 0s - loss: -102.4984 - acc: 0.01 - ETA: 0s - loss: -91.3930 - acc: 0.0189 - 5s 5ms/step - loss: -87.2384 - acc: 0.0201\n",
      "Epoch 85/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.7463 - acc: 0.02 - ETA: 3s - loss: -4.8383 - acc: 0.01 - ETA: 3s - loss: -4.5584 - acc: 0.01 - ETA: 2s - loss: -4.0113 - acc: 0.01 - ETA: 2s - loss: -3.4731 - acc: 0.01 - ETA: 1s - loss: -2.8532 - acc: 0.02 - ETA: 1s - loss: -2.6712 - acc: 0.02 - ETA: 0s - loss: -2.5471 - acc: 0.02 - ETA: 0s - loss: -30.2858 - acc: 0.020 - 6s 6ms/step - loss: -28.8835 - acc: 0.0201\n",
      "Epoch 86/500\n",
      "943/943 [==============================] - ETA: 7s - loss: -434.3448 - acc: 0.0000e+ - ETA: 5s - loss: -217.4678 - acc: 0.0100   - ETA: 4s - loss: -325.8643 - acc: 0.00 - ETA: 3s - loss: -244.3729 - acc: 0.01 - ETA: 2s - loss: -197.0747 - acc: 0.01 - ETA: 2s - loss: -165.0338 - acc: 0.01 - ETA: 1s - loss: -141.5697 - acc: 0.01 - ETA: 0s - loss: -123.9270 - acc: 0.01 - ETA: 0s - loss: -110.2910 - acc: 0.02 - 6s 6ms/step - loss: -105.2582 - acc: 0.0201\n",
      "Epoch 87/500\n",
      "943/943 [==============================] - ETA: 5s - loss: -3.1829 - acc: 0.01 - ETA: 4s - loss: -2.4885 - acc: 0.02 - ETA: 3s - loss: -8.4756 - acc: 0.01 - ETA: 3s - loss: -8.0250 - acc: 0.01 - ETA: 2s - loss: -7.1016 - acc: 0.01 - ETA: 2s - loss: -6.0050 - acc: 0.01 - ETA: 1s - loss: -5.1034 - acc: 0.01 - ETA: 0s - loss: -5.1255 - acc: 0.01 - ETA: 0s - loss: -66.8479 - acc: 0.016 - 6s 6ms/step - loss: -63.7737 - acc: 0.0201\n",
      "Epoch 88/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6716 - acc: 0.03 - ETA: 3s - loss: -0.4885 - acc: 0.04 - ETA: 3s - loss: -1.8766 - acc: 0.03 - ETA: 2s - loss: -1.6093 - acc: 0.03 - ETA: 2s - loss: -154.3105 - acc: 0.02 - ETA: 1s - loss: -128.6315 - acc: 0.02 - ETA: 1s - loss: -207.0953 - acc: 0.02 - ETA: 0s - loss: -265.9592 - acc: 0.02 - ETA: 0s - loss: -236.7466 - acc: 0.02 - 5s 6ms/step - loss: -226.0388 - acc: 0.0201\n",
      "Epoch 89/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8215 - acc: 0.01 - ETA: 4s - loss: -1.7515 - acc: 0.01 - ETA: 3s - loss: -1.5854 - acc: 0.02 - ETA: 3s - loss: -1.7067 - acc: 0.02 - ETA: 2s - loss: -1.4427 - acc: 0.02 - ETA: 1s - loss: -1.3918 - acc: 0.02 - ETA: 1s - loss: -1.4476 - acc: 0.02 - ETA: 0s - loss: -1.6102 - acc: 0.02 - ETA: 0s - loss: -35.2315 - acc: 0.020 - 6s 6ms/step - loss: -33.8695 - acc: 0.0201\n",
      "Epoch 90/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.3067 - acc: 0.01 - ETA: 3s - loss: -4.1151 - acc: 0.01 - ETA: 3s - loss: -2.8473 - acc: 0.02 - ETA: 2s - loss: -2.1910 - acc: 0.02 - ETA: 2s - loss: -1.8818 - acc: 0.02 - ETA: 1s - loss: -1.8070 - acc: 0.02 - ETA: 1s - loss: -1.6644 - acc: 0.02 - ETA: 0s - loss: -1.7595 - acc: 0.02 - ETA: 0s - loss: -77.2199 - acc: 0.021 - 5s 5ms/step - loss: -151.5727 - acc: 0.0201\n",
      "Epoch 91/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3858 - acc: 0.03 - ETA: 3s - loss: -0.7262 - acc: 0.03 - ETA: 3s - loss: -1.1808 - acc: 0.02 - ETA: 2s - loss: -1.3562 - acc: 0.02 - ETA: 2s - loss: -1.4580 - acc: 0.02 - ETA: 1s - loss: -108.7368 - acc: 0.02 - ETA: 1s - loss: -93.1838 - acc: 0.0214 - ETA: 0s - loss: -82.2543 - acc: 0.020 - ETA: 0s - loss: -73.2321 - acc: 0.021 - 5s 6ms/step - loss: -107.6837 - acc: 0.0201\n",
      "Epoch 92/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.2197 - acc: 0.03 - ETA: 3s - loss: -2.1650 - acc: 0.02 - ETA: 3s - loss: -150.4227 - acc: 0.01 - ETA: 2s - loss: -113.1530 - acc: 0.02 - ETA: 2s - loss: -91.1123 - acc: 0.0200 - ETA: 1s - loss: -76.2318 - acc: 0.018 - ETA: 1s - loss: -65.4165 - acc: 0.022 - ETA: 0s - loss: -57.2536 - acc: 0.021 - ETA: 0s - loss: -50.9878 - acc: 0.021 - 5s 6ms/step - loss: -115.7240 - acc: 0.0201\n",
      "Epoch 93/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.4167 - acc: 0.050 - ETA: 4s - loss: -445.1611 - acc: 0.02 - ETA: 3s - loss: -297.5201 - acc: 0.02 - ETA: 3s - loss: -224.0542 - acc: 0.02 - ETA: 2s - loss: -180.0869 - acc: 0.01 - ETA: 1s - loss: -150.3623 - acc: 0.01 - ETA: 1s - loss: -129.1446 - acc: 0.01 - ETA: 0s - loss: -113.2262 - acc: 0.01 - ETA: 0s - loss: -100.8096 - acc: 0.01 - 6s 6ms/step - loss: -96.2408 - acc: 0.0201\n",
      "Epoch 94/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4693 - acc: 0.03 - ETA: 4s - loss: -1.1786 - acc: 0.02 - ETA: 3s - loss: -1.8045 - acc: 0.02 - ETA: 3s - loss: -1.5702 - acc: 0.02 - ETA: 2s - loss: -1.7753 - acc: 0.02 - ETA: 2s - loss: -1.8675 - acc: 0.02 - ETA: 1s - loss: -2.0021 - acc: 0.02 - ETA: 0s - loss: -1.9858 - acc: 0.02 - ETA: 0s - loss: -1.7464 - acc: 0.02 - 6s 6ms/step - loss: -49.4087 - acc: 0.0201\n",
      "Epoch 95/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.3041 - acc: 0.030 - ETA: 4s - loss: 0.0095 - acc: 0.035 - ETA: 3s - loss: -178.7140 - acc: 0.02 - ETA: 3s - loss: -135.2766 - acc: 0.02 - ETA: 2s - loss: -108.9470 - acc: 0.02 - ETA: 2s - loss: -90.9817 - acc: 0.0200 - ETA: 1s - loss: -78.1208 - acc: 0.020 - ETA: 0s - loss: -68.4993 - acc: 0.020 - ETA: 0s - loss: -60.9779 - acc: 0.020 - 6s 6ms/step - loss: -58.2304 - acc: 0.0201\n",
      "Epoch 96/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.0162 - acc: 0.02 - ETA: 3s - loss: -2.8449 - acc: 0.01 - ETA: 3s - loss: -2.3523 - acc: 0.01 - ETA: 2s - loss: -197.2550 - acc: 0.01 - ETA: 2s - loss: -158.0847 - acc: 0.01 - ETA: 1s - loss: -131.7556 - acc: 0.01 - ETA: 1s - loss: -112.9522 - acc: 0.02 - ETA: 0s - loss: -99.2890 - acc: 0.0200 - ETA: 0s - loss: -88.5659 - acc: 0.018 - 5s 6ms/step - loss: -84.5915 - acc: 0.0201\n",
      "Epoch 97/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0892 - acc: 0.05 - ETA: 3s - loss: -652.2841 - acc: 0.02 - ETA: 3s - loss: -435.7459 - acc: 0.02 - ETA: 2s - loss: -327.2537 - acc: 0.02 - ETA: 2s - loss: -262.1241 - acc: 0.02 - ETA: 1s - loss: -352.9316 - acc: 0.01 - ETA: 1s - loss: -302.5998 - acc: 0.02 - ETA: 0s - loss: -264.9162 - acc: 0.02 - ETA: 0s - loss: -235.5464 - acc: 0.02 - 5s 6ms/step - loss: -224.7995 - acc: 0.0201\n",
      "Epoch 98/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.7523 - acc: 0.02 - ETA: 4s - loss: -3.4651 - acc: 0.01 - ETA: 4s - loss: -435.2456 - acc: 0.01 - ETA: 3s - loss: -326.4904 - acc: 0.01 - ETA: 2s - loss: -261.3257 - acc: 0.01 - ETA: 2s - loss: -218.1441 - acc: 0.01 - ETA: 1s - loss: -187.0119 - acc: 0.02 - ETA: 0s - loss: -285.8826 - acc: 0.01 - ETA: 0s - loss: -254.6644 - acc: 0.01 - 6s 6ms/step - loss: -243.0212 - acc: 0.0201\n",
      "Epoch 99/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1469 - acc: 0.02 - ETA: 3s - loss: -1.4222 - acc: 0.02 - ETA: 3s - loss: -1.2356 - acc: 0.02 - ETA: 2s - loss: -1.0695 - acc: 0.02 - ETA: 2s - loss: -2.0515 - acc: 0.02 - ETA: 1s - loss: -1.9374 - acc: 0.02 - ETA: 1s - loss: -1.7589 - acc: 0.02 - ETA: 0s - loss: -1.6213 - acc: 0.02 - ETA: 0s - loss: -1.7704 - acc: 0.02 - 5s 6ms/step - loss: -45.2698 - acc: 0.0201\n",
      "Epoch 100/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.8208 - acc: 0.01 - ETA: 3s - loss: -3.0476 - acc: 0.01 - ETA: 3s - loss: -2.6197 - acc: 0.01 - ETA: 2s - loss: -2.8898 - acc: 0.01 - ETA: 2s - loss: -2.7110 - acc: 0.01 - ETA: 1s - loss: -2.5159 - acc: 0.01 - ETA: 1s - loss: -2.4762 - acc: 0.01 - ETA: 0s - loss: -2.1586 - acc: 0.02 - ETA: 0s - loss: -1.9270 - acc: 0.02 - 5s 5ms/step - loss: -37.5658 - acc: 0.0201\n",
      "Epoch 101/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7195 - acc: 0.03 - ETA: 3s - loss: -3.2646 - acc: 0.02 - ETA: 3s - loss: -2.2662 - acc: 0.02 - ETA: 2s - loss: -220.4296 - acc: 0.02 - ETA: 2s - loss: -177.7727 - acc: 0.01 - ETA: 1s - loss: -148.1635 - acc: 0.02 - ETA: 1s - loss: -127.0186 - acc: 0.02 - ETA: 0s - loss: -111.8982 - acc: 0.02 - ETA: 0s - loss: -99.5121 - acc: 0.0211 - 5s 5ms/step - loss: -127.7960 - acc: 0.0201\n",
      "Epoch 102/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1636 - acc: 0.01 - ETA: 3s - loss: -3.2283 - acc: 0.01 - ETA: 3s - loss: -3.0485 - acc: 0.01 - ETA: 2s - loss: -3.0968 - acc: 0.01 - ETA: 2s - loss: -2.6282 - acc: 0.01 - ETA: 1s - loss: -2.2682 - acc: 0.02 - ETA: 1s - loss: -2.0320 - acc: 0.02 - ETA: 0s - loss: -1.8873 - acc: 0.02 - ETA: 0s - loss: -1.8936 - acc: 0.02 - 5s 6ms/step - loss: -44.9055 - acc: 0.0201\n",
      "Epoch 103/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5016 - acc: 0.05 - ETA: 3s - loss: -0.5961 - acc: 0.03 - ETA: 3s - loss: -0.6630 - acc: 0.03 - ETA: 2s - loss: -0.8083 - acc: 0.03 - ETA: 2s - loss: -0.7516 - acc: 0.03 - ETA: 1s - loss: -1.0015 - acc: 0.02 - ETA: 1s - loss: -1.0656 - acc: 0.02 - ETA: 0s - loss: -56.5535 - acc: 0.023 - ETA: 0s - loss: -202.1042 - acc: 0.02 - 5s 5ms/step - loss: -216.2017 - acc: 0.0201\n",
      "Epoch 104/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.8772 - acc: 0.01 - ETA: 3s - loss: -543.0513 - acc: 0.00 - ETA: 3s - loss: -362.6942 - acc: 0.01 - ETA: 2s - loss: -273.1168 - acc: 0.01 - ETA: 2s - loss: -218.9294 - acc: 0.01 - ETA: 1s - loss: -182.4443 - acc: 0.01 - ETA: 1s - loss: -156.4743 - acc: 0.01 - ETA: 0s - loss: -136.8875 - acc: 0.01 - ETA: 0s - loss: -121.7998 - acc: 0.02 - 5s 5ms/step - loss: -152.6046 - acc: 0.0201\n",
      "Epoch 105/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1943 - acc: 0.04 - ETA: 3s - loss: -2.1701 - acc: 0.02 - ETA: 3s - loss: -331.1156 - acc: 0.01 - ETA: 2s - loss: -249.2516 - acc: 0.01 - ETA: 2s - loss: -199.4235 - acc: 0.02 - ETA: 1s - loss: -166.3894 - acc: 0.02 - ETA: 1s - loss: -223.9222 - acc: 0.02 - ETA: 0s - loss: -196.1346 - acc: 0.02 - ETA: 0s - loss: -174.6647 - acc: 0.02 - 5s 5ms/step - loss: -166.8930 - acc: 0.0201\n",
      "Epoch 106/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -556.1201 - acc: 0.0000e+ - ETA: 3s - loss: -278.2847 - acc: 0.0150   - ETA: 3s - loss: -185.8253 - acc: 0.02 - ETA: 2s - loss: -140.3424 - acc: 0.01 - ETA: 2s - loss: -112.4099 - acc: 0.02 - ETA: 1s - loss: -93.6436 - acc: 0.0217 - ETA: 1s - loss: -80.4660 - acc: 0.022 - ETA: 0s - loss: -226.4651 - acc: 0.02 - ETA: 0s - loss: -201.4535 - acc: 0.02 - 5s 5ms/step - loss: -192.7386 - acc: 0.0201\n",
      "Epoch 107/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -24.0369 - acc: 0.010 - ETA: 4s - loss: -12.0761 - acc: 0.030 - ETA: 3s - loss: -8.5914 - acc: 0.030 - ETA: 2s - loss: -6.8554 - acc: 0.02 - ETA: 2s - loss: -6.5648 - acc: 0.02 - ETA: 1s - loss: -5.5935 - acc: 0.02 - ETA: 1s - loss: -5.5468 - acc: 0.02 - ETA: 0s - loss: -4.9348 - acc: 0.02 - ETA: 0s - loss: -4.7383 - acc: 0.01 - 5s 5ms/step - loss: -4.5601 - acc: 0.0201\n",
      "Epoch 108/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.0870 - acc: 0.030 - ETA: 3s - loss: -0.5466 - acc: 0.02 - ETA: 3s - loss: -2.8365 - acc: 0.02 - ETA: 2s - loss: -2.2245 - acc: 0.02 - ETA: 2s - loss: -4.8288 - acc: 0.02 - ETA: 1s - loss: -171.8919 - acc: 0.02 - ETA: 1s - loss: -147.4812 - acc: 0.02 - ETA: 0s - loss: -129.1933 - acc: 0.02 - ETA: 0s - loss: -115.6238 - acc: 0.02 - 5s 5ms/step - loss: -110.5164 - acc: 0.0201\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -5.3292 - acc: 0.01 - ETA: 3s - loss: -4.2308 - acc: 0.01 - ETA: 3s - loss: -306.4267 - acc: 0.00 - ETA: 2s - loss: -524.3733 - acc: 0.00 - ETA: 2s - loss: -420.2204 - acc: 0.00 - ETA: 1s - loss: -350.3206 - acc: 0.01 - ETA: 1s - loss: -300.4397 - acc: 0.01 - ETA: 0s - loss: -262.9224 - acc: 0.01 - ETA: 0s - loss: -233.7689 - acc: 0.01 - 5s 5ms/step - loss: -223.1241 - acc: 0.0201\n",
      "Epoch 110/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4199 - acc: 0.03 - ETA: 3s - loss: -1.5433 - acc: 0.02 - ETA: 3s - loss: -1.3567 - acc: 0.02 - ETA: 2s - loss: -1.7341 - acc: 0.02 - ETA: 2s - loss: -209.9672 - acc: 0.02 - ETA: 1s - loss: -175.0121 - acc: 0.02 - ETA: 1s - loss: -277.2480 - acc: 0.01 - ETA: 0s - loss: -242.9096 - acc: 0.01 - ETA: 0s - loss: -215.9275 - acc: 0.02 - 5s 5ms/step - loss: -266.4764 - acc: 0.0201\n",
      "Epoch 111/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7851 - acc: 0.03 - ETA: 3s - loss: -1.0848 - acc: 0.03 - ETA: 3s - loss: -1.3649 - acc: 0.02 - ETA: 2s - loss: -1.5256 - acc: 0.02 - ETA: 2s - loss: -1.5811 - acc: 0.02 - ETA: 1s - loss: -1.5223 - acc: 0.02 - ETA: 1s - loss: -1.6407 - acc: 0.02 - ETA: 0s - loss: -1.6780 - acc: 0.02 - ETA: 0s - loss: -132.4381 - acc: 0.02 - 5s 5ms/step - loss: -126.3992 - acc: 0.0201\n",
      "Epoch 112/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.1533 - acc: 0.020 - ETA: 3s - loss: -0.4003 - acc: 0.02 - ETA: 3s - loss: -0.8403 - acc: 0.02 - ETA: 2s - loss: -311.0681 - acc: 0.01 - ETA: 2s - loss: -248.8539 - acc: 0.02 - ETA: 1s - loss: -208.5995 - acc: 0.02 - ETA: 1s - loss: -178.9891 - acc: 0.02 - ETA: 0s - loss: -319.2183 - acc: 0.02 - ETA: 0s - loss: -284.2021 - acc: 0.02 - 5s 5ms/step - loss: -308.5731 - acc: 0.0201\n",
      "Epoch 113/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.1066 - acc: 0.01 - ETA: 3s - loss: -2.3638 - acc: 0.03 - ETA: 3s - loss: -2.9705 - acc: 0.02 - ETA: 2s - loss: -2.5499 - acc: 0.02 - ETA: 2s - loss: -2.6729 - acc: 0.02 - ETA: 1s - loss: -2.4568 - acc: 0.02 - ETA: 1s - loss: -73.1562 - acc: 0.021 - ETA: 0s - loss: -64.2111 - acc: 0.021 - ETA: 0s - loss: -57.1513 - acc: 0.021 - 5s 5ms/step - loss: -99.4247 - acc: 0.0201\n",
      "Epoch 114/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7547 - acc: 0.03 - ETA: 3s - loss: -0.9611 - acc: 0.03 - ETA: 3s - loss: -0.6544 - acc: 0.02 - ETA: 2s - loss: -3.5679 - acc: 0.02 - ETA: 2s - loss: -3.3227 - acc: 0.02 - ETA: 1s - loss: -2.7791 - acc: 0.02 - ETA: 1s - loss: -2.5479 - acc: 0.02 - ETA: 0s - loss: -2.3462 - acc: 0.02 - ETA: 0s - loss: -169.3331 - acc: 0.01 - 5s 5ms/step - loss: -161.6840 - acc: 0.0201\n",
      "Epoch 115/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.3304 - acc: 0.01 - ETA: 3s - loss: -3.4292 - acc: 0.02 - ETA: 3s - loss: -3.0339 - acc: 0.01 - ETA: 2s - loss: -3.6078 - acc: 0.01 - ETA: 2s - loss: -3.6114 - acc: 0.01 - ETA: 1s - loss: -2.9551 - acc: 0.01 - ETA: 1s - loss: -3.0975 - acc: 0.01 - ETA: 0s - loss: -2.6726 - acc: 0.01 - ETA: 0s - loss: -2.7880 - acc: 0.01 - 5s 5ms/step - loss: -2.6954 - acc: 0.0201\n",
      "Epoch 116/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0269 - acc: 0.03 - ETA: 3s - loss: -0.6260 - acc: 0.03 - ETA: 3s - loss: -212.9306 - acc: 0.02 - ETA: 3s - loss: -160.0876 - acc: 0.02 - ETA: 2s - loss: -128.3559 - acc: 0.02 - ETA: 1s - loss: -315.3985 - acc: 0.02 - ETA: 1s - loss: -411.7718 - acc: 0.01 - ETA: 0s - loss: -360.5387 - acc: 0.01 - ETA: 0s - loss: -320.5373 - acc: 0.01 - 6s 6ms/step - loss: -305.9308 - acc: 0.0201\n",
      "Epoch 117/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0928 - acc: 0.02 - ETA: 3s - loss: -0.9741 - acc: 0.02 - ETA: 3s - loss: -0.9708 - acc: 0.02 - ETA: 2s - loss: -1.2054 - acc: 0.02 - ETA: 2s - loss: -1.0675 - acc: 0.02 - ETA: 1s - loss: -237.4726 - acc: 0.01 - ETA: 1s - loss: -203.8992 - acc: 0.01 - ETA: 0s - loss: -178.4477 - acc: 0.02 - ETA: 0s - loss: -161.2007 - acc: 0.01 - 5s 5ms/step - loss: -153.8201 - acc: 0.0201\n",
      "Epoch 118/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7767 - acc: 0.03 - ETA: 3s - loss: -1.2727 - acc: 0.02 - ETA: 3s - loss: -1.3983 - acc: 0.02 - ETA: 2s - loss: -1.1165 - acc: 0.02 - ETA: 2s - loss: -1.6891 - acc: 0.02 - ETA: 1s - loss: -1.5928 - acc: 0.02 - ETA: 1s - loss: -1.5138 - acc: 0.02 - ETA: 0s - loss: -1.5233 - acc: 0.02 - ETA: 0s - loss: -141.9918 - acc: 0.02 - 5s 5ms/step - loss: -135.5341 - acc: 0.0201\n",
      "Epoch 119/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.0186 - acc: 0.01 - ETA: 3s - loss: -1.2949 - acc: 0.01 - ETA: 3s - loss: -0.9489 - acc: 0.02 - ETA: 2s - loss: -1.1234 - acc: 0.02 - ETA: 2s - loss: -1.0334 - acc: 0.02 - ETA: 1s - loss: -0.9191 - acc: 0.02 - ETA: 1s - loss: -1.1253 - acc: 0.02 - ETA: 0s - loss: -1.1183 - acc: 0.02 - ETA: 0s - loss: -1.8754 - acc: 0.02 - 5s 5ms/step - loss: -48.4218 - acc: 0.0201\n",
      "Epoch 120/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.2741 - acc: 0.04 - ETA: 3s - loss: -1.1707 - acc: 0.04 - ETA: 3s - loss: -0.6404 - acc: 0.03 - ETA: 2s - loss: -1.0290 - acc: 0.03 - ETA: 2s - loss: -4.1166 - acc: 0.02 - ETA: 1s - loss: -3.4947 - acc: 0.02 - ETA: 1s - loss: -211.6778 - acc: 0.02 - ETA: 0s - loss: -185.8061 - acc: 0.02 - ETA: 0s - loss: -165.7507 - acc: 0.02 - 5s 6ms/step - loss: -158.1873 - acc: 0.0201\n",
      "Epoch 121/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1159.6161 - acc: 0.0000e+0 - ETA: 4s - loss: -580.1997 - acc: 0.0150    - ETA: 4s - loss: -387.4071 - acc: 0.01 - ETA: 3s - loss: -290.7049 - acc: 0.02 - ETA: 2s - loss: -232.6415 - acc: 0.01 - ETA: 2s - loss: -194.1225 - acc: 0.02 - ETA: 1s - loss: -166.5006 - acc: 0.02 - ETA: 0s - loss: -292.3701 - acc: 0.02 - ETA: 0s - loss: -260.0541 - acc: 0.02 - 6s 6ms/step - loss: -324.8419 - acc: 0.0201\n",
      "Epoch 122/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1862 - acc: 0.02 - ETA: 3s - loss: -1.0428 - acc: 0.02 - ETA: 3s - loss: -0.6395 - acc: 0.02 - ETA: 2s - loss: -1.3069 - acc: 0.02 - ETA: 2s - loss: -279.3700 - acc: 0.01 - ETA: 1s - loss: -423.0528 - acc: 0.01 - ETA: 1s - loss: -362.8061 - acc: 0.01 - ETA: 0s - loss: -317.8117 - acc: 0.01 - ETA: 0s - loss: -282.5715 - acc: 0.02 - 5s 5ms/step - loss: -269.6806 - acc: 0.0201\n",
      "Epoch 123/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2307 - acc: 0.03 - ETA: 3s - loss: -0.8485 - acc: 0.03 - ETA: 3s - loss: -420.3808 - acc: 0.02 - ETA: 2s - loss: -315.6412 - acc: 0.02 - ETA: 2s - loss: -252.6571 - acc: 0.02 - ETA: 1s - loss: -210.6469 - acc: 0.02 - ETA: 1s - loss: -180.7629 - acc: 0.02 - ETA: 0s - loss: -158.2792 - acc: 0.02 - ETA: 0s - loss: -141.0604 - acc: 0.02 - 5s 5ms/step - loss: -206.1014 - acc: 0.0201\n",
      "Epoch 124/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1924 - acc: 0.02 - ETA: 3s - loss: -594.8331 - acc: 0.01 - ETA: 3s - loss: -396.7900 - acc: 0.01 - ETA: 2s - loss: -572.4118 - acc: 0.01 - ETA: 2s - loss: -457.8896 - acc: 0.02 - ETA: 1s - loss: -381.8060 - acc: 0.02 - ETA: 1s - loss: -327.2476 - acc: 0.02 - ETA: 0s - loss: -287.6436 - acc: 0.02 - ETA: 0s - loss: -257.9818 - acc: 0.02 - 5s 5ms/step - loss: -246.2457 - acc: 0.0201\n",
      "Epoch 125/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.6554 - acc: 0.02 - ETA: 3s - loss: -1.2615 - acc: 0.02 - ETA: 3s - loss: -1.5878 - acc: 0.01 - ETA: 2s - loss: -1.7618 - acc: 0.01 - ETA: 2s - loss: -177.8702 - acc: 0.01 - ETA: 1s - loss: -148.2678 - acc: 0.01 - ETA: 1s - loss: -127.2177 - acc: 0.02 - ETA: 0s - loss: -264.4045 - acc: 0.01 - ETA: 0s - loss: -235.1176 - acc: 0.02 - 5s 5ms/step - loss: -271.6810 - acc: 0.0201\n",
      "Epoch 126/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.7120 - acc: 0.02 - ETA: 3s - loss: -2.1189 - acc: 0.01 - ETA: 3s - loss: -301.0994 - acc: 0.01 - ETA: 2s - loss: -225.8461 - acc: 0.02 - ETA: 2s - loss: -181.4259 - acc: 0.01 - ETA: 1s - loss: -151.4352 - acc: 0.02 - ETA: 1s - loss: -129.8871 - acc: 0.02 - ETA: 0s - loss: -114.1910 - acc: 0.02 - ETA: 0s - loss: -101.7436 - acc: 0.02 - 5s 5ms/step - loss: -97.1378 - acc: 0.0201\n",
      "Epoch 127/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8322 - acc: 0.02 - ETA: 3s - loss: -631.8179 - acc: 0.01 - ETA: 3s - loss: -421.3992 - acc: 0.01 - ETA: 2s - loss: -316.7851 - acc: 0.01 - ETA: 2s - loss: -253.5797 - acc: 0.01 - ETA: 1s - loss: -211.6839 - acc: 0.01 - ETA: 1s - loss: -181.4382 - acc: 0.01 - ETA: 0s - loss: -158.9623 - acc: 0.01 - ETA: 0s - loss: -141.4218 - acc: 0.02 - 5s 6ms/step - loss: -203.7408 - acc: 0.0201\n",
      "Epoch 128/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.2620 - acc: 0.02 - ETA: 3s - loss: -2.8988 - acc: 0.01 - ETA: 3s - loss: -250.4103 - acc: 0.01 - ETA: 2s - loss: -188.1457 - acc: 0.01 - ETA: 2s - loss: -150.9126 - acc: 0.01 - ETA: 1s - loss: -126.0142 - acc: 0.01 - ETA: 1s - loss: -108.1605 - acc: 0.01 - ETA: 0s - loss: -94.6894 - acc: 0.0212 - ETA: 0s - loss: -84.3486 - acc: 0.021 - 5s 5ms/step - loss: -110.2682 - acc: 0.0201\n",
      "Epoch 129/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.0116 - acc: 0.01 - ETA: 3s - loss: -1.8571 - acc: 0.02 - ETA: 3s - loss: -1.5360 - acc: 0.02 - ETA: 2s - loss: -1.5571 - acc: 0.02 - ETA: 2s - loss: -1.5040 - acc: 0.02 - ETA: 1s - loss: -1.9417 - acc: 0.02 - ETA: 1s - loss: -2.2539 - acc: 0.02 - ETA: 0s - loss: -2.4260 - acc: 0.01 - ETA: 0s - loss: -2.1188 - acc: 0.02 - 5s 5ms/step - loss: -2.0557 - acc: 0.0201\n",
      "Epoch 130/500\n",
      "943/943 [==============================] - ETA: 5s - loss: -1262.5790 - acc: 0.0000e+0 - ETA: 4s - loss: -631.4279 - acc: 0.0150    - ETA: 3s - loss: -421.3865 - acc: 0.01 - ETA: 3s - loss: -316.1707 - acc: 0.01 - ETA: 2s - loss: -253.2100 - acc: 0.02 - ETA: 1s - loss: -420.0124 - acc: 0.01 - ETA: 1s - loss: -360.0384 - acc: 0.02 - ETA: 0s - loss: -315.2349 - acc: 0.02 - ETA: 0s - loss: -280.7333 - acc: 0.01 - 5s 6ms/step - loss: -267.9321 - acc: 0.0201\n",
      "Epoch 131/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9207 - acc: 0.02 - ETA: 3s - loss: -2.2829 - acc: 0.02 - ETA: 3s - loss: -1.9898 - acc: 0.01 - ETA: 2s - loss: -1.9983 - acc: 0.01 - ETA: 2s - loss: -1.8178 - acc: 0.01 - ETA: 1s - loss: -1.6276 - acc: 0.01 - ETA: 1s - loss: -187.2871 - acc: 0.01 - ETA: 0s - loss: -163.9360 - acc: 0.01 - ETA: 0s - loss: -145.8213 - acc: 0.02 - 5s 5ms/step - loss: -139.1854 - acc: 0.0201\n",
      "Epoch 132/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2930 - acc: 0.02 - ETA: 3s - loss: -0.8597 - acc: 0.01 - ETA: 3s - loss: -1.1208 - acc: 0.01 - ETA: 2s - loss: -1.1877 - acc: 0.01 - ETA: 2s - loss: -1.1109 - acc: 0.01 - ETA: 1s - loss: -1.3396 - acc: 0.02 - ETA: 1s - loss: -1.2845 - acc: 0.02 - ETA: 0s - loss: -1.2305 - acc: 0.02 - ETA: 0s - loss: -144.7192 - acc: 0.01 - 5s 6ms/step - loss: -138.1544 - acc: 0.0201\n",
      "Epoch 133/500\n",
      "943/943 [==============================] - ETA: 5s - loss: -2.5846 - acc: 0.01 - ETA: 4s - loss: -1.8353 - acc: 0.02 - ETA: 3s - loss: -2.1052 - acc: 0.02 - ETA: 3s - loss: -1.8417 - acc: 0.02 - ETA: 2s - loss: -2.2880 - acc: 0.02 - ETA: 2s - loss: -2.7332 - acc: 0.01 - ETA: 1s - loss: -2.5458 - acc: 0.01 - ETA: 0s - loss: -2.2075 - acc: 0.02 - ETA: 0s - loss: -2.0237 - acc: 0.02 - 6s 6ms/step - loss: -43.8047 - acc: 0.0201\n",
      "Epoch 134/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.0875 - acc: 0.01 - ETA: 3s - loss: -2.0401 - acc: 0.02 - ETA: 3s - loss: -2.2281 - acc: 0.02 - ETA: 2s - loss: -2.1614 - acc: 0.02 - ETA: 2s - loss: -1.8059 - acc: 0.02 - ETA: 1s - loss: -1.5086 - acc: 0.02 - ETA: 1s - loss: -1.6565 - acc: 0.02 - ETA: 0s - loss: -127.9256 - acc: 0.02 - ETA: 0s - loss: -237.7241 - acc: 0.02 - 5s 6ms/step - loss: -226.9419 - acc: 0.0201\n",
      "Epoch 135/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8825 - acc: 0.03 - ETA: 3s - loss: -1.2583 - acc: 0.02 - ETA: 3s - loss: -1.3928 - acc: 0.02 - ETA: 2s - loss: -1.3966 - acc: 0.02 - ETA: 2s - loss: -1.0963 - acc: 0.02 - ETA: 1s - loss: -1.0123 - acc: 0.02 - ETA: 1s - loss: -1.5344 - acc: 0.02 - ETA: 0s - loss: -1.5644 - acc: 0.02 - ETA: 0s - loss: -1.7987 - acc: 0.02 - 5s 6ms/step - loss: -76.5079 - acc: 0.0201\n",
      "Epoch 136/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3593 - acc: 0.04 - ETA: 3s - loss: -0.2421 - acc: 0.02 - ETA: 3s - loss: -1.3892 - acc: 0.02 - ETA: 2s - loss: -1.9149 - acc: 0.02 - ETA: 2s - loss: -1.9057 - acc: 0.02 - ETA: 1s - loss: -1.7779 - acc: 0.02 - ETA: 1s - loss: -1.8513 - acc: 0.02 - ETA: 0s - loss: -1.7272 - acc: 0.02 - ETA: 0s - loss: -1.6111 - acc: 0.02 - 5s 6ms/step - loss: -57.8014 - acc: 0.0201\n",
      "Epoch 137/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.2139 - acc: 0.04 - ETA: 3s - loss: -1.6379 - acc: 0.03 - ETA: 3s - loss: -1.3995 - acc: 0.02 - ETA: 2s - loss: -3.4451 - acc: 0.02 - ETA: 2s - loss: -2.7745 - acc: 0.02 - ETA: 1s - loss: -2.9370 - acc: 0.02 - ETA: 1s - loss: -2.5125 - acc: 0.02 - ETA: 0s - loss: -166.6089 - acc: 0.02 - ETA: 0s - loss: -148.2402 - acc: 0.02 - 5s 5ms/step - loss: -159.6515 - acc: 0.0201\n",
      "Epoch 138/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2084 - acc: 0.04 - ETA: 3s - loss: -0.5729 - acc: 0.03 - ETA: 3s - loss: -335.3803 - acc: 0.02 - ETA: 2s - loss: -252.9757 - acc: 0.01 - ETA: 2s - loss: -202.6285 - acc: 0.01 - ETA: 1s - loss: -169.0180 - acc: 0.02 - ETA: 1s - loss: -145.1329 - acc: 0.01 - ETA: 0s - loss: -127.2515 - acc: 0.01 - ETA: 0s - loss: -113.2650 - acc: 0.02 - 5s 5ms/step - loss: -108.1842 - acc: 0.0201\n",
      "Epoch 139/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9227 - acc: 0.03 - ETA: 3s - loss: -1.1392 - acc: 0.03 - ETA: 3s - loss: -1.1397 - acc: 0.03 - ETA: 2s - loss: -1.2169 - acc: 0.02 - ETA: 2s - loss: -0.9430 - acc: 0.02 - ETA: 1s - loss: -1.4300 - acc: 0.02 - ETA: 1s - loss: -1.3805 - acc: 0.02 - ETA: 0s - loss: -206.3469 - acc: 0.02 - ETA: 0s - loss: -183.7310 - acc: 0.02 - 5s 5ms/step - loss: -175.3319 - acc: 0.0201\n",
      "Epoch 140/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.6559 - acc: 0.01 - ETA: 3s - loss: -2.9083 - acc: 0.01 - ETA: 3s - loss: -2.3979 - acc: 0.01 - ETA: 2s - loss: -2.1649 - acc: 0.02 - ETA: 2s - loss: -3.4380 - acc: 0.01 - ETA: 1s - loss: -2.9742 - acc: 0.01 - ETA: 1s - loss: -2.7611 - acc: 0.01 - ETA: 0s - loss: -2.6133 - acc: 0.01 - ETA: 0s - loss: -2.3121 - acc: 0.02 - 5s 6ms/step - loss: -2.2196 - acc: 0.0201\n",
      "Epoch 141/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -21.6369 - acc: 0.010 - ETA: 3s - loss: -11.8553 - acc: 0.015 - ETA: 3s - loss: -8.2483 - acc: 0.013 - ETA: 2s - loss: -6.7426 - acc: 0.01 - ETA: 2s - loss: -5.4645 - acc: 0.01 - ETA: 1s - loss: -4.7237 - acc: 0.02 - ETA: 1s - loss: -4.1015 - acc: 0.02 - ETA: 0s - loss: -3.7257 - acc: 0.02 - ETA: 0s - loss: -4.0394 - acc: 0.02 - 5s 5ms/step - loss: -3.8893 - acc: 0.0201\n",
      "Epoch 142/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -6.1811 - acc: 0.01 - ETA: 3s - loss: -3.9051 - acc: 0.01 - ETA: 3s - loss: -2.7204 - acc: 0.01 - ETA: 2s - loss: -2.0731 - acc: 0.01 - ETA: 2s - loss: -2.8491 - acc: 0.01 - ETA: 1s - loss: -2.3570 - acc: 0.02 - ETA: 1s - loss: -2.3207 - acc: 0.02 - ETA: 0s - loss: -4.2326 - acc: 0.02 - ETA: 0s - loss: -4.2204 - acc: 0.02 - 5s 5ms/step - loss: -4.0580 - acc: 0.0201\n",
      "Epoch 143/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8270 - acc: 0.02 - ETA: 3s - loss: -2.1696 - acc: 0.01 - ETA: 3s - loss: -1.8631 - acc: 0.02 - ETA: 2s - loss: -2.1648 - acc: 0.01 - ETA: 2s - loss: -2.0609 - acc: 0.01 - ETA: 1s - loss: -1.7452 - acc: 0.01 - ETA: 1s - loss: -1.6387 - acc: 0.02 - ETA: 0s - loss: -1.9046 - acc: 0.02 - ETA: 0s - loss: -1.8495 - acc: 0.01 - 5s 6ms/step - loss: -1.7947 - acc: 0.0201\n",
      "Epoch 144/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4047 - acc: 0.01 - ETA: 3s - loss: -623.8899 - acc: 0.00 - ETA: 3s - loss: -861.5821 - acc: 0.00 - ETA: 2s - loss: -646.3445 - acc: 0.00 - ETA: 2s - loss: -517.2198 - acc: 0.01 - ETA: 1s - loss: -431.1313 - acc: 0.01 - ETA: 1s - loss: -369.6229 - acc: 0.02 - ETA: 0s - loss: -324.0265 - acc: 0.02 - ETA: 0s - loss: -288.1455 - acc: 0.02 - 5s 6ms/step - loss: -310.4209 - acc: 0.0201\n",
      "Epoch 145/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -0.6388 - acc: 0.03 - ETA: 3s - loss: -1.0132 - acc: 0.02 - ETA: 3s - loss: -0.9612 - acc: 0.02 - ETA: 2s - loss: -0.9818 - acc: 0.02 - ETA: 2s - loss: -0.8949 - acc: 0.02 - ETA: 1s - loss: -0.7807 - acc: 0.02 - ETA: 1s - loss: -1.2295 - acc: 0.02 - ETA: 0s - loss: -113.1995 - acc: 0.02 - ETA: 0s - loss: -101.4229 - acc: 0.02 - 5s 6ms/step - loss: -96.9152 - acc: 0.0201\n",
      "Epoch 146/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.1169 - acc: 0.01 - ETA: 3s - loss: -1.7770 - acc: 0.02 - ETA: 3s - loss: -1.4040 - acc: 0.02 - ETA: 2s - loss: -1.6362 - acc: 0.02 - ETA: 2s - loss: -1.4232 - acc: 0.02 - ETA: 1s - loss: -1.2184 - acc: 0.02 - ETA: 1s - loss: -1.2033 - acc: 0.02 - ETA: 0s - loss: -1.4535 - acc: 0.02 - ETA: 0s - loss: -1.7582 - acc: 0.02 - 5s 5ms/step - loss: -67.8219 - acc: 0.0201\n",
      "Epoch 147/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -690.2137 - acc: 0.0000e+ - ETA: 4s - loss: -356.3842 - acc: 0.0050   - ETA: 3s - loss: -237.6594 - acc: 0.01 - ETA: 2s - loss: -178.1036 - acc: 0.02 - ETA: 2s - loss: -142.8008 - acc: 0.02 - ETA: 1s - loss: -119.3120 - acc: 0.02 - ETA: 1s - loss: -102.6572 - acc: 0.01 - ETA: 0s - loss: -90.1724 - acc: 0.0200 - ETA: 0s - loss: -80.3610 - acc: 0.020 - 5s 6ms/step - loss: -76.7096 - acc: 0.0201\n",
      "Epoch 148/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6455 - acc: 0.03 - ETA: 3s - loss: -4.2898 - acc: 0.02 - ETA: 3s - loss: -3.5878 - acc: 0.02 - ETA: 3s - loss: -3.2133 - acc: 0.01 - ETA: 2s - loss: -2.8190 - acc: 0.02 - ETA: 1s - loss: -2.4935 - acc: 0.02 - ETA: 1s - loss: -2.1310 - acc: 0.02 - ETA: 0s - loss: -2.7784 - acc: 0.02 - ETA: 0s - loss: -2.6718 - acc: 0.02 - 6s 6ms/step - loss: -2.6555 - acc: 0.0201\n",
      "Epoch 149/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4661 - acc: 0.03 - ETA: 3s - loss: -1.3067 - acc: 0.03 - ETA: 3s - loss: -1.2527 - acc: 0.02 - ETA: 2s - loss: -1.6506 - acc: 0.02 - ETA: 2s - loss: -174.9544 - acc: 0.02 - ETA: 1s - loss: -145.9536 - acc: 0.02 - ETA: 1s - loss: -125.5032 - acc: 0.02 - ETA: 0s - loss: -112.2866 - acc: 0.02 - ETA: 0s - loss: -99.8379 - acc: 0.0211 - 5s 6ms/step - loss: -121.0284 - acc: 0.0201\n",
      "Epoch 150/500\n",
      "943/943 [==============================] - ETA: 5s - loss: -4.8071 - acc: 0.01 - ETA: 4s - loss: -3.0516 - acc: 0.01 - ETA: 4s - loss: -2.3122 - acc: 0.02 - ETA: 3s - loss: -2.9761 - acc: 0.01 - ETA: 2s - loss: -2.3927 - acc: 0.02 - ETA: 2s - loss: -2.0004 - acc: 0.02 - ETA: 1s - loss: -1.9794 - acc: 0.02 - ETA: 0s - loss: -2.0256 - acc: 0.02 - ETA: 0s - loss: -2.1117 - acc: 0.02 - 6s 6ms/step - loss: -58.0467 - acc: 0.0201\n",
      "Epoch 151/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1091.9036 - acc: 0.0000e+0 - ETA: 3s - loss: -547.4573 - acc: 0.0100    - ETA: 3s - loss: -365.4354 - acc: 0.01 - ETA: 2s - loss: -274.1320 - acc: 0.02 - ETA: 2s - loss: -397.0771 - acc: 0.02 - ETA: 1s - loss: -332.1182 - acc: 0.01 - ETA: 1s - loss: -284.6969 - acc: 0.02 - ETA: 0s - loss: -249.2225 - acc: 0.02 - ETA: 0s - loss: -221.6066 - acc: 0.02 - 5s 5ms/step - loss: -280.5924 - acc: 0.0201\n",
      "Epoch 152/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0922 - acc: 0.02 - ETA: 3s - loss: -1.6827 - acc: 0.01 - ETA: 3s - loss: -1.4207 - acc: 0.01 - ETA: 2s - loss: -1.1886 - acc: 0.01 - ETA: 2s - loss: -1.8536 - acc: 0.01 - ETA: 1s - loss: -155.8037 - acc: 0.01 - ETA: 1s - loss: -133.6537 - acc: 0.01 - ETA: 0s - loss: -117.1151 - acc: 0.01 - ETA: 0s - loss: -104.1649 - acc: 0.02 - 5s 6ms/step - loss: -99.5436 - acc: 0.0201\n",
      "Epoch 153/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4609 - acc: 0.04 - ETA: 3s - loss: -668.7379 - acc: 0.02 - ETA: 3s - loss: -793.3562 - acc: 0.01 - ETA: 2s - loss: -596.6705 - acc: 0.01 - ETA: 2s - loss: -477.3632 - acc: 0.01 - ETA: 1s - loss: -397.8635 - acc: 0.01 - ETA: 1s - loss: -341.1156 - acc: 0.01 - ETA: 0s - loss: -298.6441 - acc: 0.02 - ETA: 0s - loss: -344.7127 - acc: 0.02 - 5s 5ms/step - loss: -329.0278 - acc: 0.0201\n",
      "Epoch 154/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.7217 - acc: 0.02 - ETA: 3s - loss: -2.4306 - acc: 0.02 - ETA: 3s - loss: -1.8039 - acc: 0.03 - ETA: 2s - loss: -1.7773 - acc: 0.02 - ETA: 2s - loss: -1.6660 - acc: 0.02 - ETA: 1s - loss: -1.6741 - acc: 0.02 - ETA: 1s - loss: -2.3238 - acc: 0.02 - ETA: 0s - loss: -2.2605 - acc: 0.02 - ETA: 0s - loss: -2.2091 - acc: 0.01 - 5s 6ms/step - loss: -2.0926 - acc: 0.0201\n",
      "Epoch 155/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1283 - acc: 0.01 - ETA: 3s - loss: -1.0040 - acc: 0.01 - ETA: 3s - loss: -271.9473 - acc: 0.01 - ETA: 2s - loss: -204.5238 - acc: 0.01 - ETA: 2s - loss: -163.6478 - acc: 0.01 - ETA: 1s - loss: -136.6958 - acc: 0.01 - ETA: 1s - loss: -117.3214 - acc: 0.01 - ETA: 0s - loss: -103.4957 - acc: 0.01 - ETA: 0s - loss: -92.2168 - acc: 0.0189 - 5s 6ms/step - loss: -88.0345 - acc: 0.0201\n",
      "Epoch 156/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6730 - acc: 0.01 - ETA: 3s - loss: -0.6201 - acc: 0.01 - ETA: 3s - loss: -1.1543 - acc: 0.01 - ETA: 2s - loss: -1.2335 - acc: 0.02 - ETA: 2s - loss: -1.4356 - acc: 0.02 - ETA: 1s - loss: -1.4978 - acc: 0.02 - ETA: 1s - loss: -3.7979 - acc: 0.02 - ETA: 0s - loss: -3.6865 - acc: 0.01 - ETA: 0s - loss: -3.3988 - acc: 0.02 - 5s 5ms/step - loss: -3.2231 - acc: 0.0201\n",
      "Epoch 157/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.6748 - acc: 0.02 - ETA: 3s - loss: -1.2672 - acc: 0.02 - ETA: 3s - loss: -1.5223 - acc: 0.01 - ETA: 2s - loss: -1.4023 - acc: 0.01 - ETA: 2s - loss: -1.3295 - acc: 0.01 - ETA: 1s - loss: -1.6424 - acc: 0.01 - ETA: 1s - loss: -1.8458 - acc: 0.01 - ETA: 0s - loss: -1.8588 - acc: 0.01 - ETA: 0s - loss: -1.8297 - acc: 0.01 - 5s 5ms/step - loss: -1.7214 - acc: 0.0201\n",
      "Epoch 158/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2015 - acc: 0.03 - ETA: 3s - loss: -3.0535 - acc: 0.02 - ETA: 3s - loss: -2.2108 - acc: 0.03 - ETA: 2s - loss: -1.9163 - acc: 0.02 - ETA: 2s - loss: -1.7989 - acc: 0.02 - ETA: 1s - loss: -2.0330 - acc: 0.02 - ETA: 1s - loss: -164.4715 - acc: 0.02 - ETA: 0s - loss: -144.1365 - acc: 0.01 - ETA: 0s - loss: -128.2991 - acc: 0.01 - 5s 5ms/step - loss: -122.4785 - acc: 0.0201\n",
      "Epoch 159/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1521 - acc: 0.04 - ETA: 4s - loss: -1.2473 - acc: 0.03 - ETA: 3s - loss: -1.0767 - acc: 0.02 - ETA: 2s - loss: -253.7648 - acc: 0.02 - ETA: 2s - loss: -203.8589 - acc: 0.01 - ETA: 1s - loss: -170.6408 - acc: 0.01 - ETA: 1s - loss: -146.5036 - acc: 0.01 - ETA: 0s - loss: -128.2161 - acc: 0.02 - ETA: 0s - loss: -242.3133 - acc: 0.01 - 5s 6ms/step - loss: -231.2568 - acc: 0.0201\n",
      "Epoch 160/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4269 - acc: 0.02 - ETA: 3s - loss: -1.0625 - acc: 0.03 - ETA: 3s - loss: -1.1754 - acc: 0.02 - ETA: 2s - loss: -0.9685 - acc: 0.02 - ETA: 2s - loss: -0.8098 - acc: 0.02 - ETA: 1s - loss: -0.9134 - acc: 0.02 - ETA: 1s - loss: -0.7517 - acc: 0.02 - ETA: 0s - loss: -1.3586 - acc: 0.02 - ETA: 0s - loss: -1.7968 - acc: 0.02 - 5s 5ms/step - loss: -67.4737 - acc: 0.0201\n",
      "Epoch 161/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4146 - acc: 0.02 - ETA: 3s - loss: -1.5601 - acc: 0.01 - ETA: 3s - loss: -1.5531 - acc: 0.02 - ETA: 2s - loss: -3.0232 - acc: 0.01 - ETA: 2s - loss: -3.6666 - acc: 0.01 - ETA: 1s - loss: -3.1421 - acc: 0.01 - ETA: 1s - loss: -2.7298 - acc: 0.01 - ETA: 0s - loss: -2.4994 - acc: 0.01 - ETA: 0s - loss: -2.3187 - acc: 0.02 - 5s 5ms/step - loss: -2.2264 - acc: 0.0201\n",
      "Epoch 162/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3921 - acc: 0.01 - ETA: 3s - loss: -1.9391 - acc: 0.01 - ETA: 3s - loss: -2.0462 - acc: 0.01 - ETA: 2s - loss: -1.8913 - acc: 0.01 - ETA: 2s - loss: -1.7747 - acc: 0.02 - ETA: 1s - loss: -1.8053 - acc: 0.02 - ETA: 1s - loss: -1.5035 - acc: 0.02 - ETA: 0s - loss: -3.7918 - acc: 0.02 - ETA: 0s - loss: -3.4989 - acc: 0.02 - 5s 5ms/step - loss: -71.4685 - acc: 0.0201\n",
      "Epoch 163/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.4949 - acc: 0.01 - ETA: 3s - loss: -2.1521 - acc: 0.01 - ETA: 3s - loss: -2.0908 - acc: 0.01 - ETA: 2s - loss: -1.6823 - acc: 0.02 - ETA: 2s - loss: -1.8685 - acc: 0.02 - ETA: 1s - loss: -163.0323 - acc: 0.01 - ETA: 1s - loss: -139.9122 - acc: 0.01 - ETA: 0s - loss: -122.4728 - acc: 0.02 - ETA: 0s - loss: -109.2216 - acc: 0.02 - 5s 5ms/step - loss: -104.4150 - acc: 0.0201\n",
      "Epoch 164/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.5501 - acc: 0.04 - ETA: 3s - loss: -1.4824 - acc: 0.02 - ETA: 3s - loss: -1.3698 - acc: 0.02 - ETA: 2s - loss: -1.8315 - acc: 0.02 - ETA: 2s - loss: -2.5209 - acc: 0.01 - ETA: 1s - loss: -2.1064 - acc: 0.02 - ETA: 1s - loss: -2.7145 - acc: 0.02 - ETA: 0s - loss: -2.3461 - acc: 0.02 - ETA: 0s - loss: -2.3360 - acc: 0.02 - 5s 5ms/step - loss: -2.2885 - acc: 0.0201\n",
      "Epoch 165/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.5808 - acc: 0.02 - ETA: 3s - loss: -2.1533 - acc: 0.01 - ETA: 3s - loss: -2.0720 - acc: 0.01 - ETA: 2s - loss: -1.5019 - acc: 0.02 - ETA: 2s - loss: -1.3049 - acc: 0.02 - ETA: 1s - loss: -1.4628 - acc: 0.02 - ETA: 1s - loss: -178.4858 - acc: 0.01 - ETA: 0s - loss: -327.0257 - acc: 0.01 - ETA: 0s - loss: -290.7134 - acc: 0.02 - 5s 6ms/step - loss: -277.5458 - acc: 0.0201\n",
      "Epoch 166/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6441 - acc: 0.03 - ETA: 3s - loss: -0.1843 - acc: 0.04 - ETA: 3s - loss: -1.8761 - acc: 0.03 - ETA: 2s - loss: -294.5207 - acc: 0.02 - ETA: 2s - loss: -236.0011 - acc: 0.02 - ETA: 1s - loss: -377.3123 - acc: 0.02 - ETA: 1s - loss: -323.4151 - acc: 0.02 - ETA: 0s - loss: -412.6942 - acc: 0.02 - ETA: 0s - loss: -369.3438 - acc: 0.02 - 5s 5ms/step - loss: -352.5605 - acc: 0.0201\n",
      "Epoch 167/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.7087 - acc: 0.01 - ETA: 3s - loss: -3.7420 - acc: 0.01 - ETA: 3s - loss: -426.4283 - acc: 0.00 - ETA: 2s - loss: -319.9950 - acc: 0.00 - ETA: 2s - loss: -256.1131 - acc: 0.01 - ETA: 1s - loss: -213.4980 - acc: 0.01 - ETA: 1s - loss: -183.0258 - acc: 0.02 - ETA: 0s - loss: -160.3530 - acc: 0.02 - ETA: 0s - loss: -142.8248 - acc: 0.02 - 5s 5ms/step - loss: -165.2119 - acc: 0.0201\n",
      "Epoch 168/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6408 - acc: 0.03 - ETA: 3s - loss: -0.8199 - acc: 0.02 - ETA: 3s - loss: -333.3271 - acc: 0.01 - ETA: 2s - loss: -250.0203 - acc: 0.02 - ETA: 2s - loss: -200.5176 - acc: 0.02 - ETA: 1s - loss: -167.3616 - acc: 0.02 - ETA: 1s - loss: -143.7670 - acc: 0.02 - ETA: 0s - loss: -262.8027 - acc: 0.02 - ETA: 0s - loss: -233.7172 - acc: 0.01 - 5s 6ms/step - loss: -223.1614 - acc: 0.0201\n",
      "Epoch 169/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7884 - acc: 0.03 - ETA: 3s - loss: -0.4710 - acc: 0.02 - ETA: 3s - loss: -1.7140 - acc: 0.02 - ETA: 2s - loss: -1.6379 - acc: 0.01 - ETA: 2s - loss: -256.5896 - acc: 0.01 - ETA: 1s - loss: -214.1398 - acc: 0.01 - ETA: 1s - loss: -186.3603 - acc: 0.01 - ETA: 0s - loss: -163.1968 - acc: 0.01 - ETA: 0s - loss: -145.1435 - acc: 0.01 - 5s 5ms/step - loss: -138.5184 - acc: 0.0201\n",
      "Epoch 170/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -869.4091 - acc: 0.0000e+ - ETA: 3s - loss: -435.3904 - acc: 0.0100   - ETA: 3s - loss: -290.6802 - acc: 0.01 - ETA: 2s - loss: -219.4850 - acc: 0.01 - ETA: 2s - loss: -397.7827 - acc: 0.01 - ETA: 1s - loss: -331.6281 - acc: 0.01 - ETA: 1s - loss: -284.3059 - acc: 0.02 - ETA: 0s - loss: -249.2238 - acc: 0.01 - ETA: 0s - loss: -221.6569 - acc: 0.01 - 5s 5ms/step - loss: -211.5368 - acc: 0.0201\n",
      "Epoch 171/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4198 - acc: 0.02 - ETA: 3s - loss: -0.2770 - acc: 0.03 - ETA: 3s - loss: -0.3015 - acc: 0.02 - ETA: 2s - loss: -1.5546 - acc: 0.02 - ETA: 2s - loss: -1.4033 - acc: 0.02 - ETA: 1s - loss: -1.3141 - acc: 0.02 - ETA: 1s - loss: -1.3492 - acc: 0.02 - ETA: 0s - loss: -1.6226 - acc: 0.02 - ETA: 0s - loss: -1.8836 - acc: 0.02 - 5s 6ms/step - loss: -64.1313 - acc: 0.0201\n",
      "Epoch 172/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9121 - acc: 0.02 - ETA: 3s - loss: -1.0875 - acc: 0.03 - ETA: 3s - loss: -0.7830 - acc: 0.03 - ETA: 2s - loss: -1.0329 - acc: 0.03 - ETA: 2s - loss: -189.3450 - acc: 0.02 - ETA: 1s - loss: -158.5644 - acc: 0.02 - ETA: 1s - loss: -136.0373 - acc: 0.02 - ETA: 0s - loss: -119.2052 - acc: 0.02 - ETA: 0s - loss: -106.1395 - acc: 0.02 - 5s 5ms/step - loss: -177.1390 - acc: 0.0201\n",
      "Epoch 173/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6374 - acc: 0.02 - ETA: 4s - loss: -1.2341 - acc: 0.01 - ETA: 3s - loss: -1.3314 - acc: 0.02 - ETA: 2s - loss: -189.7130 - acc: 0.01 - ETA: 2s - loss: -151.8244 - acc: 0.02 - ETA: 1s - loss: -128.0189 - acc: 0.01 - ETA: 1s - loss: -110.0124 - acc: 0.01 - ETA: 0s - loss: -96.4373 - acc: 0.0187 - ETA: 0s - loss: -85.7963 - acc: 0.017 - 5s 6ms/step - loss: -81.9378 - acc: 0.0201\n",
      "Epoch 174/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0053 - acc: 0.03 - ETA: 3s - loss: -0.9468 - acc: 0.02 - ETA: 3s - loss: -0.8458 - acc: 0.02 - ETA: 2s - loss: -0.8097 - acc: 0.03 - ETA: 2s - loss: -1.2118 - acc: 0.02 - ETA: 1s - loss: -1.5296 - acc: 0.02 - ETA: 1s - loss: -1.5260 - acc: 0.02 - ETA: 0s - loss: -1.7512 - acc: 0.02 - ETA: 0s - loss: -117.2494 - acc: 0.02 - 5s 6ms/step - loss: -111.8970 - acc: 0.0201\n",
      "Epoch 175/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9213 - acc: 0.01 - ETA: 3s - loss: -2.7641 - acc: 0.01 - ETA: 3s - loss: -3.3295 - acc: 0.01 - ETA: 2s - loss: -2.6676 - acc: 0.01 - ETA: 2s - loss: -2.4732 - acc: 0.01 - ETA: 1s - loss: -2.0697 - acc: 0.01 - ETA: 1s - loss: -1.8718 - acc: 0.01 - ETA: 0s - loss: -1.7586 - acc: 0.02 - ETA: 0s - loss: -1.9162 - acc: 0.02 - 5s 5ms/step - loss: -40.0670 - acc: 0.0201\n",
      "Epoch 176/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4991 - acc: 0.01 - ETA: 3s - loss: -1.2291 - acc: 0.02 - ETA: 3s - loss: -1.0534 - acc: 0.02 - ETA: 2s - loss: -1.4035 - acc: 0.02 - ETA: 2s - loss: -2.0973 - acc: 0.02 - ETA: 1s - loss: -2.1960 - acc: 0.01 - ETA: 1s - loss: -2.9581 - acc: 0.01 - ETA: 0s - loss: -2.6042 - acc: 0.02 - ETA: 0s - loss: -2.5165 - acc: 0.02 - 5s 5ms/step - loss: -2.3716 - acc: 0.0201\n",
      "Epoch 177/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2772 - acc: 0.03 - ETA: 3s - loss: -0.3098 - acc: 0.02 - ETA: 3s - loss: -1.4346 - acc: 0.02 - ETA: 2s - loss: -1.8652 - acc: 0.01 - ETA: 2s - loss: -240.2162 - acc: 0.01 - ETA: 1s - loss: -200.4433 - acc: 0.01 - ETA: 1s - loss: -171.8568 - acc: 0.01 - ETA: 0s - loss: -150.4261 - acc: 0.01 - ETA: 0s - loss: -133.8828 - acc: 0.02 - 5s 5ms/step - loss: -127.9607 - acc: 0.0201\n",
      "Epoch 178/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.4894 - acc: 0.02 - ETA: 3s - loss: -1.6137 - acc: 0.02 - ETA: 3s - loss: -1.3265 - acc: 0.03 - ETA: 2s - loss: -1.4766 - acc: 0.03 - ETA: 2s - loss: -240.8314 - acc: 0.02 - ETA: 1s - loss: -200.7270 - acc: 0.02 - ETA: 1s - loss: -172.3625 - acc: 0.02 - ETA: 0s - loss: -280.8957 - acc: 0.02 - ETA: 0s - loss: -249.7984 - acc: 0.01 - 5s 5ms/step - loss: -238.4353 - acc: 0.0201\n",
      "Epoch 179/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4683 - acc: 0.03 - ETA: 3s - loss: -1.4329 - acc: 0.02 - ETA: 3s - loss: -3.3281 - acc: 0.01 - ETA: 2s - loss: -2.8555 - acc: 0.01 - ETA: 2s - loss: -2.4157 - acc: 0.01 - ETA: 1s - loss: -2.6513 - acc: 0.01 - ETA: 1s - loss: -2.5930 - acc: 0.01 - ETA: 0s - loss: -2.3773 - acc: 0.01 - ETA: 0s - loss: -2.1658 - acc: 0.01 - 5s 5ms/step - loss: -2.1100 - acc: 0.0201\n",
      "Epoch 180/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -7.2016 - acc: 0.01 - ETA: 3s - loss: -674.8863 - acc: 0.00 - ETA: 3s - loss: -886.3439 - acc: 0.00 - ETA: 2s - loss: -664.7759 - acc: 0.01 - ETA: 2s - loss: -532.0448 - acc: 0.01 - ETA: 1s - loss: -443.5544 - acc: 0.01 - ETA: 1s - loss: -380.3066 - acc: 0.01 - ETA: 0s - loss: -332.7758 - acc: 0.02 - ETA: 0s - loss: -296.1033 - acc: 0.02 - 5s 5ms/step - loss: -282.5801 - acc: 0.0201\n",
      "Epoch 181/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -1547.5991 - acc: 0.0000e+0 - ETA: 3s - loss: -774.2504 - acc: 0.0200    - ETA: 3s - loss: -517.7638 - acc: 0.01 - ETA: 2s - loss: -388.3862 - acc: 0.02 - ETA: 2s - loss: -310.8249 - acc: 0.02 - ETA: 1s - loss: -447.2021 - acc: 0.01 - ETA: 1s - loss: -383.5779 - acc: 0.01 - ETA: 0s - loss: -335.8185 - acc: 0.01 - ETA: 0s - loss: -298.5391 - acc: 0.02 - 5s 5ms/step - loss: -307.8870 - acc: 0.0201\n",
      "Epoch 182/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4130 - acc: 0.01 - ETA: 4s - loss: -0.8005 - acc: 0.02 - ETA: 3s - loss: -1.7614 - acc: 0.02 - ETA: 2s - loss: -257.7869 - acc: 0.01 - ETA: 2s - loss: -206.2046 - acc: 0.01 - ETA: 1s - loss: -172.0753 - acc: 0.02 - ETA: 1s - loss: -148.3411 - acc: 0.02 - ETA: 0s - loss: -130.1367 - acc: 0.01 - ETA: 0s - loss: -115.7388 - acc: 0.01 - 5s 6ms/step - loss: -110.4933 - acc: 0.0201\n",
      "Epoch 183/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0657 - acc: 0.02 - ETA: 3s - loss: -2.5236 - acc: 0.01 - ETA: 3s - loss: -1.6861 - acc: 0.02 - ETA: 2s - loss: -1.5552 - acc: 0.02 - ETA: 2s - loss: -1.7993 - acc: 0.02 - ETA: 1s - loss: -1.6717 - acc: 0.02 - ETA: 1s - loss: -1.6118 - acc: 0.02 - ETA: 0s - loss: -1.4934 - acc: 0.02 - ETA: 0s - loss: -1.6545 - acc: 0.01 - 5s 6ms/step - loss: -1.7436 - acc: 0.0201\n",
      "Epoch 184/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5792 - acc: 0.02 - ETA: 3s - loss: -0.6381 - acc: 0.03 - ETA: 3s - loss: -0.2885 - acc: 0.03 - ETA: 2s - loss: -0.8202 - acc: 0.03 - ETA: 2s - loss: -0.6523 - acc: 0.02 - ETA: 1s - loss: -0.9923 - acc: 0.02 - ETA: 1s - loss: -208.2850 - acc: 0.02 - ETA: 0s - loss: -182.8137 - acc: 0.02 - ETA: 0s - loss: -162.6516 - acc: 0.02 - 5s 5ms/step - loss: -225.1420 - acc: 0.0201\n",
      "Epoch 185/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1192.9509 - acc: 0.0000e+0 - ETA: 3s - loss: -596.9855 - acc: 0.0050    - ETA: 3s - loss: -398.0278 - acc: 0.01 - ETA: 2s - loss: -299.3467 - acc: 0.01 - ETA: 2s - loss: -239.8766 - acc: 0.02 - ETA: 1s - loss: -370.8929 - acc: 0.01 - ETA: 1s - loss: -318.0742 - acc: 0.02 - ETA: 0s - loss: -278.5406 - acc: 0.02 - ETA: 0s - loss: -247.7152 - acc: 0.02 - 5s 5ms/step - loss: -236.3870 - acc: 0.0201\n",
      "Epoch 186/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3927 - acc: 0.03 - ETA: 3s - loss: -0.5383 - acc: 0.03 - ETA: 3s - loss: -0.6233 - acc: 0.02 - ETA: 2s - loss: -0.6228 - acc: 0.02 - ETA: 2s - loss: -0.7555 - acc: 0.02 - ETA: 1s - loss: -173.9962 - acc: 0.02 - ETA: 1s - loss: -361.5324 - acc: 0.01 - ETA: 0s - loss: -316.4308 - acc: 0.02 - ETA: 0s - loss: -281.8060 - acc: 0.01 - 5s 5ms/step - loss: -268.9631 - acc: 0.0201\n",
      "Epoch 187/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.3489 - acc: 0.040 - ETA: 3s - loss: -1.0767 - acc: 0.03 - ETA: 3s - loss: -0.6991 - acc: 0.03 - ETA: 2s - loss: -1.4461 - acc: 0.02 - ETA: 2s - loss: -1.6041 - acc: 0.02 - ETA: 1s - loss: -1.6876 - acc: 0.02 - ETA: 1s - loss: -1.8770 - acc: 0.02 - ETA: 0s - loss: -1.9394 - acc: 0.02 - ETA: 0s - loss: -141.0339 - acc: 0.01 - 5s 5ms/step - loss: -134.6443 - acc: 0.0201\n",
      "Epoch 188/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4033 - acc: 0.02 - ETA: 3s - loss: -631.4336 - acc: 0.01 - ETA: 3s - loss: -912.5418 - acc: 0.00 - ETA: 2s - loss: -684.6633 - acc: 0.01 - ETA: 2s - loss: -547.8420 - acc: 0.01 - ETA: 1s - loss: -457.0641 - acc: 0.01 - ETA: 1s - loss: -391.8830 - acc: 0.01 - ETA: 0s - loss: -343.0893 - acc: 0.01 - ETA: 0s - loss: -304.9722 - acc: 0.02 - 5s 5ms/step - loss: -340.9079 - acc: 0.0201\n",
      "Epoch 189/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3358 - acc: 0.02 - ETA: 3s - loss: -1.9892 - acc: 0.02 - ETA: 3s - loss: -1.4458 - acc: 0.01 - ETA: 2s - loss: -1.3030 - acc: 0.01 - ETA: 2s - loss: -1.4714 - acc: 0.02 - ETA: 1s - loss: -213.6138 - acc: 0.01 - ETA: 1s - loss: -183.5340 - acc: 0.01 - ETA: 0s - loss: -160.6080 - acc: 0.02 - ETA: 0s - loss: -142.9338 - acc: 0.02 - 5s 5ms/step - loss: -136.4729 - acc: 0.0201\n",
      "Epoch 190/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -643.8980 - acc: 0.0000e+ - ETA: 3s - loss: -322.9070 - acc: 0.0200   - ETA: 3s - loss: -216.1157 - acc: 0.01 - ETA: 2s - loss: -162.3399 - acc: 0.02 - ETA: 2s - loss: -129.9561 - acc: 0.02 - ETA: 1s - loss: -108.4685 - acc: 0.02 - ETA: 1s - loss: -270.8911 - acc: 0.02 - ETA: 0s - loss: -237.0462 - acc: 0.02 - ETA: 0s - loss: -211.0612 - acc: 0.02 - 5s 5ms/step - loss: -248.6832 - acc: 0.0201\n",
      "Epoch 191/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1589.1670 - acc: 0.0000e+0 - ETA: 3s - loss: -795.5167 - acc: 0.0050    - ETA: 3s - loss: -530.7932 - acc: 0.01 - ETA: 2s - loss: -560.9647 - acc: 0.00 - ETA: 2s - loss: -448.8556 - acc: 0.01 - ETA: 1s - loss: -374.8345 - acc: 0.01 - ETA: 1s - loss: -321.5637 - acc: 0.01 - ETA: 0s - loss: -281.4233 - acc: 0.02 - ETA: 0s - loss: -250.7140 - acc: 0.01 - 5s 5ms/step - loss: -239.2659 - acc: 0.0201\n",
      "Epoch 192/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.2770 - acc: 0.03 - ETA: 3s - loss: -0.6943 - acc: 0.02 - ETA: 3s - loss: -0.9341 - acc: 0.02 - ETA: 2s - loss: -1.9651 - acc: 0.02 - ETA: 2s - loss: -2.0789 - acc: 0.02 - ETA: 1s - loss: -1.7677 - acc: 0.02 - ETA: 1s - loss: -197.8091 - acc: 0.01 - ETA: 0s - loss: -173.1249 - acc: 0.02 - ETA: 0s - loss: -154.0032 - acc: 0.01 - 5s 5ms/step - loss: -147.0259 - acc: 0.0201\n",
      "Epoch 193/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0212 - acc: 0.02 - ETA: 3s - loss: -0.9227 - acc: 0.01 - ETA: 3s - loss: -1.0978 - acc: 0.01 - ETA: 2s - loss: -327.6981 - acc: 0.01 - ETA: 2s - loss: -262.4954 - acc: 0.01 - ETA: 1s - loss: -218.7981 - acc: 0.02 - ETA: 1s - loss: -187.6714 - acc: 0.02 - ETA: 0s - loss: -164.4112 - acc: 0.02 - ETA: 0s - loss: -146.3928 - acc: 0.02 - 5s 6ms/step - loss: -208.1240 - acc: 0.0201\n",
      "Epoch 194/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.6632 - acc: 0.01 - ETA: 3s - loss: -1.7001 - acc: 0.02 - ETA: 3s - loss: -1.9378 - acc: 0.02 - ETA: 2s - loss: -341.1539 - acc: 0.01 - ETA: 2s - loss: -273.2879 - acc: 0.01 - ETA: 1s - loss: -228.4098 - acc: 0.01 - ETA: 1s - loss: -196.2630 - acc: 0.01 - ETA: 0s - loss: -171.7318 - acc: 0.01 - ETA: 0s - loss: -152.6522 - acc: 0.01 - 5s 5ms/step - loss: -145.6995 - acc: 0.0201\n",
      "Epoch 195/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6467 - acc: 0.02 - ETA: 3s - loss: -2.2356 - acc: 0.02 - ETA: 3s - loss: -1.6819 - acc: 0.02 - ETA: 2s - loss: -1.4486 - acc: 0.02 - ETA: 2s - loss: -245.6805 - acc: 0.01 - ETA: 1s - loss: -204.8846 - acc: 0.02 - ETA: 1s - loss: -176.2659 - acc: 0.01 - ETA: 0s - loss: -154.2713 - acc: 0.01 - ETA: 0s - loss: -137.1761 - acc: 0.02 - 5s 5ms/step - loss: -178.2200 - acc: 0.0201\n",
      "Epoch 196/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1207.0714 - acc: 0.0000e+0 - ETA: 3s - loss: -605.3552 - acc: 0.0050    - ETA: 3s - loss: -404.0545 - acc: 0.01 - ETA: 2s - loss: -304.0321 - acc: 0.01 - ETA: 2s - loss: -243.3353 - acc: 0.01 - ETA: 1s - loss: -202.9491 - acc: 0.01 - ETA: 1s - loss: -174.1228 - acc: 0.01 - ETA: 0s - loss: -152.4643 - acc: 0.01 - ETA: 0s - loss: -135.5561 - acc: 0.02 - 5s 6ms/step - loss: -159.2534 - acc: 0.0201\n",
      "Epoch 197/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6958 - acc: 0.02 - ETA: 3s - loss: -11.2493 - acc: 0.015 - ETA: 3s - loss: -444.5575 - acc: 0.01 - ETA: 2s - loss: -333.9187 - acc: 0.01 - ETA: 2s - loss: -267.3311 - acc: 0.01 - ETA: 1s - loss: -222.8193 - acc: 0.02 - ETA: 1s - loss: -191.1007 - acc: 0.02 - ETA: 0s - loss: -167.2860 - acc: 0.02 - ETA: 0s - loss: -148.7166 - acc: 0.02 - 5s 5ms/step - loss: -180.9788 - acc: 0.0201\n",
      "Epoch 198/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5442 - acc: 0.02 - ETA: 3s - loss: -0.5076 - acc: 0.02 - ETA: 3s - loss: -1.5036 - acc: 0.02 - ETA: 2s - loss: -1.1798 - acc: 0.02 - ETA: 2s - loss: -1.8056 - acc: 0.01 - ETA: 1s - loss: -2.6269 - acc: 0.01 - ETA: 1s - loss: -2.3234 - acc: 0.01 - ETA: 0s - loss: -169.8243 - acc: 0.01 - ETA: 0s - loss: -151.0006 - acc: 0.01 - 5s 5ms/step - loss: -144.1677 - acc: 0.0201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5908 - acc: 0.05 - ETA: 3s - loss: -2.5594 - acc: 0.03 - ETA: 3s - loss: -1.8252 - acc: 0.02 - ETA: 2s - loss: -1.6352 - acc: 0.02 - ETA: 2s - loss: -249.9619 - acc: 0.01 - ETA: 1s - loss: -208.3950 - acc: 0.02 - ETA: 1s - loss: -181.1434 - acc: 0.01 - ETA: 0s - loss: -159.0918 - acc: 0.01 - ETA: 0s - loss: -141.6915 - acc: 0.01 - 5s 5ms/step - loss: -135.2406 - acc: 0.0201\n",
      "Epoch 200/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8448 - acc: 0.02 - ETA: 3s - loss: -1.1400 - acc: 0.03 - ETA: 3s - loss: -1.3362 - acc: 0.02 - ETA: 2s - loss: -1.2923 - acc: 0.02 - ETA: 2s - loss: -1.7551 - acc: 0.02 - ETA: 1s - loss: -1.9893 - acc: 0.02 - ETA: 1s - loss: -2.0789 - acc: 0.02 - ETA: 0s - loss: -1.9318 - acc: 0.02 - ETA: 0s - loss: -1.7215 - acc: 0.02 - 5s 5ms/step - loss: -38.9822 - acc: 0.0201\n",
      "Epoch 201/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1337.9224 - acc: 0.0000e+0 - ETA: 3s - loss: -669.2586 - acc: 0.0150    - ETA: 3s - loss: -446.9749 - acc: 0.01 - ETA: 2s - loss: -335.4809 - acc: 0.01 - ETA: 2s - loss: -268.6562 - acc: 0.02 - ETA: 1s - loss: -224.0143 - acc: 0.02 - ETA: 1s - loss: -192.5025 - acc: 0.02 - ETA: 0s - loss: -168.5695 - acc: 0.02 - ETA: 0s - loss: -149.9015 - acc: 0.02 - 5s 5ms/step - loss: -203.8769 - acc: 0.0201\n",
      "Epoch 202/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.5785 - acc: 0.03 - ETA: 3s - loss: -1.4948 - acc: 0.02 - ETA: 3s - loss: -2.2178 - acc: 0.01 - ETA: 2s - loss: -2.4401 - acc: 0.01 - ETA: 2s - loss: -1.8469 - acc: 0.01 - ETA: 1s - loss: -1.9144 - acc: 0.02 - ETA: 1s - loss: -1.6787 - acc: 0.02 - ETA: 0s - loss: -1.5996 - acc: 0.02 - ETA: 0s - loss: -1.6186 - acc: 0.02 - 5s 5ms/step - loss: -1.6536 - acc: 0.0201\n",
      "Epoch 203/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.1169 - acc: 0.02 - ETA: 3s - loss: -403.1131 - acc: 0.01 - ETA: 3s - loss: -269.8946 - acc: 0.01 - ETA: 2s - loss: -202.5923 - acc: 0.01 - ETA: 2s - loss: -162.6421 - acc: 0.01 - ETA: 1s - loss: -135.6866 - acc: 0.01 - ETA: 1s - loss: -116.3931 - acc: 0.01 - ETA: 0s - loss: -101.9745 - acc: 0.01 - ETA: 0s - loss: -90.8547 - acc: 0.0200 - 5s 5ms/step - loss: -86.7237 - acc: 0.0201\n",
      "Epoch 204/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.6835 - acc: 0.01 - ETA: 3s - loss: -3.5649 - acc: 0.02 - ETA: 3s - loss: -404.0011 - acc: 0.01 - ETA: 2s - loss: -615.9822 - acc: 0.01 - ETA: 2s - loss: -492.6976 - acc: 0.02 - ETA: 1s - loss: -599.2218 - acc: 0.01 - ETA: 1s - loss: -513.6392 - acc: 0.02 - ETA: 0s - loss: -449.7057 - acc: 0.02 - ETA: 0s - loss: -400.0897 - acc: 0.02 - 5s 5ms/step - loss: -419.1602 - acc: 0.0201\n",
      "Epoch 205/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.3136 - acc: 0.01 - ETA: 3s - loss: -3.0766 - acc: 0.01 - ETA: 3s - loss: -2.2541 - acc: 0.02 - ETA: 2s - loss: -2.4208 - acc: 0.01 - ETA: 2s - loss: -2.3217 - acc: 0.01 - ETA: 1s - loss: -2.0197 - acc: 0.02 - ETA: 1s - loss: -1.7756 - acc: 0.02 - ETA: 0s - loss: -164.8490 - acc: 0.01 - ETA: 0s - loss: -146.6317 - acc: 0.02 - 5s 5ms/step - loss: -140.0035 - acc: 0.0201\n",
      "Epoch 206/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.6229 - acc: 0.01 - ETA: 3s - loss: -1.9176 - acc: 0.01 - ETA: 3s - loss: -1.8412 - acc: 0.01 - ETA: 2s - loss: -1.5427 - acc: 0.01 - ETA: 2s - loss: -1.3484 - acc: 0.02 - ETA: 1s - loss: -1.4452 - acc: 0.02 - ETA: 1s - loss: -1.3615 - acc: 0.02 - ETA: 0s - loss: -156.8586 - acc: 0.01 - ETA: 0s - loss: -139.4635 - acc: 0.02 - 5s 5ms/step - loss: -133.2427 - acc: 0.0201\n",
      "Epoch 207/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.6881 - acc: 0.01 - ETA: 3s - loss: -3.9332 - acc: 0.01 - ETA: 3s - loss: -507.9601 - acc: 0.01 - ETA: 2s - loss: -579.6809 - acc: 0.00 - ETA: 2s - loss: -463.7821 - acc: 0.01 - ETA: 1s - loss: -386.5592 - acc: 0.01 - ETA: 1s - loss: -331.7283 - acc: 0.01 - ETA: 0s - loss: -290.2184 - acc: 0.02 - ETA: 0s - loss: -258.1688 - acc: 0.02 - 5s 5ms/step - loss: -246.4079 - acc: 0.0201\n",
      "Epoch 208/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.2410 - acc: 0.01 - ETA: 3s - loss: -1.6626 - acc: 0.01 - ETA: 3s - loss: -1.3240 - acc: 0.02 - ETA: 2s - loss: -1.1168 - acc: 0.02 - ETA: 2s - loss: -330.4898 - acc: 0.02 - ETA: 1s - loss: -275.5888 - acc: 0.02 - ETA: 1s - loss: -236.2389 - acc: 0.02 - ETA: 0s - loss: -206.9348 - acc: 0.02 - ETA: 0s - loss: -184.3223 - acc: 0.02 - 5s 5ms/step - loss: -213.7155 - acc: 0.0201\n",
      "Epoch 209/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0112 - acc: 0.01 - ETA: 3s - loss: -1.0929 - acc: 0.02 - ETA: 3s - loss: -0.9260 - acc: 0.02 - ETA: 2s - loss: -0.8896 - acc: 0.02 - ETA: 2s - loss: -176.7498 - acc: 0.02 - ETA: 1s - loss: -147.9287 - acc: 0.01 - ETA: 1s - loss: -127.6985 - acc: 0.01 - ETA: 0s - loss: -111.9822 - acc: 0.01 - ETA: 0s - loss: -99.6159 - acc: 0.0189 - 5s 5ms/step - loss: -95.1332 - acc: 0.0201\n",
      "Epoch 210/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.2267 - acc: 0.02 - ETA: 3s - loss: -0.5404 - acc: 0.02 - ETA: 3s - loss: -0.8043 - acc: 0.02 - ETA: 2s - loss: -0.7105 - acc: 0.02 - ETA: 2s - loss: -1.2027 - acc: 0.02 - ETA: 1s - loss: -1.5141 - acc: 0.02 - ETA: 1s - loss: -1.5957 - acc: 0.02 - ETA: 0s - loss: -159.4997 - acc: 0.02 - ETA: 0s - loss: -141.8325 - acc: 0.02 - 5s 5ms/step - loss: -198.5394 - acc: 0.0201\n",
      "Epoch 211/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.5034 - acc: 0.03 - ETA: 3s - loss: -1.1688 - acc: 0.03 - ETA: 3s - loss: -0.9172 - acc: 0.03 - ETA: 2s - loss: -0.8510 - acc: 0.03 - ETA: 2s - loss: -1.0370 - acc: 0.02 - ETA: 1s - loss: -176.9269 - acc: 0.02 - ETA: 1s - loss: -152.1324 - acc: 0.02 - ETA: 0s - loss: -290.7495 - acc: 0.01 - ETA: 0s - loss: -258.5658 - acc: 0.02 - 5s 6ms/step - loss: -246.7544 - acc: 0.0201\n",
      "Epoch 212/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3450 - acc: 0.04 - ETA: 3s - loss: -2.2623 - acc: 0.02 - ETA: 3s - loss: -377.5687 - acc: 0.01 - ETA: 2s - loss: -283.3358 - acc: 0.01 - ETA: 2s - loss: -227.3962 - acc: 0.01 - ETA: 1s - loss: -190.6882 - acc: 0.01 - ETA: 1s - loss: -163.7118 - acc: 0.01 - ETA: 0s - loss: -143.3069 - acc: 0.01 - ETA: 0s - loss: -127.4569 - acc: 0.01 - 5s 5ms/step - loss: -121.6282 - acc: 0.0201\n",
      "Epoch 213/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4302 - acc: 0.03 - ETA: 3s - loss: -0.7933 - acc: 0.03 - ETA: 3s - loss: -1.0171 - acc: 0.03 - ETA: 2s - loss: -1.0116 - acc: 0.03 - ETA: 2s - loss: -1.0321 - acc: 0.03 - ETA: 1s - loss: -174.3466 - acc: 0.02 - ETA: 1s - loss: -149.3816 - acc: 0.02 - ETA: 0s - loss: -130.9329 - acc: 0.02 - ETA: 0s - loss: -261.4853 - acc: 0.02 - 5s 5ms/step - loss: -318.4070 - acc: 0.0201\n",
      "Epoch 214/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2184 - acc: 0.02 - ETA: 3s - loss: -1.6392 - acc: 0.01 - ETA: 3s - loss: -1.6595 - acc: 0.01 - ETA: 2s - loss: -1.5461 - acc: 0.02 - ETA: 2s - loss: -2.2715 - acc: 0.01 - ETA: 1s - loss: -2.0626 - acc: 0.01 - ETA: 1s - loss: -1.8396 - acc: 0.01 - ETA: 0s - loss: -2.1811 - acc: 0.01 - ETA: 0s - loss: -1.9669 - acc: 0.02 - 5s 5ms/step - loss: -1.9589 - acc: 0.0201\n",
      "Epoch 215/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.3876 - acc: 0.01 - ETA: 3s - loss: -2.7666 - acc: 0.02 - ETA: 3s - loss: -302.2864 - acc: 0.01 - ETA: 2s - loss: -227.4168 - acc: 0.01 - ETA: 2s - loss: -182.6672 - acc: 0.01 - ETA: 1s - loss: -152.9197 - acc: 0.01 - ETA: 1s - loss: -131.1576 - acc: 0.01 - ETA: 0s - loss: -114.8373 - acc: 0.01 - ETA: 0s - loss: -102.0779 - acc: 0.01 - 5s 5ms/step - loss: -97.4214 - acc: 0.0201\n",
      "Epoch 216/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1647.1516 - acc: 0.0000e+0 - ETA: 3s - loss: -1372.3144 - acc: 0.0000e+0 - ETA: 3s - loss: -914.8786 - acc: 0.0100    - ETA: 2s - loss: -686.4170 - acc: 0.01 - ETA: 2s - loss: -549.2282 - acc: 0.01 - ETA: 1s - loss: -457.7886 - acc: 0.01 - ETA: 1s - loss: -392.6671 - acc: 0.01 - ETA: 0s - loss: -343.8449 - acc: 0.02 - ETA: 0s - loss: -306.1306 - acc: 0.01 - 5s 5ms/step - loss: -292.1394 - acc: 0.0201\n",
      "Epoch 217/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0059 - acc: 0.03 - ETA: 3s - loss: -0.6677 - acc: 0.03 - ETA: 3s - loss: -0.9903 - acc: 0.03 - ETA: 2s - loss: -1.4445 - acc: 0.02 - ETA: 2s - loss: -1.2726 - acc: 0.02 - ETA: 1s - loss: -177.0050 - acc: 0.02 - ETA: 1s - loss: -151.9769 - acc: 0.02 - ETA: 0s - loss: -133.9924 - acc: 0.01 - ETA: 0s - loss: -119.1588 - acc: 0.02 - 5s 5ms/step - loss: -113.7379 - acc: 0.0201\n",
      "Epoch 218/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1269 - acc: 0.03 - ETA: 3s - loss: -1.8953 - acc: 0.02 - ETA: 3s - loss: -1.7509 - acc: 0.02 - ETA: 2s - loss: -1.5215 - acc: 0.02 - ETA: 2s - loss: -1.7459 - acc: 0.02 - ETA: 1s - loss: -1.6937 - acc: 0.02 - ETA: 1s - loss: -1.6095 - acc: 0.02 - ETA: 0s - loss: -1.8199 - acc: 0.02 - ETA: 0s - loss: -1.6350 - acc: 0.02 - 5s 6ms/step - loss: -56.9396 - acc: 0.0201\n",
      "Epoch 219/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6783 - acc: 0.01 - ETA: 3s - loss: -3.5308 - acc: 0.01 - ETA: 3s - loss: -2.4939 - acc: 0.01 - ETA: 2s - loss: -2.0072 - acc: 0.01 - ETA: 2s - loss: -2.8743 - acc: 0.01 - ETA: 1s - loss: -2.5644 - acc: 0.01 - ETA: 1s - loss: -2.5483 - acc: 0.01 - ETA: 0s - loss: -2.9621 - acc: 0.01 - ETA: 0s - loss: -2.6076 - acc: 0.02 - 5s 5ms/step - loss: -2.4675 - acc: 0.0201\n",
      "Epoch 220/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3856 - acc: 0.03 - ETA: 4s - loss: -0.9868 - acc: 0.03 - ETA: 3s - loss: -0.9428 - acc: 0.02 - ETA: 2s - loss: -0.8286 - acc: 0.03 - ETA: 2s - loss: -0.9373 - acc: 0.02 - ETA: 1s - loss: -210.5974 - acc: 0.02 - ETA: 1s - loss: -318.7610 - acc: 0.01 - ETA: 0s - loss: -279.0934 - acc: 0.02 - ETA: 0s - loss: -372.7964 - acc: 0.02 - 5s 5ms/step - loss: -355.8313 - acc: 0.0201\n",
      "Epoch 221/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.6509 - acc: 0.02 - ETA: 3s - loss: -2.3198 - acc: 0.01 - ETA: 3s - loss: -446.3519 - acc: 0.01 - ETA: 2s - loss: -334.7743 - acc: 0.01 - ETA: 2s - loss: -477.0176 - acc: 0.01 - ETA: 1s - loss: -398.2956 - acc: 0.01 - ETA: 1s - loss: -341.4261 - acc: 0.01 - ETA: 0s - loss: -298.7907 - acc: 0.02 - ETA: 0s - loss: -265.6377 - acc: 0.02 - 5s 5ms/step - loss: -335.4971 - acc: 0.0201\n",
      "Epoch 222/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9992 - acc: 0.01 - ETA: 3s - loss: -0.7937 - acc: 0.01 - ETA: 3s - loss: -267.8291 - acc: 0.00 - ETA: 2s - loss: -201.0296 - acc: 0.01 - ETA: 2s - loss: -161.2788 - acc: 0.01 - ETA: 1s - loss: -135.0312 - acc: 0.01 - ETA: 1s - loss: -115.7778 - acc: 0.01 - ETA: 0s - loss: -101.8906 - acc: 0.01 - ETA: 0s - loss: -90.6162 - acc: 0.0189 - 5s 5ms/step - loss: -86.5497 - acc: 0.0201\n",
      "Epoch 223/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0637 - acc: 0.03 - ETA: 3s - loss: -1.6497 - acc: 0.02 - ETA: 3s - loss: -1.4757 - acc: 0.02 - ETA: 2s - loss: -245.7032 - acc: 0.01 - ETA: 2s - loss: -196.6618 - acc: 0.01 - ETA: 1s - loss: -165.1715 - acc: 0.01 - ETA: 1s - loss: -141.7329 - acc: 0.01 - ETA: 0s - loss: -123.9722 - acc: 0.02 - ETA: 0s - loss: -110.7393 - acc: 0.02 - 5s 5ms/step - loss: -105.7666 - acc: 0.0201\n",
      "Epoch 224/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0298 - acc: 0.03 - ETA: 3s - loss: -1.2207 - acc: 0.02 - ETA: 3s - loss: -0.9461 - acc: 0.02 - ETA: 2s - loss: -2.1268 - acc: 0.02 - ETA: 2s - loss: -6.2087 - acc: 0.02 - ETA: 1s - loss: -5.3456 - acc: 0.02 - ETA: 1s - loss: -4.9623 - acc: 0.01 - ETA: 0s - loss: -4.9647 - acc: 0.01 - ETA: 0s - loss: -4.4281 - acc: 0.02 - 5s 5ms/step - loss: -4.2500 - acc: 0.0201\n",
      "Epoch 225/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7740 - acc: 0.03 - ETA: 3s - loss: -0.9011 - acc: 0.02 - ETA: 3s - loss: -1.0137 - acc: 0.02 - ETA: 2s - loss: -0.8526 - acc: 0.02 - ETA: 2s - loss: -1.8320 - acc: 0.02 - ETA: 1s - loss: -187.8908 - acc: 0.02 - ETA: 1s - loss: -161.3580 - acc: 0.02 - ETA: 0s - loss: -141.4232 - acc: 0.02 - ETA: 0s - loss: -125.7106 - acc: 0.02 - 5s 5ms/step - loss: -175.5666 - acc: 0.0201\n",
      "Epoch 226/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1077 - acc: 0.02 - ETA: 3s - loss: -1.2346 - acc: 0.02 - ETA: 3s - loss: -1.2755 - acc: 0.02 - ETA: 2s - loss: -1.4262 - acc: 0.02 - ETA: 2s - loss: -1.7813 - acc: 0.02 - ETA: 1s - loss: -1.5107 - acc: 0.02 - ETA: 1s - loss: -211.1336 - acc: 0.02 - ETA: 0s - loss: -184.7105 - acc: 0.02 - ETA: 0s - loss: -164.3557 - acc: 0.02 - 5s 5ms/step - loss: -177.3923 - acc: 0.0201\n",
      "Epoch 227/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8034 - acc: 0.02 - ETA: 3s - loss: -736.4021 - acc: 0.01 - ETA: 3s - loss: -889.2530 - acc: 0.00 - ETA: 2s - loss: -666.9611 - acc: 0.01 - ETA: 2s - loss: -533.7717 - acc: 0.01 - ETA: 1s - loss: -618.4512 - acc: 0.01 - ETA: 1s - loss: -530.2028 - acc: 0.01 - ETA: 0s - loss: -464.2601 - acc: 0.01 - ETA: 0s - loss: -412.7018 - acc: 0.01 - 5s 5ms/step - loss: -393.8998 - acc: 0.0201\n",
      "Epoch 228/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4165 - acc: 0.03 - ETA: 3s - loss: -1.1959 - acc: 0.02 - ETA: 3s - loss: -1.1373 - acc: 0.02 - ETA: 2s - loss: -0.7446 - acc: 0.03 - ETA: 2s - loss: -1.7709 - acc: 0.02 - ETA: 1s - loss: -1.7139 - acc: 0.02 - ETA: 1s - loss: -2.1388 - acc: 0.02 - ETA: 0s - loss: -2.0474 - acc: 0.02 - ETA: 0s - loss: -2.2777 - acc: 0.02 - 5s 5ms/step - loss: -2.1863 - acc: 0.0201\n",
      "Epoch 229/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8145 - acc: 0.02 - ETA: 3s - loss: -1.6483 - acc: 0.03 - ETA: 3s - loss: -1.0230 - acc: 0.03 - ETA: 2s - loss: -1.9074 - acc: 0.02 - ETA: 2s - loss: -1.5772 - acc: 0.02 - ETA: 1s - loss: -1.7368 - acc: 0.02 - ETA: 1s - loss: -2.0139 - acc: 0.02 - ETA: 0s - loss: -2.3025 - acc: 0.02 - ETA: 0s - loss: -2.5907 - acc: 0.01 - 5s 5ms/step - loss: -2.4407 - acc: 0.0201\n",
      "Epoch 230/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6045 - acc: 0.04 - ETA: 3s - loss: -0.2040 - acc: 0.04 - ETA: 3s - loss: -0.8673 - acc: 0.03 - ETA: 2s - loss: -5.5926 - acc: 0.02 - ETA: 2s - loss: -5.2185 - acc: 0.02 - ETA: 1s - loss: -4.8733 - acc: 0.02 - ETA: 1s - loss: -4.9143 - acc: 0.02 - ETA: 0s - loss: -5.3365 - acc: 0.01 - ETA: 0s - loss: -4.7992 - acc: 0.02 - 5s 5ms/step - loss: -4.6382 - acc: 0.0201\n",
      "Epoch 231/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.0829 - acc: 0.01 - ETA: 3s - loss: -1.6086 - acc: 0.01 - ETA: 3s - loss: -1.7042 - acc: 0.01 - ETA: 2s - loss: -1.4766 - acc: 0.02 - ETA: 2s - loss: -360.3872 - acc: 0.01 - ETA: 1s - loss: -300.3520 - acc: 0.01 - ETA: 1s - loss: -257.4378 - acc: 0.02 - ETA: 0s - loss: -225.5590 - acc: 0.01 - ETA: 0s - loss: -200.6653 - acc: 0.01 - 5s 5ms/step - loss: -191.5397 - acc: 0.0201\n",
      "Epoch 232/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8882 - acc: 0.03 - ETA: 3s - loss: -0.6496 - acc: 0.03 - ETA: 3s - loss: -0.2362 - acc: 0.03 - ETA: 2s - loss: -0.2389 - acc: 0.03 - ETA: 2s - loss: -5.0049 - acc: 0.02 - ETA: 1s - loss: -4.4835 - acc: 0.02 - ETA: 1s - loss: -161.9198 - acc: 0.02 - ETA: 0s - loss: -142.0123 - acc: 0.02 - ETA: 0s - loss: -126.9887 - acc: 0.02 - 5s 5ms/step - loss: -121.2558 - acc: 0.0201\n",
      "Epoch 233/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3551 - acc: 0.02 - ETA: 3s - loss: -2.2960 - acc: 0.01 - ETA: 3s - loss: -1.4426 - acc: 0.02 - ETA: 2s - loss: -1.6391 - acc: 0.02 - ETA: 2s - loss: -1.5158 - acc: 0.02 - ETA: 1s - loss: -1.7893 - acc: 0.02 - ETA: 1s - loss: -2.0393 - acc: 0.02 - ETA: 0s - loss: -1.9398 - acc: 0.02 - ETA: 0s - loss: -1.8119 - acc: 0.02 - 5s 5ms/step - loss: -1.7415 - acc: 0.0201\n",
      "Epoch 234/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6319 - acc: 0.02 - ETA: 3s - loss: -0.9576 - acc: 0.02 - ETA: 3s - loss: -0.6944 - acc: 0.02 - ETA: 2s - loss: -1.8437 - acc: 0.02 - ETA: 2s - loss: -2.2094 - acc: 0.01 - ETA: 1s - loss: -2.4045 - acc: 0.01 - ETA: 1s - loss: -2.1835 - acc: 0.01 - ETA: 0s - loss: -1.9972 - acc: 0.02 - ETA: 0s - loss: -2.0757 - acc: 0.01 - 5s 5ms/step - loss: -2.0184 - acc: 0.0201\n",
      "Epoch 235/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -1.5447 - acc: 0.02 - ETA: 3s - loss: -1.0003 - acc: 0.02 - ETA: 3s - loss: -1.3095 - acc: 0.02 - ETA: 2s - loss: -223.8497 - acc: 0.01 - ETA: 2s - loss: -434.2055 - acc: 0.01 - ETA: 1s - loss: -361.9431 - acc: 0.01 - ETA: 1s - loss: -310.4578 - acc: 0.01 - ETA: 0s - loss: -271.6392 - acc: 0.02 - ETA: 0s - loss: -241.6203 - acc: 0.02 - 5s 5ms/step - loss: -307.1458 - acc: 0.0201\n",
      "Epoch 236/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.4205 - acc: 0.01 - ETA: 3s - loss: -1.7430 - acc: 0.02 - ETA: 3s - loss: -1.6112 - acc: 0.02 - ETA: 2s - loss: -318.4709 - acc: 0.01 - ETA: 2s - loss: -254.9160 - acc: 0.01 - ETA: 1s - loss: -212.7255 - acc: 0.01 - ETA: 1s - loss: -182.4791 - acc: 0.02 - ETA: 0s - loss: -160.2581 - acc: 0.01 - ETA: 0s - loss: -142.5397 - acc: 0.02 - 5s 5ms/step - loss: -136.0190 - acc: 0.0201\n",
      "Epoch 237/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9986 - acc: 0.01 - ETA: 3s - loss: -0.5951 - acc: 0.02 - ETA: 3s - loss: -1.9474 - acc: 0.02 - ETA: 2s - loss: -2.3670 - acc: 0.01 - ETA: 2s - loss: -1.9811 - acc: 0.01 - ETA: 1s - loss: -2.0734 - acc: 0.01 - ETA: 1s - loss: -2.0301 - acc: 0.02 - ETA: 0s - loss: -1.8602 - acc: 0.02 - ETA: 0s - loss: -1.9105 - acc: 0.02 - 5s 5ms/step - loss: -1.8360 - acc: 0.0201\n",
      "Epoch 238/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8631 - acc: 0.02 - ETA: 3s - loss: -656.9597 - acc: 0.01 - ETA: 3s - loss: -438.3951 - acc: 0.01 - ETA: 2s - loss: -329.6978 - acc: 0.01 - ETA: 2s - loss: -264.0165 - acc: 0.01 - ETA: 1s - loss: -221.1898 - acc: 0.01 - ETA: 1s - loss: -189.7682 - acc: 0.01 - ETA: 0s - loss: -166.0123 - acc: 0.01 - ETA: 0s - loss: -148.6372 - acc: 0.01 - 5s 5ms/step - loss: -141.8766 - acc: 0.0201\n",
      "Epoch 239/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.3643 - acc: 0.01 - ETA: 3s - loss: -2.7427 - acc: 0.01 - ETA: 3s - loss: -2.9768 - acc: 0.01 - ETA: 2s - loss: -2.3517 - acc: 0.01 - ETA: 2s - loss: -1.9303 - acc: 0.01 - ETA: 1s - loss: -2.2698 - acc: 0.01 - ETA: 1s - loss: -2.1156 - acc: 0.01 - ETA: 0s - loss: -3.9120 - acc: 0.01 - ETA: 0s - loss: -3.5650 - acc: 0.02 - 5s 5ms/step - loss: -3.3963 - acc: 0.0201\n",
      "Epoch 240/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7173 - acc: 0.04 - ETA: 3s - loss: -1.0308 - acc: 0.03 - ETA: 3s - loss: -1.1422 - acc: 0.03 - ETA: 2s - loss: -1.3715 - acc: 0.03 - ETA: 2s - loss: -1.0851 - acc: 0.03 - ETA: 1s - loss: -236.3998 - acc: 0.02 - ETA: 1s - loss: -332.6613 - acc: 0.02 - ETA: 0s - loss: -291.1563 - acc: 0.02 - ETA: 0s - loss: -258.9805 - acc: 0.02 - 5s 5ms/step - loss: -247.2280 - acc: 0.0201\n",
      "Epoch 241/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.0327 - acc: 0.01 - ETA: 3s - loss: -12.4099 - acc: 0.010 - ETA: 3s - loss: -9.0144 - acc: 0.010 - ETA: 2s - loss: -6.9927 - acc: 0.01 - ETA: 2s - loss: -5.5841 - acc: 0.02 - ETA: 1s - loss: -4.7420 - acc: 0.02 - ETA: 1s - loss: -4.5782 - acc: 0.02 - ETA: 0s - loss: -125.6764 - acc: 0.02 - ETA: 0s - loss: -111.7824 - acc: 0.02 - 5s 5ms/step - loss: -145.0104 - acc: 0.0201\n",
      "Epoch 242/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1044.0398 - acc: 0.0000e+0 - ETA: 3s - loss: -521.9962 - acc: 0.0150    - ETA: 3s - loss: -348.1268 - acc: 0.02 - ETA: 2s - loss: -262.1985 - acc: 0.02 - ETA: 2s - loss: -433.7078 - acc: 0.01 - ETA: 1s - loss: -362.5677 - acc: 0.01 - ETA: 1s - loss: -310.9563 - acc: 0.01 - ETA: 0s - loss: -272.2799 - acc: 0.01 - ETA: 0s - loss: -242.0750 - acc: 0.02 - 5s 5ms/step - loss: -306.1157 - acc: 0.0201\n",
      "Epoch 243/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1789 - acc: 0.03 - ETA: 3s - loss: -0.4094 - acc: 0.03 - ETA: 3s - loss: -0.5910 - acc: 0.03 - ETA: 2s - loss: -0.8578 - acc: 0.02 - ETA: 2s - loss: -0.7224 - acc: 0.03 - ETA: 1s - loss: -212.9316 - acc: 0.02 - ETA: 1s - loss: -182.6799 - acc: 0.02 - ETA: 0s - loss: -283.9246 - acc: 0.02 - ETA: 0s - loss: -391.6377 - acc: 0.02 - 5s 5ms/step - loss: -436.5963 - acc: 0.0201\n",
      "Epoch 244/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1415.2073 - acc: 0.0000e+0 - ETA: 3s - loss: -707.5442 - acc: 0.0100    - ETA: 3s - loss: -472.8076 - acc: 0.01 - ETA: 2s - loss: -354.7792 - acc: 0.01 - ETA: 2s - loss: -548.2011 - acc: 0.01 - ETA: 1s - loss: -456.8459 - acc: 0.01 - ETA: 1s - loss: -391.7168 - acc: 0.01 - ETA: 0s - loss: -342.9701 - acc: 0.01 - ETA: 0s - loss: -304.8588 - acc: 0.02 - 5s 5ms/step - loss: -291.0673 - acc: 0.0201\n",
      "Epoch 245/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.1135 - acc: 0.01 - ETA: 3s - loss: -1.7652 - acc: 0.02 - ETA: 3s - loss: -2.0921 - acc: 0.02 - ETA: 2s - loss: -1.6662 - acc: 0.02 - ETA: 2s - loss: -4.8633 - acc: 0.02 - ETA: 1s - loss: -4.1157 - acc: 0.02 - ETA: 1s - loss: -4.0465 - acc: 0.02 - ETA: 0s - loss: -3.9997 - acc: 0.02 - ETA: 0s - loss: -3.7147 - acc: 0.02 - 5s 5ms/step - loss: -32.8186 - acc: 0.0201\n",
      "Epoch 246/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.8136 - acc: 0.02 - ETA: 3s - loss: -1.8675 - acc: 0.02 - ETA: 3s - loss: -2.8198 - acc: 0.01 - ETA: 2s - loss: -246.3599 - acc: 0.01 - ETA: 2s - loss: -197.2226 - acc: 0.01 - ETA: 1s - loss: -164.4342 - acc: 0.01 - ETA: 1s - loss: -141.0653 - acc: 0.01 - ETA: 0s - loss: -123.6083 - acc: 0.02 - ETA: 0s - loss: -228.8462 - acc: 0.01 - 5s 5ms/step - loss: -218.3956 - acc: 0.0201\n",
      "Epoch 247/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6456 - acc: 0.02 - ETA: 3s - loss: -2.9718 - acc: 0.01 - ETA: 3s - loss: -2.0389 - acc: 0.02 - ETA: 2s - loss: -3.1565 - acc: 0.02 - ETA: 2s - loss: -2.8667 - acc: 0.02 - ETA: 1s - loss: -2.6911 - acc: 0.02 - ETA: 1s - loss: -168.2844 - acc: 0.01 - ETA: 0s - loss: -147.2211 - acc: 0.02 - ETA: 0s - loss: -131.1868 - acc: 0.02 - 5s 5ms/step - loss: -125.1892 - acc: 0.0201\n",
      "Epoch 248/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.7859 - acc: 0.01 - ETA: 3s - loss: -1.3974 - acc: 0.01 - ETA: 3s - loss: -3.1399 - acc: 0.01 - ETA: 2s - loss: -2.4071 - acc: 0.01 - ETA: 2s - loss: -2.8795 - acc: 0.01 - ETA: 1s - loss: -2.3219 - acc: 0.01 - ETA: 1s - loss: -2.1339 - acc: 0.01 - ETA: 0s - loss: -2.0413 - acc: 0.01 - ETA: 0s - loss: -1.9904 - acc: 0.02 - 5s 5ms/step - loss: -2.0322 - acc: 0.0201\n",
      "Epoch 249/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3725 - acc: 0.02 - ETA: 3s - loss: -0.8030 - acc: 0.02 - ETA: 3s - loss: -0.7566 - acc: 0.01 - ETA: 2s - loss: -1.4102 - acc: 0.01 - ETA: 2s - loss: -1.7077 - acc: 0.01 - ETA: 1s - loss: -1.5320 - acc: 0.02 - ETA: 1s - loss: -1.5479 - acc: 0.02 - ETA: 0s - loss: -1.4971 - acc: 0.02 - ETA: 0s - loss: -1.6820 - acc: 0.02 - 5s 5ms/step - loss: -17.7506 - acc: 0.0201\n",
      "Epoch 250/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3339 - acc: 0.03 - ETA: 3s - loss: -1.5047 - acc: 0.02 - ETA: 3s - loss: -2.3857 - acc: 0.01 - ETA: 2s - loss: -1.8354 - acc: 0.02 - ETA: 2s - loss: -2.5418 - acc: 0.02 - ETA: 1s - loss: -2.4877 - acc: 0.01 - ETA: 1s - loss: -2.3800 - acc: 0.01 - ETA: 0s - loss: -2.3172 - acc: 0.01 - ETA: 0s - loss: -2.0464 - acc: 0.02 - 5s 5ms/step - loss: -64.6982 - acc: 0.0201\n",
      "Epoch 251/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.0650 - acc: 0.020 - ETA: 3s - loss: -0.8783 - acc: 0.01 - ETA: 3s - loss: -1.4668 - acc: 0.01 - ETA: 2s - loss: -1.1990 - acc: 0.01 - ETA: 2s - loss: -1.2476 - acc: 0.02 - ETA: 1s - loss: -1.2292 - acc: 0.02 - ETA: 1s - loss: -1.0623 - acc: 0.02 - ETA: 0s - loss: -1.4551 - acc: 0.02 - ETA: 0s - loss: -157.6889 - acc: 0.02 - 5s 5ms/step - loss: -189.3863 - acc: 0.0201\n",
      "Epoch 252/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.5831 - acc: 0.04 - ETA: 3s - loss: -1.5598 - acc: 0.03 - ETA: 3s - loss: -1.0454 - acc: 0.03 - ETA: 2s - loss: -1.1324 - acc: 0.02 - ETA: 2s - loss: -238.4449 - acc: 0.02 - ETA: 1s - loss: -441.1517 - acc: 0.01 - ETA: 1s - loss: -378.3685 - acc: 0.01 - ETA: 0s - loss: -331.0559 - acc: 0.02 - ETA: 0s - loss: -294.6815 - acc: 0.01 - 5s 5ms/step - loss: -281.2154 - acc: 0.0201\n",
      "Epoch 253/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9325 - acc: 0.03 - ETA: 3s - loss: -1.1424 - acc: 0.02 - ETA: 3s - loss: -1.5707 - acc: 0.02 - ETA: 2s - loss: -1.6906 - acc: 0.02 - ETA: 2s - loss: -1.7895 - acc: 0.02 - ETA: 1s - loss: -1.7226 - acc: 0.02 - ETA: 1s - loss: -1.5541 - acc: 0.02 - ETA: 0s - loss: -1.4889 - acc: 0.01 - ETA: 0s - loss: -1.5298 - acc: 0.02 - 5s 5ms/step - loss: -1.7848 - acc: 0.0201\n",
      "Epoch 254/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3060 - acc: 0.02 - ETA: 3s - loss: -1.4443 - acc: 0.02 - ETA: 3s - loss: -1.0727 - acc: 0.02 - ETA: 2s - loss: -0.7576 - acc: 0.02 - ETA: 2s - loss: -0.7848 - acc: 0.02 - ETA: 1s - loss: -285.2320 - acc: 0.02 - ETA: 1s - loss: -244.7135 - acc: 0.02 - ETA: 0s - loss: -214.1314 - acc: 0.02 - ETA: 0s - loss: -190.5876 - acc: 0.02 - 5s 5ms/step - loss: -257.1214 - acc: 0.0201\n",
      "Epoch 255/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6078 - acc: 0.03 - ETA: 3s - loss: -529.1400 - acc: 0.01 - ETA: 3s - loss: -352.7064 - acc: 0.01 - ETA: 2s - loss: -264.6889 - acc: 0.02 - ETA: 2s - loss: -211.8235 - acc: 0.02 - ETA: 1s - loss: -176.8680 - acc: 0.02 - ETA: 1s - loss: -342.1042 - acc: 0.02 - ETA: 0s - loss: -299.5610 - acc: 0.02 - ETA: 0s - loss: -266.4016 - acc: 0.02 - 5s 5ms/step - loss: -301.9718 - acc: 0.0201\n",
      "Epoch 256/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3606 - acc: 0.03 - ETA: 3s - loss: -0.8310 - acc: 0.02 - ETA: 3s - loss: -1.5004 - acc: 0.02 - ETA: 2s - loss: -1.6971 - acc: 0.02 - ETA: 2s - loss: -1.7099 - acc: 0.02 - ETA: 1s - loss: -1.6311 - acc: 0.02 - ETA: 1s - loss: -1.3705 - acc: 0.02 - ETA: 0s - loss: -1.5923 - acc: 0.02 - ETA: 0s - loss: -125.2302 - acc: 0.02 - 5s 5ms/step - loss: -119.5323 - acc: 0.0201\n",
      "Epoch 257/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.1277 - acc: 0.040 - ETA: 3s - loss: -1.5111 - acc: 0.02 - ETA: 3s - loss: -425.8477 - acc: 0.01 - ETA: 2s - loss: -319.3717 - acc: 0.02 - ETA: 2s - loss: -255.9531 - acc: 0.02 - ETA: 1s - loss: -213.9152 - acc: 0.02 - ETA: 1s - loss: -391.3409 - acc: 0.01 - ETA: 0s - loss: -342.4748 - acc: 0.01 - ETA: 0s - loss: -304.6950 - acc: 0.02 - 5s 5ms/step - loss: -290.7953 - acc: 0.0201\n",
      "Epoch 258/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4034 - acc: 0.01 - ETA: 3s - loss: -2.5018 - acc: 0.01 - ETA: 3s - loss: -468.9815 - acc: 0.00 - ETA: 2s - loss: -351.9098 - acc: 0.01 - ETA: 2s - loss: -281.5917 - acc: 0.01 - ETA: 1s - loss: -234.8426 - acc: 0.01 - ETA: 1s - loss: -201.3929 - acc: 0.01 - ETA: 0s - loss: -176.2420 - acc: 0.02 - ETA: 0s - loss: -157.1758 - acc: 0.02 - 5s 5ms/step - loss: -197.3657 - acc: 0.0201\n",
      "Epoch 259/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0127 - acc: 0.04 - ETA: 3s - loss: -1.8077 - acc: 0.02 - ETA: 3s - loss: -1.2093 - acc: 0.03 - ETA: 2s - loss: -1.9361 - acc: 0.02 - ETA: 2s - loss: -2.7240 - acc: 0.02 - ETA: 1s - loss: -2.3570 - acc: 0.02 - ETA: 1s - loss: -4.7006 - acc: 0.02 - ETA: 0s - loss: -4.2251 - acc: 0.02 - ETA: 0s - loss: -145.2595 - acc: 0.01 - 5s 5ms/step - loss: -138.6428 - acc: 0.0201\n",
      "Epoch 260/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2054 - acc: 0.02 - ETA: 3s - loss: -0.1115 - acc: 0.03 - ETA: 3s - loss: -449.3436 - acc: 0.02 - ETA: 2s - loss: -337.9421 - acc: 0.02 - ETA: 2s - loss: -270.3416 - acc: 0.02 - ETA: 1s - loss: -225.6748 - acc: 0.02 - ETA: 1s - loss: -193.5530 - acc: 0.02 - ETA: 0s - loss: -169.5669 - acc: 0.01 - ETA: 0s - loss: -150.9522 - acc: 0.02 - 5s 5ms/step - loss: -144.2069 - acc: 0.0201\n",
      "Epoch 261/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.8755 - acc: 0.02 - ETA: 3s - loss: -2.8950 - acc: 0.02 - ETA: 3s - loss: -2.7711 - acc: 0.01 - ETA: 2s - loss: -2.1289 - acc: 0.01 - ETA: 2s - loss: -1.8527 - acc: 0.01 - ETA: 1s - loss: -1.6806 - acc: 0.02 - ETA: 1s - loss: -1.8906 - acc: 0.01 - ETA: 0s - loss: -1.9130 - acc: 0.01 - ETA: 0s - loss: -110.4209 - acc: 0.01 - 5s 5ms/step - loss: -105.3948 - acc: 0.0201\n",
      "Epoch 262/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.7630 - acc: 0.01 - ETA: 3s - loss: -3.9015 - acc: 0.01 - ETA: 3s - loss: -2.6455 - acc: 0.02 - ETA: 2s - loss: -2.6734 - acc: 0.02 - ETA: 2s - loss: -2.3359 - acc: 0.02 - ETA: 1s - loss: -2.1855 - acc: 0.02 - ETA: 1s - loss: -1.8596 - acc: 0.02 - ETA: 0s - loss: -124.4279 - acc: 0.02 - ETA: 0s - loss: -110.9520 - acc: 0.02 - 5s 5ms/step - loss: -105.9048 - acc: 0.0201\n",
      "Epoch 263/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6639 - acc: 0.01 - ETA: 3s - loss: -1.4199 - acc: 0.01 - ETA: 3s - loss: -0.9701 - acc: 0.02 - ETA: 2s - loss: -1.3141 - acc: 0.02 - ETA: 2s - loss: -1.6814 - acc: 0.02 - ETA: 1s - loss: -1.5675 - acc: 0.02 - ETA: 1s - loss: -1.3679 - acc: 0.02 - ETA: 0s - loss: -186.4685 - acc: 0.02 - ETA: 0s - loss: -165.9642 - acc: 0.02 - 5s 5ms/step - loss: -158.3913 - acc: 0.0201\n",
      "Epoch 264/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.9548 - acc: 0.02 - ETA: 3s - loss: -2.4129 - acc: 0.02 - ETA: 3s - loss: -1.9019 - acc: 0.02 - ETA: 2s - loss: -1.9771 - acc: 0.02 - ETA: 2s - loss: -1.6297 - acc: 0.02 - ETA: 1s - loss: -1.8041 - acc: 0.02 - ETA: 1s - loss: -1.7235 - acc: 0.02 - ETA: 0s - loss: -1.8417 - acc: 0.02 - ETA: 0s - loss: -1.6956 - acc: 0.02 - 5s 5ms/step - loss: -21.9193 - acc: 0.0201\n",
      "Epoch 265/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1385 - acc: 0.02 - ETA: 3s - loss: -1.0025 - acc: 0.02 - ETA: 3s - loss: -0.8323 - acc: 0.02 - ETA: 2s - loss: -0.6899 - acc: 0.02 - ETA: 2s - loss: -0.9036 - acc: 0.02 - ETA: 1s - loss: -147.2138 - acc: 0.02 - ETA: 1s - loss: -126.3247 - acc: 0.02 - ETA: 0s - loss: -110.7552 - acc: 0.02 - ETA: 0s - loss: -98.8238 - acc: 0.0211 - 5s 5ms/step - loss: -157.4397 - acc: 0.0201\n",
      "Epoch 266/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6557 - acc: 0.03 - ETA: 3s - loss: -0.9714 - acc: 0.02 - ETA: 3s - loss: -0.9832 - acc: 0.02 - ETA: 2s - loss: -0.7397 - acc: 0.02 - ETA: 2s - loss: -338.3317 - acc: 0.02 - ETA: 1s - loss: -282.5516 - acc: 0.01 - ETA: 1s - loss: -242.3026 - acc: 0.02 - ETA: 0s - loss: -212.0805 - acc: 0.02 - ETA: 0s - loss: -188.6580 - acc: 0.02 - 5s 5ms/step - loss: -235.4535 - acc: 0.0201\n",
      "Epoch 267/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5262 - acc: 0.03 - ETA: 3s - loss: -1.4782 - acc: 0.02 - ETA: 3s - loss: -1.1278 - acc: 0.02 - ETA: 2s - loss: -300.3827 - acc: 0.01 - ETA: 2s - loss: -240.2168 - acc: 0.02 - ETA: 1s - loss: -201.0347 - acc: 0.02 - ETA: 1s - loss: -172.7742 - acc: 0.02 - ETA: 0s - loss: -151.4307 - acc: 0.02 - ETA: 0s - loss: -134.5930 - acc: 0.02 - 5s 5ms/step - loss: -198.8086 - acc: 0.0201\n",
      "Epoch 268/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3757 - acc: 0.02 - ETA: 3s - loss: -0.2914 - acc: 0.02 - ETA: 3s - loss: -0.3084 - acc: 0.02 - ETA: 2s - loss: -1.8257 - acc: 0.02 - ETA: 2s - loss: -1.7741 - acc: 0.02 - ETA: 1s - loss: -237.0225 - acc: 0.01 - ETA: 1s - loss: -203.2738 - acc: 0.02 - ETA: 0s - loss: -178.1135 - acc: 0.02 - ETA: 0s - loss: -158.3899 - acc: 0.02 - 5s 5ms/step - loss: -206.9024 - acc: 0.0201\n",
      "Epoch 269/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.6683 - acc: 0.02 - ETA: 3s - loss: -0.8832 - acc: 0.02 - ETA: 3s - loss: -0.7596 - acc: 0.02 - ETA: 2s - loss: -1.0960 - acc: 0.02 - ETA: 2s - loss: -1.0195 - acc: 0.02 - ETA: 1s - loss: -1.2212 - acc: 0.02 - ETA: 1s - loss: -1.5198 - acc: 0.02 - ETA: 0s - loss: -1.6447 - acc: 0.02 - ETA: 0s - loss: -68.1253 - acc: 0.020 - 5s 5ms/step - loss: -65.0625 - acc: 0.0201\n",
      "Epoch 270/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9537 - acc: 0.02 - ETA: 3s - loss: -2.7858 - acc: 0.01 - ETA: 3s - loss: -2.5937 - acc: 0.01 - ETA: 2s - loss: -2.7919 - acc: 0.01 - ETA: 2s - loss: -2.5389 - acc: 0.01 - ETA: 1s - loss: -2.2725 - acc: 0.02 - ETA: 1s - loss: -131.0342 - acc: 0.01 - ETA: 0s - loss: -114.7260 - acc: 0.01 - ETA: 0s - loss: -101.9985 - acc: 0.02 - 5s 5ms/step - loss: -97.3995 - acc: 0.0201\n",
      "Epoch 271/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -1117.9272 - acc: 0.0000e+0 - ETA: 3s - loss: -559.1411 - acc: 0.0100    - ETA: 3s - loss: -372.6370 - acc: 0.01 - ETA: 2s - loss: -679.9395 - acc: 0.01 - ETA: 2s - loss: -544.8983 - acc: 0.01 - ETA: 1s - loss: -454.3066 - acc: 0.01 - ETA: 1s - loss: -389.6209 - acc: 0.01 - ETA: 0s - loss: -341.1335 - acc: 0.01 - ETA: 0s - loss: -303.2358 - acc: 0.01 - 5s 5ms/step - loss: -289.4156 - acc: 0.0201\n",
      "Epoch 272/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4846 - acc: 0.01 - ETA: 3s - loss: -1.5598 - acc: 0.01 - ETA: 3s - loss: -1.5229 - acc: 0.01 - ETA: 2s - loss: -1.6840 - acc: 0.01 - ETA: 2s - loss: -2.3720 - acc: 0.01 - ETA: 1s - loss: -2.2961 - acc: 0.01 - ETA: 1s - loss: -2.5117 - acc: 0.01 - ETA: 0s - loss: -105.0152 - acc: 0.01 - ETA: 0s - loss: -93.3482 - acc: 0.0200 - 5s 5ms/step - loss: -89.0706 - acc: 0.0201\n",
      "Epoch 273/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.8782 - acc: 0.01 - ETA: 3s - loss: -2.3644 - acc: 0.02 - ETA: 3s - loss: -3.0948 - acc: 0.01 - ETA: 2s - loss: -3.0155 - acc: 0.01 - ETA: 2s - loss: -2.4837 - acc: 0.02 - ETA: 1s - loss: -2.1633 - acc: 0.02 - ETA: 1s - loss: -2.0010 - acc: 0.01 - ETA: 0s - loss: -1.7787 - acc: 0.01 - ETA: 0s - loss: -1.7413 - acc: 0.02 - 5s 5ms/step - loss: -1.7444 - acc: 0.0201\n",
      "Epoch 274/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.3206 - acc: 0.01 - ETA: 3s - loss: -2.6173 - acc: 0.02 - ETA: 3s - loss: -2.4848 - acc: 0.02 - ETA: 2s - loss: -2.9562 - acc: 0.01 - ETA: 2s - loss: -2.5682 - acc: 0.01 - ETA: 1s - loss: -2.3018 - acc: 0.01 - ETA: 1s - loss: -2.0371 - acc: 0.02 - ETA: 0s - loss: -2.2073 - acc: 0.02 - ETA: 0s - loss: -2.1606 - acc: 0.02 - 5s 5ms/step - loss: -2.0906 - acc: 0.0201\n",
      "Epoch 275/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -979.0508 - acc: 0.0000e+ - ETA: 3s - loss: -490.4920 - acc: 0.0100   - ETA: 3s - loss: -328.5579 - acc: 0.01 - ETA: 2s - loss: -246.6950 - acc: 0.01 - ETA: 2s - loss: -324.9470 - acc: 0.01 - ETA: 1s - loss: -271.0228 - acc: 0.01 - ETA: 1s - loss: -232.4157 - acc: 0.01 - ETA: 0s - loss: -203.5980 - acc: 0.01 - ETA: 0s - loss: -181.1601 - acc: 0.01 - 5s 5ms/step - loss: -172.8991 - acc: 0.0201\n",
      "Epoch 276/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1269.4723 - acc: 0.0000e+0 - ETA: 3s - loss: -635.7343 - acc: 0.0150    - ETA: 3s - loss: -424.1614 - acc: 0.01 - ETA: 2s - loss: -318.5814 - acc: 0.01 - ETA: 2s - loss: -254.9977 - acc: 0.02 - ETA: 1s - loss: -212.8711 - acc: 0.02 - ETA: 1s - loss: -182.5182 - acc: 0.02 - ETA: 0s - loss: -159.9806 - acc: 0.02 - ETA: 0s - loss: -142.2545 - acc: 0.02 - 5s 5ms/step - loss: -191.1629 - acc: 0.0201\n",
      "Epoch 277/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -802.8710 - acc: 0.0000e+ - ETA: 3s - loss: -401.8789 - acc: 0.0150   - ETA: 3s - loss: -268.4029 - acc: 0.02 - ETA: 2s - loss: -201.2597 - acc: 0.03 - ETA: 2s - loss: -161.9233 - acc: 0.02 - ETA: 1s - loss: -135.6707 - acc: 0.02 - ETA: 1s - loss: -116.3429 - acc: 0.02 - ETA: 0s - loss: -204.3179 - acc: 0.02 - ETA: 0s - loss: -346.3439 - acc: 0.02 - 5s 5ms/step - loss: -330.5849 - acc: 0.0201\n",
      "Epoch 278/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0773 - acc: 0.04 - ETA: 3s - loss: -2.6299 - acc: 0.02 - ETA: 3s - loss: -2.2324 - acc: 0.02 - ETA: 2s - loss: -1.9381 - acc: 0.02 - ETA: 2s - loss: -1.7302 - acc: 0.02 - ETA: 1s - loss: -1.6159 - acc: 0.02 - ETA: 1s - loss: -1.5532 - acc: 0.02 - ETA: 0s - loss: -152.3403 - acc: 0.02 - ETA: 0s - loss: -135.6237 - acc: 0.02 - 5s 5ms/step - loss: -129.4334 - acc: 0.0201\n",
      "Epoch 279/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1120 - acc: 0.03 - ETA: 3s - loss: -0.5782 - acc: 0.02 - ETA: 3s - loss: -0.9117 - acc: 0.02 - ETA: 2s - loss: -1.7431 - acc: 0.02 - ETA: 2s - loss: -2.2305 - acc: 0.02 - ETA: 1s - loss: -1.9716 - acc: 0.02 - ETA: 1s - loss: -2.2432 - acc: 0.02 - ETA: 0s - loss: -2.0726 - acc: 0.02 - ETA: 0s - loss: -2.0359 - acc: 0.02 - 5s 5ms/step - loss: -1.9998 - acc: 0.0201\n",
      "Epoch 280/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.8457 - acc: 0.01 - ETA: 3s - loss: -494.3324 - acc: 0.00 - ETA: 3s - loss: -329.6483 - acc: 0.02 - ETA: 2s - loss: -247.4897 - acc: 0.02 - ETA: 2s - loss: -198.0330 - acc: 0.02 - ETA: 1s - loss: -165.2249 - acc: 0.02 - ETA: 1s - loss: -142.0487 - acc: 0.02 - ETA: 0s - loss: -124.8398 - acc: 0.02 - ETA: 0s - loss: -111.0862 - acc: 0.02 - 5s 5ms/step - loss: -153.1464 - acc: 0.0201\n",
      "Epoch 281/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.9609 - acc: 0.01 - ETA: 3s - loss: -3.3211 - acc: 0.02 - ETA: 3s - loss: -2.3816 - acc: 0.02 - ETA: 2s - loss: -1.7864 - acc: 0.03 - ETA: 2s - loss: -1.5602 - acc: 0.02 - ETA: 1s - loss: -1.3354 - acc: 0.02 - ETA: 1s - loss: -218.6100 - acc: 0.02 - ETA: 0s - loss: -191.7448 - acc: 0.02 - ETA: 0s - loss: -278.9207 - acc: 0.02 - 5s 5ms/step - loss: -266.1963 - acc: 0.0201\n",
      "Epoch 282/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.2663 - acc: 0.03 - ETA: 3s - loss: -637.1285 - acc: 0.01 - ETA: 3s - loss: -993.9249 - acc: 0.01 - ETA: 2s - loss: -745.5541 - acc: 0.01 - ETA: 2s - loss: -596.4808 - acc: 0.02 - ETA: 1s - loss: -497.0674 - acc: 0.02 - ETA: 1s - loss: -426.3108 - acc: 0.02 - ETA: 0s - loss: -375.3682 - acc: 0.01 - ETA: 0s - loss: -333.7729 - acc: 0.02 - 5s 5ms/step - loss: -318.6107 - acc: 0.0201\n",
      "Epoch 283/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.1466 - acc: 0.030 - ETA: 3s - loss: -0.4966 - acc: 0.03 - ETA: 3s - loss: -0.7580 - acc: 0.03 - ETA: 2s - loss: -1.1997 - acc: 0.02 - ETA: 2s - loss: -0.9127 - acc: 0.02 - ETA: 1s - loss: -1.0782 - acc: 0.02 - ETA: 1s - loss: -4.1403 - acc: 0.02 - ETA: 0s - loss: -3.8969 - acc: 0.02 - ETA: 0s - loss: -160.9461 - acc: 0.02 - 5s 5ms/step - loss: -153.5861 - acc: 0.0201\n",
      "Epoch 284/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0542 - acc: 0.02 - ETA: 3s - loss: -9.3589 - acc: 0.01 - ETA: 3s - loss: -6.4472 - acc: 0.01 - ETA: 2s - loss: -5.0155 - acc: 0.02 - ETA: 2s - loss: -4.1325 - acc: 0.02 - ETA: 1s - loss: -4.0793 - acc: 0.02 - ETA: 1s - loss: -3.7551 - acc: 0.02 - ETA: 0s - loss: -3.3396 - acc: 0.02 - ETA: 0s - loss: -141.5734 - acc: 0.02 - 5s 5ms/step - loss: -190.8342 - acc: 0.0201\n",
      "Epoch 285/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.7935 - acc: 0.02 - ETA: 3s - loss: -3.7386 - acc: 0.01 - ETA: 3s - loss: -3.3890 - acc: 0.01 - ETA: 2s - loss: -2.4686 - acc: 0.02 - ETA: 2s - loss: -2.1323 - acc: 0.02 - ETA: 1s - loss: -1.9449 - acc: 0.02 - ETA: 1s - loss: -1.8276 - acc: 0.02 - ETA: 0s - loss: -1.9877 - acc: 0.02 - ETA: 0s - loss: -109.1275 - acc: 0.02 - 5s 5ms/step - loss: -133.5209 - acc: 0.0201\n",
      "Epoch 286/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1716 - acc: 0.03 - ETA: 3s - loss: -1.9214 - acc: 0.02 - ETA: 3s - loss: -1.5314 - acc: 0.02 - ETA: 2s - loss: -1.6874 - acc: 0.02 - ETA: 2s - loss: -1.6623 - acc: 0.02 - ETA: 1s - loss: -1.7793 - acc: 0.02 - ETA: 1s - loss: -168.7940 - acc: 0.02 - ETA: 0s - loss: -237.7959 - acc: 0.01 - ETA: 0s - loss: -211.4198 - acc: 0.02 - 5s 5ms/step - loss: -201.8368 - acc: 0.0201\n",
      "Epoch 287/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -6.4982 - acc: 0.01 - ETA: 3s - loss: -3.3349 - acc: 0.02 - ETA: 3s - loss: -2.8739 - acc: 0.02 - ETA: 2s - loss: -3.2390 - acc: 0.01 - ETA: 2s - loss: -2.6116 - acc: 0.01 - ETA: 1s - loss: -2.2239 - acc: 0.02 - ETA: 1s - loss: -2.5670 - acc: 0.02 - ETA: 0s - loss: -2.5832 - acc: 0.02 - ETA: 0s - loss: -2.6035 - acc: 0.02 - 5s 5ms/step - loss: -2.5186 - acc: 0.0201\n",
      "Epoch 288/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3841 - acc: 0.02 - ETA: 3s - loss: -0.7455 - acc: 0.02 - ETA: 3s - loss: -0.7805 - acc: 0.02 - ETA: 2s - loss: -0.8929 - acc: 0.02 - ETA: 2s - loss: -1.0920 - acc: 0.02 - ETA: 1s - loss: -1.1485 - acc: 0.02 - ETA: 1s - loss: -1.3845 - acc: 0.02 - ETA: 0s - loss: -1.6085 - acc: 0.02 - ETA: 0s - loss: -1.5289 - acc: 0.02 - 5s 5ms/step - loss: -1.5064 - acc: 0.0201\n",
      "Epoch 289/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4806 - acc: 0.04 - ETA: 3s - loss: -0.5747 - acc: 0.04 - ETA: 3s - loss: -1.2597 - acc: 0.03 - ETA: 2s - loss: -1.2956 - acc: 0.02 - ETA: 2s - loss: -1.2088 - acc: 0.02 - ETA: 1s - loss: -200.6537 - acc: 0.02 - ETA: 1s - loss: -172.5222 - acc: 0.02 - ETA: 0s - loss: -151.0180 - acc: 0.02 - ETA: 0s - loss: -134.7501 - acc: 0.02 - 5s 5ms/step - loss: -174.6609 - acc: 0.0201\n",
      "Epoch 290/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.9293 - acc: 0.01 - ETA: 3s - loss: -2.0662 - acc: 0.01 - ETA: 3s - loss: -2.1250 - acc: 0.01 - ETA: 2s - loss: -1.7963 - acc: 0.01 - ETA: 2s - loss: -309.7899 - acc: 0.01 - ETA: 1s - loss: -258.3694 - acc: 0.01 - ETA: 1s - loss: -221.5365 - acc: 0.01 - ETA: 0s - loss: -194.0452 - acc: 0.01 - ETA: 0s - loss: -172.5314 - acc: 0.01 - 5s 5ms/step - loss: -164.7655 - acc: 0.0201\n",
      "Epoch 291/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0892 - acc: 0.05 - ETA: 3s - loss: -703.2352 - acc: 0.02 - ETA: 3s - loss: -468.9679 - acc: 0.02 - ETA: 2s - loss: -352.6440 - acc: 0.02 - ETA: 2s - loss: -537.9544 - acc: 0.01 - ETA: 1s - loss: -448.6318 - acc: 0.01 - ETA: 1s - loss: -385.1145 - acc: 0.01 - ETA: 0s - loss: -337.0592 - acc: 0.01 - ETA: 0s - loss: -299.7008 - acc: 0.02 - 5s 6ms/step - loss: -286.0182 - acc: 0.0201\n",
      "Epoch 292/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3319 - acc: 0.04 - ETA: 3s - loss: -0.8852 - acc: 0.03 - ETA: 3s - loss: -0.8418 - acc: 0.03 - ETA: 2s - loss: -1.3344 - acc: 0.02 - ETA: 2s - loss: -254.5183 - acc: 0.02 - ETA: 1s - loss: -212.1404 - acc: 0.02 - ETA: 1s - loss: -360.9538 - acc: 0.02 - ETA: 0s - loss: -315.9076 - acc: 0.02 - ETA: 0s - loss: -415.2987 - acc: 0.01 - 5s 5ms/step - loss: -396.3795 - acc: 0.0201\n",
      "Epoch 293/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.3954 - acc: 0.02 - ETA: 3s - loss: -1.7905 - acc: 0.02 - ETA: 3s - loss: -1.0546 - acc: 0.03 - ETA: 2s - loss: -1.7639 - acc: 0.02 - ETA: 2s - loss: -1.5993 - acc: 0.02 - ETA: 1s - loss: -1.5462 - acc: 0.02 - ETA: 1s - loss: -130.3523 - acc: 0.02 - ETA: 0s - loss: -114.2311 - acc: 0.02 - ETA: 0s - loss: -101.8499 - acc: 0.02 - 5s 5ms/step - loss: -194.2418 - acc: 0.0201\n",
      "Epoch 294/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.6945 - acc: 0.02 - ETA: 3s - loss: -2.6561 - acc: 0.02 - ETA: 3s - loss: -273.1707 - acc: 0.01 - ETA: 2s - loss: -205.2211 - acc: 0.01 - ETA: 2s - loss: -164.5364 - acc: 0.01 - ETA: 1s - loss: -137.5937 - acc: 0.01 - ETA: 1s - loss: -118.0626 - acc: 0.01 - ETA: 0s - loss: -103.2387 - acc: 0.02 - ETA: 0s - loss: -92.1302 - acc: 0.0200 - 5s 5ms/step - loss: -87.9875 - acc: 0.0201\n",
      "Epoch 295/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6779 - acc: 0.01 - ETA: 3s - loss: -1.5416 - acc: 0.01 - ETA: 3s - loss: -2.0824 - acc: 0.01 - ETA: 2s - loss: -1.6662 - acc: 0.01 - ETA: 2s - loss: -2.1233 - acc: 0.02 - ETA: 1s - loss: -1.8239 - acc: 0.02 - ETA: 1s - loss: -1.8121 - acc: 0.02 - ETA: 0s - loss: -1.6923 - acc: 0.02 - ETA: 0s - loss: -1.5425 - acc: 0.02 - 5s 5ms/step - loss: -1.5716 - acc: 0.0201\n",
      "Epoch 296/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.9957 - acc: 0.01 - ETA: 3s - loss: -13.7751 - acc: 0.010 - ETA: 3s - loss: -9.3021 - acc: 0.010 - ETA: 2s - loss: -279.2493 - acc: 0.00 - ETA: 2s - loss: -223.7109 - acc: 0.01 - ETA: 1s - loss: -186.4231 - acc: 0.01 - ETA: 1s - loss: -159.9055 - acc: 0.01 - ETA: 0s - loss: -140.0009 - acc: 0.02 - ETA: 0s - loss: -124.6654 - acc: 0.02 - 5s 5ms/step - loss: -119.0375 - acc: 0.0201\n",
      "Epoch 297/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.1496 - acc: 0.02 - ETA: 3s - loss: -3.6606 - acc: 0.01 - ETA: 3s - loss: -4.1842 - acc: 0.01 - ETA: 2s - loss: -246.6319 - acc: 0.01 - ETA: 2s - loss: -197.1980 - acc: 0.01 - ETA: 1s - loss: -164.4100 - acc: 0.02 - ETA: 1s - loss: -141.0449 - acc: 0.02 - ETA: 0s - loss: -123.6842 - acc: 0.02 - ETA: 0s - loss: -110.0329 - acc: 0.02 - 5s 5ms/step - loss: -167.3082 - acc: 0.0201\n",
      "Epoch 298/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.3003 - acc: 0.02 - ETA: 3s - loss: -1.3285 - acc: 0.01 - ETA: 3s - loss: -1.6691 - acc: 0.01 - ETA: 2s - loss: -1.6663 - acc: 0.02 - ETA: 2s - loss: -1.5328 - acc: 0.01 - ETA: 1s - loss: -1.3949 - acc: 0.01 - ETA: 1s - loss: -1.5974 - acc: 0.01 - ETA: 0s - loss: -1.7578 - acc: 0.01 - ETA: 0s - loss: -1.6848 - acc: 0.01 - 5s 5ms/step - loss: -1.6030 - acc: 0.0201\n",
      "Epoch 299/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.0709 - acc: 0.040 - ETA: 3s - loss: -448.0432 - acc: 0.02 - ETA: 3s - loss: -299.0677 - acc: 0.02 - ETA: 2s - loss: -224.6841 - acc: 0.02 - ETA: 2s - loss: -180.1567 - acc: 0.02 - ETA: 1s - loss: -150.1178 - acc: 0.02 - ETA: 1s - loss: -129.0073 - acc: 0.02 - ETA: 0s - loss: -113.0190 - acc: 0.02 - ETA: 0s - loss: -101.0418 - acc: 0.02 - 5s 5ms/step - loss: -159.2901 - acc: 0.0201\n",
      "Epoch 300/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3136 - acc: 0.04 - ETA: 3s - loss: -0.4619 - acc: 0.03 - ETA: 3s - loss: -2.0954 - acc: 0.02 - ETA: 2s - loss: -1.5771 - acc: 0.03 - ETA: 2s - loss: -1.3675 - acc: 0.03 - ETA: 1s - loss: -1.8244 - acc: 0.02 - ETA: 1s - loss: -2.0116 - acc: 0.02 - ETA: 0s - loss: -178.9602 - acc: 0.02 - ETA: 0s - loss: -300.4534 - acc: 0.02 - 5s 5ms/step - loss: -286.7554 - acc: 0.0201\n",
      "Epoch 301/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.7697 - acc: 0.01 - ETA: 3s - loss: -1.8177 - acc: 0.01 - ETA: 3s - loss: -2.1886 - acc: 0.01 - ETA: 2s - loss: -334.7522 - acc: 0.01 - ETA: 2s - loss: -268.7381 - acc: 0.01 - ETA: 1s - loss: -224.0006 - acc: 0.01 - ETA: 1s - loss: -192.0621 - acc: 0.01 - ETA: 0s - loss: -168.0940 - acc: 0.02 - ETA: 0s - loss: -267.1443 - acc: 0.01 - 5s 5ms/step - loss: -254.9493 - acc: 0.0201\n",
      "Epoch 302/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.7982 - acc: 0.01 - ETA: 3s - loss: -0.8424 - acc: 0.02 - ETA: 3s - loss: -1.1904 - acc: 0.02 - ETA: 2s - loss: -1.7695 - acc: 0.02 - ETA: 2s - loss: -1.8518 - acc: 0.01 - ETA: 1s - loss: -1.6445 - acc: 0.02 - ETA: 1s - loss: -1.3679 - acc: 0.02 - ETA: 0s - loss: -2.0651 - acc: 0.02 - ETA: 0s - loss: -2.3496 - acc: 0.02 - 5s 5ms/step - loss: -2.3349 - acc: 0.0201\n",
      "Epoch 303/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -20.7886 - acc: 0.010 - ETA: 3s - loss: -420.6786 - acc: 0.00 - ETA: 3s - loss: -280.8236 - acc: 0.01 - ETA: 2s - loss: -211.7365 - acc: 0.01 - ETA: 2s - loss: -169.8341 - acc: 0.01 - ETA: 1s - loss: -142.0518 - acc: 0.01 - ETA: 1s - loss: -121.9261 - acc: 0.01 - ETA: 0s - loss: -106.8619 - acc: 0.02 - ETA: 0s - loss: -94.9308 - acc: 0.0211 - 5s 5ms/step - loss: -111.0383 - acc: 0.0201\n",
      "Epoch 304/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -7.2647 - acc: 0.01 - ETA: 3s - loss: -3.8098 - acc: 0.01 - ETA: 3s - loss: -2.9862 - acc: 0.01 - ETA: 2s - loss: -2.2913 - acc: 0.01 - ETA: 2s - loss: -2.0679 - acc: 0.01 - ETA: 1s - loss: -5.7353 - acc: 0.01 - ETA: 1s - loss: -5.2610 - acc: 0.01 - ETA: 0s - loss: -4.7364 - acc: 0.01 - ETA: 0s - loss: -4.4001 - acc: 0.01 - 5s 5ms/step - loss: -4.2003 - acc: 0.0201\n",
      "Epoch 305/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8964 - acc: 0.03 - ETA: 3s - loss: -1.0726 - acc: 0.03 - ETA: 3s - loss: -1.6088 - acc: 0.02 - ETA: 2s - loss: -3.2611 - acc: 0.02 - ETA: 2s - loss: -2.5818 - acc: 0.02 - ETA: 1s - loss: -2.3099 - acc: 0.02 - ETA: 1s - loss: -2.3037 - acc: 0.02 - ETA: 0s - loss: -2.1902 - acc: 0.02 - ETA: 0s - loss: -2.0328 - acc: 0.02 - 5s 5ms/step - loss: -49.6556 - acc: 0.0201\n",
      "Epoch 306/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8640 - acc: 0.02 - ETA: 3s - loss: -1.4540 - acc: 0.03 - ETA: 3s - loss: -1.9595 - acc: 0.02 - ETA: 2s - loss: -2.0018 - acc: 0.02 - ETA: 2s - loss: -198.0807 - acc: 0.01 - ETA: 1s - loss: -165.2355 - acc: 0.01 - ETA: 1s - loss: -321.6771 - acc: 0.01 - ETA: 0s - loss: -281.5431 - acc: 0.01 - ETA: 0s - loss: -250.3408 - acc: 0.01 - 5s 5ms/step - loss: -238.9551 - acc: 0.0201\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -1433.7540 - acc: 0.0000e+0 - ETA: 3s - loss: -717.2124 - acc: 0.0050    - ETA: 3s - loss: -478.8590 - acc: 0.00 - ETA: 2s - loss: -729.9511 - acc: 0.00 - ETA: 2s - loss: -584.1285 - acc: 0.01 - ETA: 1s - loss: -487.0180 - acc: 0.01 - ETA: 1s - loss: -417.4183 - acc: 0.02 - ETA: 0s - loss: -365.2854 - acc: 0.02 - ETA: 0s - loss: -324.7740 - acc: 0.02 - 5s 5ms/step - loss: -379.2106 - acc: 0.0201\n",
      "Epoch 308/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4215 - acc: 0.04 - ETA: 3s - loss: -0.6041 - acc: 0.03 - ETA: 3s - loss: -1.0666 - acc: 0.02 - ETA: 2s - loss: -1.2644 - acc: 0.02 - ETA: 2s - loss: -1.5401 - acc: 0.02 - ETA: 1s - loss: -1.3400 - acc: 0.02 - ETA: 1s - loss: -1.3394 - acc: 0.02 - ETA: 0s - loss: -1.6872 - acc: 0.02 - ETA: 0s - loss: -1.8478 - acc: 0.02 - 5s 5ms/step - loss: -57.6234 - acc: 0.0201\n",
      "Epoch 309/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1040 - acc: 0.03 - ETA: 3s - loss: -1.0068 - acc: 0.03 - ETA: 3s - loss: -416.4236 - acc: 0.02 - ETA: 2s - loss: -312.4111 - acc: 0.02 - ETA: 2s - loss: -250.7032 - acc: 0.02 - ETA: 1s - loss: -209.7225 - acc: 0.01 - ETA: 1s - loss: -179.7231 - acc: 0.02 - ETA: 0s - loss: -157.4696 - acc: 0.01 - ETA: 0s - loss: -140.2138 - acc: 0.01 - 5s 5ms/step - loss: -133.9014 - acc: 0.0201\n",
      "Epoch 310/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4910 - acc: 0.02 - ETA: 3s - loss: -0.8655 - acc: 0.02 - ETA: 3s - loss: -0.9174 - acc: 0.02 - ETA: 2s - loss: -0.8629 - acc: 0.02 - ETA: 2s - loss: -0.9115 - acc: 0.02 - ETA: 1s - loss: -1.4613 - acc: 0.02 - ETA: 1s - loss: -1.3245 - acc: 0.02 - ETA: 0s - loss: -1.3392 - acc: 0.02 - ETA: 0s - loss: -1.3792 - acc: 0.02 - 5s 5ms/step - loss: -94.5459 - acc: 0.0201\n",
      "Epoch 311/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1520.6554 - acc: 0.0000e+0 - ETA: 3s - loss: -760.2350 - acc: 0.0150    - ETA: 3s - loss: -507.6627 - acc: 0.01 - ETA: 2s - loss: -718.0576 - acc: 0.01 - ETA: 2s - loss: -574.5416 - acc: 0.01 - ETA: 1s - loss: -478.7947 - acc: 0.02 - ETA: 1s - loss: -411.2284 - acc: 0.02 - ETA: 0s - loss: -359.9508 - acc: 0.02 - ETA: 0s - loss: -320.1432 - acc: 0.02 - 5s 5ms/step - loss: -305.5628 - acc: 0.0201\n",
      "Epoch 312/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1379.1113 - acc: 0.0000e+0 - ETA: 3s - loss: -692.4675 - acc: 0.0050    - ETA: 3s - loss: -461.8660 - acc: 0.01 - ETA: 2s - loss: -346.5652 - acc: 0.01 - ETA: 2s - loss: -277.4999 - acc: 0.02 - ETA: 1s - loss: -231.5490 - acc: 0.02 - ETA: 1s - loss: -198.9560 - acc: 0.02 - ETA: 0s - loss: -174.2590 - acc: 0.01 - ETA: 0s - loss: -154.8774 - acc: 0.02 - 5s 5ms/step - loss: -147.8480 - acc: 0.0201\n",
      "Epoch 313/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2195 - acc: 0.02 - ETA: 3s - loss: -1.2139 - acc: 0.01 - ETA: 3s - loss: -1.0646 - acc: 0.01 - ETA: 2s - loss: -1.9468 - acc: 0.01 - ETA: 2s - loss: -2.1014 - acc: 0.01 - ETA: 1s - loss: -2.6057 - acc: 0.01 - ETA: 1s - loss: -2.6780 - acc: 0.01 - ETA: 0s - loss: -2.3762 - acc: 0.02 - ETA: 0s - loss: -2.3412 - acc: 0.02 - 5s 5ms/step - loss: -22.5546 - acc: 0.0201\n",
      "Epoch 314/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1172 - acc: 0.03 - ETA: 3s - loss: -0.1407 - acc: 0.03 - ETA: 3s - loss: -1.5140 - acc: 0.03 - ETA: 2s - loss: -1.1071 - acc: 0.03 - ETA: 2s - loss: -1.0370 - acc: 0.03 - ETA: 1s - loss: -199.1716 - acc: 0.02 - ETA: 1s - loss: -170.8300 - acc: 0.02 - ETA: 0s - loss: -314.2840 - acc: 0.02 - ETA: 0s - loss: -279.5602 - acc: 0.02 - 5s 5ms/step - loss: -330.1167 - acc: 0.0201\n",
      "Epoch 315/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.5127 - acc: 0.01 - ETA: 3s - loss: -2.8099 - acc: 0.01 - ETA: 3s - loss: -2.1799 - acc: 0.02 - ETA: 2s - loss: -1.7997 - acc: 0.02 - ETA: 2s - loss: -267.0068 - acc: 0.02 - ETA: 1s - loss: -222.5708 - acc: 0.02 - ETA: 1s - loss: -191.1529 - acc: 0.02 - ETA: 0s - loss: -167.5347 - acc: 0.02 - ETA: 0s - loss: -149.0054 - acc: 0.02 - 5s 5ms/step - loss: -197.9454 - acc: 0.0201\n",
      "Epoch 316/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.6459 - acc: 0.01 - ETA: 3s - loss: -2.5123 - acc: 0.01 - ETA: 3s - loss: -2.2386 - acc: 0.01 - ETA: 2s - loss: -2.8187 - acc: 0.01 - ETA: 2s - loss: -2.2282 - acc: 0.02 - ETA: 1s - loss: -225.4341 - acc: 0.01 - ETA: 1s - loss: -193.2652 - acc: 0.02 - ETA: 0s - loss: -169.3271 - acc: 0.02 - ETA: 0s - loss: -150.9201 - acc: 0.02 - 5s 5ms/step - loss: -174.2023 - acc: 0.0201\n",
      "Epoch 317/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6768 - acc: 0.01 - ETA: 3s - loss: -605.3955 - acc: 0.00 - ETA: 3s - loss: -403.9485 - acc: 0.01 - ETA: 2s - loss: -303.1045 - acc: 0.01 - ETA: 2s - loss: -242.6308 - acc: 0.02 - ETA: 1s - loss: -202.2706 - acc: 0.02 - ETA: 1s - loss: -173.6515 - acc: 0.02 - ETA: 0s - loss: -152.5199 - acc: 0.02 - ETA: 0s - loss: -135.7943 - acc: 0.02 - 5s 6ms/step - loss: -192.3783 - acc: 0.0201\n",
      "Epoch 318/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8157 - acc: 0.03 - ETA: 4s - loss: -0.6572 - acc: 0.03 - ETA: 3s - loss: -7.3623 - acc: 0.02 - ETA: 3s - loss: -5.6255 - acc: 0.02 - ETA: 2s - loss: -4.4745 - acc: 0.02 - ETA: 2s - loss: -3.9973 - acc: 0.02 - ETA: 1s - loss: -3.7548 - acc: 0.02 - ETA: 0s - loss: -162.6844 - acc: 0.02 - ETA: 0s - loss: -144.9020 - acc: 0.02 - 6s 6ms/step - loss: -176.9797 - acc: 0.0201\n",
      "Epoch 319/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.4477 - acc: 0.02 - ETA: 3s - loss: -2.3772 - acc: 0.02 - ETA: 3s - loss: -2.4657 - acc: 0.01 - ETA: 2s - loss: -2.8949 - acc: 0.01 - ETA: 2s - loss: -2.5911 - acc: 0.01 - ETA: 1s - loss: -2.3360 - acc: 0.01 - ETA: 1s - loss: -2.0897 - acc: 0.01 - ETA: 0s - loss: -1.8325 - acc: 0.02 - ETA: 0s - loss: -124.3529 - acc: 0.01 - 5s 6ms/step - loss: -118.7185 - acc: 0.0201\n",
      "Epoch 320/500\n",
      "943/943 [==============================] - ETA: 8s - loss: -2.1903 - acc: 0.01 - ETA: 5s - loss: -3.1741 - acc: 0.01 - ETA: 4s - loss: -2.5891 - acc: 0.01 - ETA: 3s - loss: -2.5001 - acc: 0.01 - ETA: 2s - loss: -2.1694 - acc: 0.01 - ETA: 2s - loss: -1.8332 - acc: 0.02 - ETA: 1s - loss: -1.9398 - acc: 0.02 - ETA: 0s - loss: -1.7117 - acc: 0.02 - ETA: 0s - loss: -1.7182 - acc: 0.02 - 6s 6ms/step - loss: -56.9691 - acc: 0.0201\n",
      "Epoch 321/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9658 - acc: 0.03 - ETA: 4s - loss: -2.5627 - acc: 0.02 - ETA: 3s - loss: -3.2391 - acc: 0.01 - ETA: 2s - loss: -3.0047 - acc: 0.02 - ETA: 2s - loss: -2.5442 - acc: 0.01 - ETA: 1s - loss: -2.4217 - acc: 0.01 - ETA: 1s - loss: -2.0358 - acc: 0.02 - ETA: 0s - loss: -1.9561 - acc: 0.02 - ETA: 0s - loss: -1.9387 - acc: 0.02 - 5s 6ms/step - loss: -49.1246 - acc: 0.0201\n",
      "Epoch 322/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.3149 - acc: 0.02 - ETA: 3s - loss: -1.7864 - acc: 0.02 - ETA: 3s - loss: -2.0179 - acc: 0.02 - ETA: 2s - loss: -2.1777 - acc: 0.01 - ETA: 2s - loss: -2.0198 - acc: 0.02 - ETA: 1s - loss: -1.8412 - acc: 0.02 - ETA: 1s - loss: -1.7202 - acc: 0.02 - ETA: 0s - loss: -1.6259 - acc: 0.02 - ETA: 0s - loss: -1.5321 - acc: 0.02 - 5s 6ms/step - loss: -40.2781 - acc: 0.0201\n",
      "Epoch 323/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.0251 - acc: 0.02 - ETA: 3s - loss: -4.5677 - acc: 0.01 - ETA: 3s - loss: -3.3064 - acc: 0.01 - ETA: 2s - loss: -2.6812 - acc: 0.02 - ETA: 2s - loss: -2.3892 - acc: 0.02 - ETA: 1s - loss: -2.0390 - acc: 0.02 - ETA: 1s - loss: -2.3021 - acc: 0.02 - ETA: 0s - loss: -2.2105 - acc: 0.02 - ETA: 0s - loss: -2.0166 - acc: 0.02 - 5s 5ms/step - loss: -65.2729 - acc: 0.0201\n",
      "Epoch 324/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -967.4795 - acc: 0.0000e+ - ETA: 3s - loss: -484.3538 - acc: 0.0150   - ETA: 3s - loss: -324.2813 - acc: 0.01 - ETA: 2s - loss: -243.6728 - acc: 0.01 - ETA: 2s - loss: -195.0370 - acc: 0.01 - ETA: 1s - loss: -163.2944 - acc: 0.01 - ETA: 1s - loss: -140.2026 - acc: 0.01 - ETA: 0s - loss: -122.8772 - acc: 0.01 - ETA: 0s - loss: -109.2573 - acc: 0.02 - 5s 5ms/step - loss: -143.3658 - acc: 0.0201\n",
      "Epoch 325/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.2072 - acc: 0.01 - ETA: 3s - loss: -1.6032 - acc: 0.01 - ETA: 3s - loss: -1.4763 - acc: 0.01 - ETA: 2s - loss: -1.1712 - acc: 0.02 - ETA: 2s - loss: -2.0254 - acc: 0.01 - ETA: 1s - loss: -2.3097 - acc: 0.01 - ETA: 1s - loss: -2.1408 - acc: 0.02 - ETA: 0s - loss: -2.0652 - acc: 0.02 - ETA: 0s - loss: -2.0902 - acc: 0.02 - 5s 6ms/step - loss: -32.1162 - acc: 0.0201\n",
      "Epoch 326/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.0047 - acc: 0.02 - ETA: 3s - loss: -1.6390 - acc: 0.01 - ETA: 3s - loss: -1.3934 - acc: 0.02 - ETA: 2s - loss: -302.0040 - acc: 0.01 - ETA: 2s - loss: -508.2720 - acc: 0.01 - ETA: 1s - loss: -423.6327 - acc: 0.02 - ETA: 1s - loss: -363.4957 - acc: 0.01 - ETA: 0s - loss: -318.0540 - acc: 0.01 - ETA: 0s - loss: -283.0277 - acc: 0.02 - 5s 6ms/step - loss: -270.1167 - acc: 0.0201\n",
      "Epoch 327/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.6051 - acc: 0.01 - ETA: 3s - loss: -4.7834 - acc: 0.01 - ETA: 3s - loss: -3.8493 - acc: 0.01 - ETA: 2s - loss: -2.9699 - acc: 0.01 - ETA: 2s - loss: -2.6518 - acc: 0.01 - ETA: 1s - loss: -2.2275 - acc: 0.01 - ETA: 1s - loss: -2.3497 - acc: 0.02 - ETA: 0s - loss: -2.2653 - acc: 0.02 - ETA: 0s - loss: -2.0384 - acc: 0.01 - 5s 5ms/step - loss: -1.9525 - acc: 0.0201\n",
      "Epoch 328/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9620 - acc: 0.05 - ETA: 3s - loss: -2.5401 - acc: 0.03 - ETA: 3s - loss: -1.5982 - acc: 0.03 - ETA: 2s - loss: -355.3442 - acc: 0.02 - ETA: 2s - loss: -284.2961 - acc: 0.02 - ETA: 1s - loss: -237.0039 - acc: 0.02 - ETA: 1s - loss: -432.0801 - acc: 0.02 - ETA: 0s - loss: -378.4107 - acc: 0.02 - ETA: 0s - loss: -336.5629 - acc: 0.02 - 6s 7ms/step - loss: -321.1948 - acc: 0.0201\n",
      "Epoch 329/500\n",
      "943/943 [==============================] - ETA: 6s - loss: -2.6719 - acc: 0.01 - ETA: 5s - loss: -1.7387 - acc: 0.01 - ETA: 4s - loss: -1.0563 - acc: 0.02 - ETA: 3s - loss: -1.0954 - acc: 0.02 - ETA: 2s - loss: -1.9161 - acc: 0.02 - ETA: 2s - loss: -1.8633 - acc: 0.02 - ETA: 1s - loss: -1.6495 - acc: 0.02 - ETA: 0s - loss: -2.0328 - acc: 0.02 - ETA: 0s - loss: -110.6568 - acc: 0.02 - 6s 6ms/step - loss: -183.7366 - acc: 0.0201\n",
      "Epoch 330/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.6692 - acc: 0.03 - ETA: 3s - loss: -1.1335 - acc: 0.03 - ETA: 3s - loss: -1.0310 - acc: 0.03 - ETA: 2s - loss: -319.9208 - acc: 0.02 - ETA: 2s - loss: -256.1900 - acc: 0.02 - ETA: 1s - loss: -213.4829 - acc: 0.02 - ETA: 1s - loss: -400.5664 - acc: 0.02 - ETA: 0s - loss: -350.5479 - acc: 0.02 - ETA: 0s - loss: -411.5998 - acc: 0.01 - 5s 5ms/step - loss: -392.9065 - acc: 0.0201\n",
      "Epoch 331/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1664 - acc: 0.04 - ETA: 3s - loss: -1.2404 - acc: 0.03 - ETA: 3s - loss: -1.8809 - acc: 0.03 - ETA: 2s - loss: -1.8485 - acc: 0.02 - ETA: 2s - loss: -1.9090 - acc: 0.02 - ETA: 1s - loss: -1.6341 - acc: 0.02 - ETA: 1s - loss: -130.4289 - acc: 0.02 - ETA: 0s - loss: -114.1076 - acc: 0.02 - ETA: 0s - loss: -101.8894 - acc: 0.02 - 5s 5ms/step - loss: -97.3266 - acc: 0.0201\n",
      "Epoch 332/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.0107 - acc: 0.02 - ETA: 3s - loss: -2.1074 - acc: 0.02 - ETA: 3s - loss: -3.9796 - acc: 0.01 - ETA: 2s - loss: -3.9246 - acc: 0.01 - ETA: 2s - loss: -3.2471 - acc: 0.02 - ETA: 1s - loss: -2.7401 - acc: 0.02 - ETA: 1s - loss: -2.8876 - acc: 0.02 - ETA: 0s - loss: -2.5416 - acc: 0.02 - ETA: 0s - loss: -136.8345 - acc: 0.02 - 5s 5ms/step - loss: -130.6077 - acc: 0.0201\n",
      "Epoch 333/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9390 - acc: 0.03 - ETA: 3s - loss: -605.7489 - acc: 0.01 - ETA: 3s - loss: -404.7107 - acc: 0.01 - ETA: 2s - loss: -304.0852 - acc: 0.01 - ETA: 2s - loss: -243.3349 - acc: 0.01 - ETA: 1s - loss: -202.8856 - acc: 0.01 - ETA: 1s - loss: -174.0216 - acc: 0.02 - ETA: 0s - loss: -311.1469 - acc: 0.01 - ETA: 0s - loss: -276.8713 - acc: 0.01 - 5s 5ms/step - loss: -264.2231 - acc: 0.0201\n",
      "Epoch 334/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9730 - acc: 0.03 - ETA: 3s - loss: -1.0047 - acc: 0.04 - ETA: 3s - loss: -0.8900 - acc: 0.03 - ETA: 2s - loss: -0.7840 - acc: 0.03 - ETA: 2s - loss: -310.2302 - acc: 0.02 - ETA: 1s - loss: -258.6300 - acc: 0.02 - ETA: 1s - loss: -222.4253 - acc: 0.02 - ETA: 0s - loss: -194.8047 - acc: 0.02 - ETA: 0s - loss: -298.8192 - acc: 0.01 - 5s 5ms/step - loss: -285.2488 - acc: 0.0201\n",
      "Epoch 335/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.1083 - acc: 0.01 - ETA: 3s - loss: -3.0980 - acc: 0.02 - ETA: 3s - loss: -2.8074 - acc: 0.01 - ETA: 2s - loss: -2.5517 - acc: 0.02 - ETA: 2s - loss: -2.9893 - acc: 0.02 - ETA: 1s - loss: -3.3710 - acc: 0.01 - ETA: 1s - loss: -2.9675 - acc: 0.01 - ETA: 0s - loss: -2.7317 - acc: 0.01 - ETA: 0s - loss: -2.3932 - acc: 0.01 - 5s 5ms/step - loss: -2.2672 - acc: 0.0201\n",
      "Epoch 336/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -727.6212 - acc: 0.0000e+ - ETA: 3s - loss: -365.7187 - acc: 0.0100   - ETA: 3s - loss: -244.2723 - acc: 0.01 - ETA: 2s - loss: -184.2467 - acc: 0.01 - ETA: 2s - loss: -147.4390 - acc: 0.01 - ETA: 1s - loss: -122.8901 - acc: 0.01 - ETA: 1s - loss: -105.5778 - acc: 0.02 - ETA: 0s - loss: -92.6548 - acc: 0.0187 - ETA: 0s - loss: -82.5663 - acc: 0.020 - 5s 5ms/step - loss: -78.9027 - acc: 0.0201\n",
      "Epoch 337/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5647 - acc: 0.03 - ETA: 3s - loss: -0.2805 - acc: 0.03 - ETA: 3s - loss: -0.0808 - acc: 0.03 - ETA: 2s - loss: -0.8442 - acc: 0.02 - ETA: 2s - loss: -1.2404 - acc: 0.02 - ETA: 1s - loss: -1.4034 - acc: 0.02 - ETA: 1s - loss: -238.0872 - acc: 0.02 - ETA: 0s - loss: -209.1598 - acc: 0.02 - ETA: 0s - loss: -186.1346 - acc: 0.01 - 5s 5ms/step - loss: -177.6880 - acc: 0.0201\n",
      "Epoch 338/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6513 - acc: 0.02 - ETA: 3s - loss: -1.0789 - acc: 0.02 - ETA: 3s - loss: -1.4324 - acc: 0.02 - ETA: 2s - loss: -1.2649 - acc: 0.02 - ETA: 2s - loss: -1.6437 - acc: 0.02 - ETA: 1s - loss: -1.5543 - acc: 0.02 - ETA: 1s - loss: -1.8710 - acc: 0.02 - ETA: 0s - loss: -1.7545 - acc: 0.02 - ETA: 0s - loss: -1.6733 - acc: 0.02 - 5s 5ms/step - loss: -64.2498 - acc: 0.0201\n",
      "Epoch 339/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3825 - acc: 0.01 - ETA: 3s - loss: -4.4518 - acc: 0.01 - ETA: 3s - loss: -5.8942 - acc: 0.01 - ETA: 2s - loss: -4.2883 - acc: 0.01 - ETA: 2s - loss: -3.6256 - acc: 0.01 - ETA: 1s - loss: -3.1960 - acc: 0.01 - ETA: 1s - loss: -3.4478 - acc: 0.01 - ETA: 0s - loss: -3.4121 - acc: 0.01 - ETA: 0s - loss: -3.0268 - acc: 0.02 - 5s 5ms/step - loss: -66.1784 - acc: 0.0201\n",
      "Epoch 340/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3327 - acc: 0.02 - ETA: 3s - loss: -0.4134 - acc: 0.02 - ETA: 3s - loss: -427.1507 - acc: 0.01 - ETA: 2s - loss: -320.4254 - acc: 0.02 - ETA: 2s - loss: -523.5232 - acc: 0.01 - ETA: 1s - loss: -436.5693 - acc: 0.01 - ETA: 1s - loss: -374.8037 - acc: 0.01 - ETA: 0s - loss: -328.1359 - acc: 0.02 - ETA: 0s - loss: -291.7763 - acc: 0.02 - 5s 5ms/step - loss: -278.4503 - acc: 0.0201\n",
      "Epoch 341/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2375 - acc: 0.05 - ETA: 3s - loss: -1.4480 - acc: 0.03 - ETA: 3s - loss: -1.7300 - acc: 0.02 - ETA: 2s - loss: -1.6205 - acc: 0.02 - ETA: 2s - loss: -1.3787 - acc: 0.02 - ETA: 1s - loss: -1.3087 - acc: 0.02 - ETA: 1s - loss: -1.6435 - acc: 0.02 - ETA: 0s - loss: -150.7803 - acc: 0.02 - ETA: 0s - loss: -134.1487 - acc: 0.02 - 5s 5ms/step - loss: -128.1147 - acc: 0.0201\n",
      "Epoch 342/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6107 - acc: 0.03 - ETA: 3s - loss: -3.5048 - acc: 0.02 - ETA: 3s - loss: -2.8474 - acc: 0.02 - ETA: 2s - loss: -2.3488 - acc: 0.02 - ETA: 2s - loss: -2.3207 - acc: 0.01 - ETA: 1s - loss: -2.4524 - acc: 0.01 - ETA: 1s - loss: -2.3523 - acc: 0.02 - ETA: 0s - loss: -2.0226 - acc: 0.02 - ETA: 0s - loss: -2.0996 - acc: 0.02 - 5s 5ms/step - loss: -22.6220 - acc: 0.0201\n",
      "Epoch 343/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -0.4005 - acc: 0.05 - ETA: 3s - loss: -569.4188 - acc: 0.02 - ETA: 3s - loss: -380.1062 - acc: 0.02 - ETA: 2s - loss: -285.2915 - acc: 0.02 - ETA: 2s - loss: -228.5089 - acc: 0.02 - ETA: 1s - loss: -190.3789 - acc: 0.02 - ETA: 1s - loss: -163.2823 - acc: 0.02 - ETA: 0s - loss: -143.6354 - acc: 0.02 - ETA: 0s - loss: -311.7809 - acc: 0.02 - 5s 5ms/step - loss: -336.4709 - acc: 0.0201\n",
      "Epoch 344/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.6469 - acc: 0.01 - ETA: 4s - loss: -3.4032 - acc: 0.01 - ETA: 3s - loss: -2.8705 - acc: 0.01 - ETA: 2s - loss: -2.3632 - acc: 0.01 - ETA: 2s - loss: -2.3113 - acc: 0.01 - ETA: 1s - loss: -2.0767 - acc: 0.02 - ETA: 1s - loss: -1.9501 - acc: 0.02 - ETA: 0s - loss: -2.2398 - acc: 0.02 - ETA: 0s - loss: -1.9707 - acc: 0.02 - 5s 5ms/step - loss: -41.0352 - acc: 0.0201\n",
      "Epoch 345/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8650 - acc: 0.02 - ETA: 3s - loss: -10.8194 - acc: 0.015 - ETA: 3s - loss: -7.1746 - acc: 0.030 - ETA: 2s - loss: -5.5753 - acc: 0.03 - ETA: 2s - loss: -198.2392 - acc: 0.02 - ETA: 1s - loss: -389.3505 - acc: 0.02 - ETA: 1s - loss: -333.9958 - acc: 0.02 - ETA: 0s - loss: -292.2855 - acc: 0.02 - ETA: 0s - loss: -260.5972 - acc: 0.02 - 5s 5ms/step - loss: -311.4358 - acc: 0.0201\n",
      "Epoch 346/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.2541 - acc: 0.02 - ETA: 3s - loss: -0.6166 - acc: 0.03 - ETA: 3s - loss: -0.8844 - acc: 0.03 - ETA: 2s - loss: -1.5690 - acc: 0.02 - ETA: 2s - loss: -344.7378 - acc: 0.02 - ETA: 1s - loss: -396.0084 - acc: 0.01 - ETA: 1s - loss: -339.7585 - acc: 0.01 - ETA: 0s - loss: -297.4324 - acc: 0.01 - ETA: 0s - loss: -264.3541 - acc: 0.01 - 5s 5ms/step - loss: -252.3344 - acc: 0.0201\n",
      "Epoch 347/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.0675 - acc: 0.030 - ETA: 3s - loss: -0.1044 - acc: 0.03 - ETA: 3s - loss: -2.2068 - acc: 0.02 - ETA: 2s - loss: -1.7895 - acc: 0.02 - ETA: 2s - loss: -2.3442 - acc: 0.02 - ETA: 1s - loss: -2.1249 - acc: 0.02 - ETA: 1s - loss: -1.9112 - acc: 0.02 - ETA: 0s - loss: -143.5799 - acc: 0.02 - ETA: 0s - loss: -127.9447 - acc: 0.02 - 5s 5ms/step - loss: -161.3653 - acc: 0.0201\n",
      "Epoch 348/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9806 - acc: 0.04 - ETA: 3s - loss: -0.9177 - acc: 0.04 - ETA: 3s - loss: -448.8325 - acc: 0.02 - ETA: 2s - loss: -337.0719 - acc: 0.02 - ETA: 2s - loss: -269.7816 - acc: 0.02 - ETA: 1s - loss: -225.0678 - acc: 0.02 - ETA: 1s - loss: -192.9524 - acc: 0.02 - ETA: 0s - loss: -169.0351 - acc: 0.02 - ETA: 0s - loss: -298.4013 - acc: 0.02 - 5s 5ms/step - loss: -284.8415 - acc: 0.0201\n",
      "Epoch 349/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.1971 - acc: 0.040 - ETA: 3s - loss: -0.8264 - acc: 0.02 - ETA: 3s - loss: -325.4278 - acc: 0.01 - ETA: 2s - loss: -244.0575 - acc: 0.02 - ETA: 2s - loss: -437.2968 - acc: 0.01 - ETA: 1s - loss: -364.5086 - acc: 0.01 - ETA: 1s - loss: -312.8441 - acc: 0.02 - ETA: 0s - loss: -273.9833 - acc: 0.02 - ETA: 0s - loss: -244.7120 - acc: 0.02 - 5s 5ms/step - loss: -233.5655 - acc: 0.0201\n",
      "Epoch 350/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.1495 - acc: 0.01 - ETA: 3s - loss: -490.4982 - acc: 0.00 - ETA: 3s - loss: -328.8450 - acc: 0.00 - ETA: 2s - loss: -246.6466 - acc: 0.01 - ETA: 2s - loss: -197.4681 - acc: 0.02 - ETA: 1s - loss: -164.5445 - acc: 0.02 - ETA: 1s - loss: -142.1717 - acc: 0.02 - ETA: 0s - loss: -124.5751 - acc: 0.02 - ETA: 0s - loss: -110.9897 - acc: 0.02 - 5s 5ms/step - loss: -175.4562 - acc: 0.0201\n",
      "Epoch 351/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8604 - acc: 0.04 - ETA: 3s - loss: -1.4411 - acc: 0.02 - ETA: 3s - loss: -2.0389 - acc: 0.02 - ETA: 2s - loss: -337.2140 - acc: 0.01 - ETA: 2s - loss: -269.9024 - acc: 0.02 - ETA: 1s - loss: -225.0306 - acc: 0.02 - ETA: 1s - loss: -192.9952 - acc: 0.02 - ETA: 0s - loss: -261.1220 - acc: 0.01 - ETA: 0s - loss: -232.1685 - acc: 0.02 - 5s 5ms/step - loss: -222.1335 - acc: 0.0201\n",
      "Epoch 352/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3008 - acc: 0.02 - ETA: 3s - loss: -0.9325 - acc: 0.02 - ETA: 3s - loss: -0.8212 - acc: 0.02 - ETA: 2s - loss: -1.1678 - acc: 0.02 - ETA: 2s - loss: -1.8783 - acc: 0.02 - ETA: 1s - loss: -1.8558 - acc: 0.02 - ETA: 1s - loss: -2.0418 - acc: 0.01 - ETA: 0s - loss: -2.0623 - acc: 0.01 - ETA: 0s - loss: -1.9161 - acc: 0.02 - 5s 5ms/step - loss: -32.0369 - acc: 0.0201\n",
      "Epoch 353/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.7638 - acc: 0.01 - ETA: 3s - loss: -2.5865 - acc: 0.02 - ETA: 3s - loss: -1.8270 - acc: 0.02 - ETA: 2s - loss: -7.0016 - acc: 0.01 - ETA: 2s - loss: -5.7772 - acc: 0.01 - ETA: 1s - loss: -5.6916 - acc: 0.01 - ETA: 1s - loss: -4.9806 - acc: 0.01 - ETA: 0s - loss: -4.6880 - acc: 0.01 - ETA: 0s - loss: -4.2012 - acc: 0.02 - 5s 5ms/step - loss: -3.9885 - acc: 0.0201\n",
      "Epoch 354/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0258 - acc: 0.04 - ETA: 3s - loss: -1.9005 - acc: 0.02 - ETA: 3s - loss: -536.2218 - acc: 0.01 - ETA: 2s - loss: -402.2980 - acc: 0.01 - ETA: 2s - loss: -321.8028 - acc: 0.02 - ETA: 1s - loss: -269.0121 - acc: 0.01 - ETA: 1s - loss: -230.8877 - acc: 0.01 - ETA: 0s - loss: -202.2269 - acc: 0.01 - ETA: 0s - loss: -179.8600 - acc: 0.02 - 5s 5ms/step - loss: -171.7499 - acc: 0.0201\n",
      "Epoch 355/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6586 - acc: 0.01 - ETA: 3s - loss: -2.2689 - acc: 0.01 - ETA: 3s - loss: -1.6301 - acc: 0.01 - ETA: 2s - loss: -1.3534 - acc: 0.01 - ETA: 2s - loss: -1.2050 - acc: 0.02 - ETA: 1s - loss: -1.7243 - acc: 0.01 - ETA: 1s - loss: -1.5453 - acc: 0.02 - ETA: 0s - loss: -1.6923 - acc: 0.02 - ETA: 0s - loss: -1.7002 - acc: 0.02 - 5s 5ms/step - loss: -1.7896 - acc: 0.0201\n",
      "Epoch 356/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6241 - acc: 0.02 - ETA: 3s - loss: -1.0334 - acc: 0.02 - ETA: 3s - loss: -2.4240 - acc: 0.02 - ETA: 2s - loss: -2.0311 - acc: 0.02 - ETA: 2s - loss: -2.0570 - acc: 0.02 - ETA: 1s - loss: -2.0484 - acc: 0.02 - ETA: 1s - loss: -2.0692 - acc: 0.02 - ETA: 0s - loss: -2.2019 - acc: 0.01 - ETA: 0s - loss: -2.0551 - acc: 0.01 - 5s 5ms/step - loss: -1.9593 - acc: 0.0201\n",
      "Epoch 357/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.1589 - acc: 0.050 - ETA: 3s - loss: -1.2358 - acc: 0.03 - ETA: 3s - loss: -1.1641 - acc: 0.02 - ETA: 2s - loss: -1.2180 - acc: 0.02 - ETA: 2s - loss: -1.0819 - acc: 0.02 - ETA: 1s - loss: -1.2421 - acc: 0.01 - ETA: 1s - loss: -1.7769 - acc: 0.01 - ETA: 0s - loss: -2.0203 - acc: 0.01 - ETA: 0s - loss: -1.9883 - acc: 0.01 - 5s 5ms/step - loss: -2.0248 - acc: 0.0201\n",
      "Epoch 358/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.0633 - acc: 0.050 - ETA: 3s - loss: -0.1337 - acc: 0.04 - ETA: 3s - loss: -0.3322 - acc: 0.03 - ETA: 2s - loss: -0.7031 - acc: 0.03 - ETA: 2s - loss: -220.4804 - acc: 0.02 - ETA: 1s - loss: -184.2552 - acc: 0.02 - ETA: 1s - loss: -158.0234 - acc: 0.02 - ETA: 0s - loss: -138.5247 - acc: 0.02 - ETA: 0s - loss: -287.9691 - acc: 0.02 - 5s 5ms/step - loss: -274.9755 - acc: 0.0201\n",
      "Epoch 359/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.6643 - acc: 0.01 - ETA: 3s - loss: -1.9437 - acc: 0.01 - ETA: 3s - loss: -1.7572 - acc: 0.02 - ETA: 2s - loss: -2.2236 - acc: 0.01 - ETA: 2s - loss: -2.8305 - acc: 0.01 - ETA: 1s - loss: -2.4620 - acc: 0.02 - ETA: 1s - loss: -2.4274 - acc: 0.02 - ETA: 0s - loss: -2.5129 - acc: 0.01 - ETA: 0s - loss: -2.4292 - acc: 0.01 - 5s 5ms/step - loss: -2.2875 - acc: 0.0201\n",
      "Epoch 360/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1432 - acc: 0.03 - ETA: 4s - loss: -0.2462 - acc: 0.02 - ETA: 3s - loss: -424.5173 - acc: 0.01 - ETA: 2s - loss: -318.2828 - acc: 0.02 - ETA: 2s - loss: -255.1573 - acc: 0.01 - ETA: 1s - loss: -434.0625 - acc: 0.01 - ETA: 1s - loss: -372.4659 - acc: 0.01 - ETA: 0s - loss: -326.2836 - acc: 0.01 - ETA: 0s - loss: -290.1144 - acc: 0.01 - 5s 5ms/step - loss: -276.9416 - acc: 0.0201\n",
      "Epoch 361/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.1259 - acc: 0.01 - ETA: 3s - loss: -1.7559 - acc: 0.02 - ETA: 3s - loss: -1.5522 - acc: 0.02 - ETA: 2s - loss: -1.7896 - acc: 0.02 - ETA: 2s - loss: -1.9657 - acc: 0.02 - ETA: 1s - loss: -1.8603 - acc: 0.02 - ETA: 1s - loss: -1.8310 - acc: 0.02 - ETA: 0s - loss: -1.9357 - acc: 0.02 - ETA: 0s - loss: -1.7535 - acc: 0.02 - 5s 5ms/step - loss: -40.8599 - acc: 0.0201\n",
      "Epoch 362/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.2945 - acc: 0.01 - ETA: 3s - loss: -328.7937 - acc: 0.00 - ETA: 3s - loss: -219.8302 - acc: 0.01 - ETA: 2s - loss: -165.1271 - acc: 0.01 - ETA: 2s - loss: -132.1707 - acc: 0.01 - ETA: 1s - loss: -110.4308 - acc: 0.02 - ETA: 1s - loss: -94.8038 - acc: 0.0200 - ETA: 0s - loss: -83.0130 - acc: 0.022 - ETA: 0s - loss: -221.4797 - acc: 0.02 - 5s 5ms/step - loss: -211.5183 - acc: 0.0201\n",
      "Epoch 363/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.0490 - acc: 0.02 - ETA: 3s - loss: -2.5750 - acc: 0.01 - ETA: 3s - loss: -348.8420 - acc: 0.01 - ETA: 2s - loss: -261.6452 - acc: 0.02 - ETA: 2s - loss: -210.1426 - acc: 0.02 - ETA: 1s - loss: -407.1424 - acc: 0.01 - ETA: 1s - loss: -349.0448 - acc: 0.01 - ETA: 0s - loss: -305.4823 - acc: 0.02 - ETA: 0s - loss: -271.6986 - acc: 0.02 - 5s 5ms/step - loss: -259.6354 - acc: 0.0201\n",
      "Epoch 364/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.2115 - acc: 0.04 - ETA: 3s - loss: -0.6155 - acc: 0.04 - ETA: 3s - loss: -0.7147 - acc: 0.03 - ETA: 2s - loss: -0.7829 - acc: 0.03 - ETA: 2s - loss: -1.4651 - acc: 0.02 - ETA: 1s - loss: -1.3990 - acc: 0.02 - ETA: 1s - loss: -1.8718 - acc: 0.02 - ETA: 0s - loss: -133.3824 - acc: 0.02 - ETA: 0s - loss: -118.6744 - acc: 0.02 - 5s 5ms/step - loss: -168.7768 - acc: 0.0201\n",
      "Epoch 365/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.9084 - acc: 0.01 - ETA: 3s - loss: -2.3329 - acc: 0.02 - ETA: 3s - loss: -2.0198 - acc: 0.01 - ETA: 2s - loss: -1.5190 - acc: 0.02 - ETA: 2s - loss: -1.4288 - acc: 0.02 - ETA: 1s - loss: -2.8863 - acc: 0.02 - ETA: 1s - loss: -2.5328 - acc: 0.02 - ETA: 0s - loss: -124.7205 - acc: 0.02 - ETA: 0s - loss: -111.0614 - acc: 0.02 - 5s 5ms/step - loss: -106.1391 - acc: 0.0201\n",
      "Epoch 366/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6705 - acc: 0.01 - ETA: 3s - loss: -4.2399 - acc: 0.01 - ETA: 3s - loss: -4.2658 - acc: 0.01 - ETA: 2s - loss: -3.9737 - acc: 0.01 - ETA: 2s - loss: -3.2687 - acc: 0.01 - ETA: 1s - loss: -3.6819 - acc: 0.01 - ETA: 1s - loss: -3.2755 - acc: 0.01 - ETA: 0s - loss: -3.0082 - acc: 0.01 - ETA: 0s - loss: -2.6736 - acc: 0.02 - 5s 5ms/step - loss: -2.5186 - acc: 0.0201\n",
      "Epoch 367/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.6452 - acc: 0.02 - ETA: 3s - loss: -4.7075 - acc: 0.01 - ETA: 3s - loss: -3.2116 - acc: 0.01 - ETA: 2s - loss: -2.3486 - acc: 0.02 - ETA: 2s - loss: -2.0143 - acc: 0.02 - ETA: 1s - loss: -1.7884 - acc: 0.02 - ETA: 1s - loss: -1.6281 - acc: 0.02 - ETA: 0s - loss: -3.0204 - acc: 0.02 - ETA: 0s - loss: -3.1038 - acc: 0.02 - 5s 5ms/step - loss: -3.0194 - acc: 0.0201\n",
      "Epoch 368/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8383 - acc: 0.02 - ETA: 3s - loss: -0.5353 - acc: 0.02 - ETA: 3s - loss: -0.8138 - acc: 0.02 - ETA: 2s - loss: -3.1459 - acc: 0.01 - ETA: 2s - loss: -2.7440 - acc: 0.02 - ETA: 1s - loss: -2.4964 - acc: 0.02 - ETA: 1s - loss: -2.2991 - acc: 0.02 - ETA: 0s - loss: -193.5339 - acc: 0.01 - ETA: 0s - loss: -172.4899 - acc: 0.01 - 5s 5ms/step - loss: -164.6016 - acc: 0.0201\n",
      "Epoch 369/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0915 - acc: 0.04 - ETA: 3s - loss: -0.8683 - acc: 0.03 - ETA: 3s - loss: -0.8235 - acc: 0.03 - ETA: 2s - loss: -371.0308 - acc: 0.02 - ETA: 2s - loss: -297.2696 - acc: 0.02 - ETA: 1s - loss: -247.7916 - acc: 0.02 - ETA: 1s - loss: -212.4039 - acc: 0.02 - ETA: 0s - loss: -369.2697 - acc: 0.02 - ETA: 0s - loss: -426.9275 - acc: 0.02 - 5s 5ms/step - loss: -407.5420 - acc: 0.0201\n",
      "Epoch 370/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.3104 - acc: 0.020 - ETA: 3s - loss: -0.6234 - acc: 0.02 - ETA: 3s - loss: -0.7835 - acc: 0.02 - ETA: 2s - loss: -226.4597 - acc: 0.01 - ETA: 2s - loss: -182.0412 - acc: 0.01 - ETA: 1s - loss: -151.9342 - acc: 0.01 - ETA: 1s - loss: -130.4936 - acc: 0.02 - ETA: 0s - loss: -114.2018 - acc: 0.02 - ETA: 0s - loss: -101.5677 - acc: 0.02 - 5s 5ms/step - loss: -178.9796 - acc: 0.0201\n",
      "Epoch 371/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.7704 - acc: 0.01 - ETA: 3s - loss: -1.7824 - acc: 0.01 - ETA: 3s - loss: -2.0128 - acc: 0.01 - ETA: 2s - loss: -2.0365 - acc: 0.01 - ETA: 2s - loss: -1.7115 - acc: 0.01 - ETA: 1s - loss: -1.6379 - acc: 0.01 - ETA: 1s - loss: -1.8512 - acc: 0.01 - ETA: 0s - loss: -1.7801 - acc: 0.01 - ETA: 0s - loss: -1.6528 - acc: 0.02 - 5s 5ms/step - loss: -57.2510 - acc: 0.0201\n",
      "Epoch 372/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0126 - acc: 0.03 - ETA: 3s - loss: -3.2083 - acc: 0.02 - ETA: 3s - loss: -2.4550 - acc: 0.02 - ETA: 2s - loss: -2.8200 - acc: 0.02 - ETA: 2s - loss: -2.7013 - acc: 0.01 - ETA: 1s - loss: -3.3131 - acc: 0.01 - ETA: 1s - loss: -2.9378 - acc: 0.01 - ETA: 0s - loss: -2.6386 - acc: 0.02 - ETA: 0s - loss: -2.4842 - acc: 0.02 - 5s 5ms/step - loss: -22.9119 - acc: 0.0201\n",
      "Epoch 373/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0915 - acc: 0.04 - ETA: 3s - loss: -3.6013 - acc: 0.02 - ETA: 3s - loss: -3.2868 - acc: 0.02 - ETA: 2s - loss: -3.6354 - acc: 0.01 - ETA: 2s - loss: -3.2008 - acc: 0.02 - ETA: 1s - loss: -2.9643 - acc: 0.02 - ETA: 1s - loss: -3.2101 - acc: 0.02 - ETA: 0s - loss: -3.1434 - acc: 0.01 - ETA: 0s - loss: -2.7505 - acc: 0.02 - 5s 5ms/step - loss: -2.6267 - acc: 0.0201\n",
      "Epoch 374/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1060.0576 - acc: 0.0000e+0 - ETA: 3s - loss: -530.0085 - acc: 0.0150    - ETA: 3s - loss: -353.5327 - acc: 0.02 - ETA: 2s - loss: -266.0297 - acc: 0.01 - ETA: 2s - loss: -212.8744 - acc: 0.02 - ETA: 1s - loss: -177.6045 - acc: 0.02 - ETA: 1s - loss: -153.0619 - acc: 0.01 - ETA: 0s - loss: -135.2811 - acc: 0.01 - ETA: 0s - loss: -120.3745 - acc: 0.01 - 5s 5ms/step - loss: -114.9148 - acc: 0.0201\n",
      "Epoch 375/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1202.0490 - acc: 0.0000e+0 - ETA: 3s - loss: -601.7344 - acc: 0.0150    - ETA: 3s - loss: -401.3218 - acc: 0.02 - ETA: 2s - loss: -565.0452 - acc: 0.01 - ETA: 2s - loss: -452.5676 - acc: 0.01 - ETA: 1s - loss: -634.1214 - acc: 0.01 - ETA: 1s - loss: -543.9233 - acc: 0.01 - ETA: 0s - loss: -476.0237 - acc: 0.01 - ETA: 0s - loss: -423.1249 - acc: 0.01 - 5s 5ms/step - loss: -403.8435 - acc: 0.0201\n",
      "Epoch 376/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1401.3577 - acc: 0.0000e+0 - ETA: 3s - loss: -700.8625 - acc: 0.0150    - ETA: 3s - loss: -467.3387 - acc: 0.01 - ETA: 2s - loss: -350.7835 - acc: 0.02 - ETA: 2s - loss: -280.9031 - acc: 0.01 - ETA: 1s - loss: -234.3602 - acc: 0.01 - ETA: 1s - loss: -200.9282 - acc: 0.02 - ETA: 0s - loss: -176.2886 - acc: 0.02 - ETA: 0s - loss: -156.9756 - acc: 0.02 - 5s 5ms/step - loss: -149.9244 - acc: 0.0201\n",
      "Epoch 377/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.3456 - acc: 0.01 - ETA: 3s - loss: -2.5722 - acc: 0.01 - ETA: 3s - loss: -2.5954 - acc: 0.01 - ETA: 2s - loss: -4.2751 - acc: 0.01 - ETA: 2s - loss: -3.7855 - acc: 0.01 - ETA: 1s - loss: -3.2556 - acc: 0.01 - ETA: 1s - loss: -2.8080 - acc: 0.01 - ETA: 0s - loss: -2.5641 - acc: 0.01 - ETA: 0s - loss: -2.4692 - acc: 0.02 - 5s 5ms/step - loss: -2.4132 - acc: 0.0201\n",
      "Epoch 378/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1365 - acc: 0.03 - ETA: 3s - loss: -411.2571 - acc: 0.01 - ETA: 3s - loss: -275.3875 - acc: 0.01 - ETA: 2s - loss: -206.5475 - acc: 0.01 - ETA: 2s - loss: -392.0820 - acc: 0.01 - ETA: 1s - loss: -326.9173 - acc: 0.01 - ETA: 1s - loss: -280.3139 - acc: 0.02 - ETA: 0s - loss: -245.4765 - acc: 0.02 - ETA: 0s - loss: -344.3573 - acc: 0.01 - 5s 5ms/step - loss: -328.7907 - acc: 0.0201\n",
      "Epoch 379/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: -1.1337 - acc: 0.03 - ETA: 3s - loss: -0.9159 - acc: 0.03 - ETA: 3s - loss: -0.8374 - acc: 0.03 - ETA: 2s - loss: -259.2900 - acc: 0.02 - ETA: 2s - loss: -207.6899 - acc: 0.02 - ETA: 1s - loss: -173.5634 - acc: 0.02 - ETA: 1s - loss: -149.4497 - acc: 0.02 - ETA: 0s - loss: -130.7892 - acc: 0.02 - ETA: 0s - loss: -249.9494 - acc: 0.02 - 5s 5ms/step - loss: -286.1636 - acc: 0.0201\n",
      "Epoch 380/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.4403 - acc: 0.02 - ETA: 3s - loss: -4.2246 - acc: 0.02 - ETA: 3s - loss: -3.7446 - acc: 0.02 - ETA: 2s - loss: -2.9732 - acc: 0.01 - ETA: 2s - loss: -2.6091 - acc: 0.02 - ETA: 1s - loss: -2.1789 - acc: 0.02 - ETA: 1s - loss: -2.0116 - acc: 0.02 - ETA: 0s - loss: -2.0040 - acc: 0.02 - ETA: 0s - loss: -92.5854 - acc: 0.021 - 5s 5ms/step - loss: -126.1527 - acc: 0.0201\n",
      "Epoch 381/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4593 - acc: 0.03 - ETA: 3s - loss: -604.9665 - acc: 0.01 - ETA: 3s - loss: -916.3199 - acc: 0.01 - ETA: 2s - loss: -687.4752 - acc: 0.01 - ETA: 2s - loss: -550.4011 - acc: 0.01 - ETA: 1s - loss: -458.8120 - acc: 0.01 - ETA: 1s - loss: -393.5954 - acc: 0.01 - ETA: 0s - loss: -344.4582 - acc: 0.02 - ETA: 0s - loss: -306.4399 - acc: 0.01 - 5s 5ms/step - loss: -292.4613 - acc: 0.0201\n",
      "Epoch 382/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4280 - acc: 0.02 - ETA: 3s - loss: -1.0615 - acc: 0.02 - ETA: 3s - loss: -1.9242 - acc: 0.02 - ETA: 2s - loss: -2.1787 - acc: 0.01 - ETA: 2s - loss: -1.8924 - acc: 0.01 - ETA: 1s - loss: -1.7436 - acc: 0.02 - ETA: 1s - loss: -1.9833 - acc: 0.02 - ETA: 0s - loss: -1.8602 - acc: 0.02 - ETA: 0s - loss: -1.8365 - acc: 0.02 - 5s 5ms/step - loss: -49.5656 - acc: 0.0201\n",
      "Epoch 383/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1334.5560 - acc: 0.0000e+0 - ETA: 3s - loss: -670.0612 - acc: 0.0100    - ETA: 3s - loss: -447.3012 - acc: 0.01 - ETA: 2s - loss: -336.2848 - acc: 0.01 - ETA: 2s - loss: -269.1203 - acc: 0.01 - ETA: 1s - loss: -224.2810 - acc: 0.02 - ETA: 1s - loss: -320.4150 - acc: 0.01 - ETA: 0s - loss: -280.3755 - acc: 0.01 - ETA: 0s - loss: -249.5285 - acc: 0.01 - 5s 5ms/step - loss: -238.1553 - acc: 0.0201\n",
      "Epoch 384/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9218 - acc: 0.03 - ETA: 3s - loss: -1.4709 - acc: 0.02 - ETA: 3s - loss: -1.8616 - acc: 0.02 - ETA: 2s - loss: -1.4990 - acc: 0.02 - ETA: 2s - loss: -2.2599 - acc: 0.02 - ETA: 1s - loss: -2.2872 - acc: 0.02 - ETA: 1s - loss: -2.2038 - acc: 0.02 - ETA: 0s - loss: -143.3395 - acc: 0.01 - ETA: 0s - loss: -127.4838 - acc: 0.02 - 5s 5ms/step - loss: -151.5845 - acc: 0.0201\n",
      "Epoch 385/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.8161 - acc: 0.02 - ETA: 3s - loss: -3.8495 - acc: 0.01 - ETA: 3s - loss: -2.9717 - acc: 0.02 - ETA: 2s - loss: -247.4096 - acc: 0.01 - ETA: 2s - loss: -198.0444 - acc: 0.01 - ETA: 1s - loss: -166.0201 - acc: 0.01 - ETA: 1s - loss: -142.2840 - acc: 0.01 - ETA: 0s - loss: -124.8493 - acc: 0.01 - ETA: 0s - loss: -111.0306 - acc: 0.02 - 5s 5ms/step - loss: -106.0765 - acc: 0.0201\n",
      "Epoch 386/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1410.4009 - acc: 0.0000e+0 - ETA: 3s - loss: -705.4341 - acc: 0.0150    - ETA: 3s - loss: -470.7381 - acc: 0.02 - ETA: 2s - loss: -353.8511 - acc: 0.02 - ETA: 2s - loss: -283.3294 - acc: 0.02 - ETA: 1s - loss: -236.1596 - acc: 0.02 - ETA: 1s - loss: -202.6099 - acc: 0.02 - ETA: 0s - loss: -177.7226 - acc: 0.02 - ETA: 0s - loss: -160.0614 - acc: 0.02 - 5s 5ms/step - loss: -152.7959 - acc: 0.0201\n",
      "Epoch 387/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.1831 - acc: 0.01 - ETA: 3s - loss: -3.2526 - acc: 0.01 - ETA: 3s - loss: -2.9042 - acc: 0.01 - ETA: 2s - loss: -2.2981 - acc: 0.02 - ETA: 2s - loss: -2.4314 - acc: 0.02 - ETA: 1s - loss: -2.1124 - acc: 0.02 - ETA: 1s - loss: -2.1875 - acc: 0.02 - ETA: 0s - loss: -2.1390 - acc: 0.02 - ETA: 0s - loss: -1.9194 - acc: 0.02 - 5s 5ms/step - loss: -49.5101 - acc: 0.0201\n",
      "Epoch 388/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.1946 - acc: 0.01 - ETA: 3s - loss: -2.7502 - acc: 0.01 - ETA: 3s - loss: -1.8678 - acc: 0.02 - ETA: 2s - loss: -1.6117 - acc: 0.02 - ETA: 2s - loss: -242.2965 - acc: 0.02 - ETA: 1s - loss: -202.1360 - acc: 0.02 - ETA: 1s - loss: -173.5537 - acc: 0.02 - ETA: 0s - loss: -152.1350 - acc: 0.02 - ETA: 0s - loss: -135.1812 - acc: 0.02 - 5s 6ms/step - loss: -211.7269 - acc: 0.0201\n",
      "Epoch 389/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.5387 - acc: 0.02 - ETA: 3s - loss: -2.8263 - acc: 0.01 - ETA: 3s - loss: -3.3062 - acc: 0.01 - ETA: 2s - loss: -2.8942 - acc: 0.02 - ETA: 2s - loss: -2.4512 - acc: 0.02 - ETA: 1s - loss: -2.1073 - acc: 0.02 - ETA: 1s - loss: -2.1472 - acc: 0.02 - ETA: 0s - loss: -1.9784 - acc: 0.02 - ETA: 0s - loss: -1.9611 - acc: 0.02 - 5s 5ms/step - loss: -1.8841 - acc: 0.0201\n",
      "Epoch 390/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.7755 - acc: 0.01 - ETA: 3s - loss: -0.7403 - acc: 0.02 - ETA: 3s - loss: -0.9510 - acc: 0.02 - ETA: 2s - loss: -337.8667 - acc: 0.01 - ETA: 2s - loss: -270.4971 - acc: 0.02 - ETA: 1s - loss: -226.3700 - acc: 0.01 - ETA: 1s - loss: -194.3407 - acc: 0.01 - ETA: 0s - loss: -170.2080 - acc: 0.01 - ETA: 0s - loss: -151.4330 - acc: 0.02 - 5s 5ms/step - loss: -144.6650 - acc: 0.0201\n",
      "Epoch 391/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.4859 - acc: 0.02 - ETA: 3s - loss: -2.8152 - acc: 0.01 - ETA: 3s - loss: -2.1016 - acc: 0.02 - ETA: 2s - loss: -186.5632 - acc: 0.01 - ETA: 2s - loss: -150.7490 - acc: 0.01 - ETA: 1s - loss: -125.8569 - acc: 0.01 - ETA: 1s - loss: -107.9201 - acc: 0.02 - ETA: 0s - loss: -206.7203 - acc: 0.02 - ETA: 0s - loss: -183.8675 - acc: 0.02 - 5s 5ms/step - loss: -245.7240 - acc: 0.0201\n",
      "Epoch 392/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5425 - acc: 0.02 - ETA: 3s - loss: -0.6395 - acc: 0.02 - ETA: 3s - loss: -1.3631 - acc: 0.02 - ETA: 2s - loss: -2.0848 - acc: 0.02 - ETA: 2s - loss: -1.7598 - acc: 0.02 - ETA: 1s - loss: -1.8379 - acc: 0.02 - ETA: 1s - loss: -1.8935 - acc: 0.02 - ETA: 0s - loss: -1.7741 - acc: 0.02 - ETA: 0s - loss: -1.8704 - acc: 0.02 - 5s 5ms/step - loss: -1.8676 - acc: 0.0201\n",
      "Epoch 393/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1195.6501 - acc: 0.0000e+0 - ETA: 3s - loss: -598.8857 - acc: 0.0100    - ETA: 3s - loss: -399.9609 - acc: 0.01 - ETA: 2s - loss: -300.2773 - acc: 0.01 - ETA: 2s - loss: -240.6605 - acc: 0.01 - ETA: 1s - loss: -200.5720 - acc: 0.02 - ETA: 1s - loss: -172.4402 - acc: 0.02 - ETA: 0s - loss: -150.9323 - acc: 0.02 - ETA: 0s - loss: -134.3288 - acc: 0.02 - 5s 5ms/step - loss: -128.3781 - acc: 0.0201\n",
      "Epoch 394/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6440 - acc: 0.02 - ETA: 3s - loss: -2.7586 - acc: 0.01 - ETA: 3s - loss: -2.6518 - acc: 0.01 - ETA: 2s - loss: -2.1685 - acc: 0.02 - ETA: 2s - loss: -1.7583 - acc: 0.02 - ETA: 1s - loss: -1.7997 - acc: 0.02 - ETA: 1s - loss: -1.6813 - acc: 0.02 - ETA: 0s - loss: -1.5960 - acc: 0.02 - ETA: 0s - loss: -151.5156 - acc: 0.02 - 5s 5ms/step - loss: -144.6886 - acc: 0.0201\n",
      "Epoch 395/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4110 - acc: 0.03 - ETA: 3s - loss: -771.2066 - acc: 0.01 - ETA: 3s - loss: -514.1761 - acc: 0.02 - ETA: 2s - loss: -385.7868 - acc: 0.02 - ETA: 2s - loss: -309.4538 - acc: 0.02 - ETA: 1s - loss: -257.9991 - acc: 0.02 - ETA: 1s - loss: -221.4646 - acc: 0.02 - ETA: 0s - loss: -385.4924 - acc: 0.02 - ETA: 0s - loss: -342.6922 - acc: 0.02 - 5s 5ms/step - loss: -327.0782 - acc: 0.0201\n",
      "Epoch 396/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1513 - acc: 0.03 - ETA: 3s - loss: -2.7032 - acc: 0.02 - ETA: 3s - loss: -381.4524 - acc: 0.01 - ETA: 2s - loss: -286.1076 - acc: 0.01 - ETA: 2s - loss: -229.0573 - acc: 0.01 - ETA: 1s - loss: -190.9861 - acc: 0.02 - ETA: 1s - loss: -165.0172 - acc: 0.01 - ETA: 0s - loss: -144.4025 - acc: 0.02 - ETA: 0s - loss: -128.9068 - acc: 0.02 - 5s 5ms/step - loss: -123.1380 - acc: 0.0201\n",
      "Epoch 397/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1210.4254 - acc: 0.0000e+0 - ETA: 3s - loss: -607.2609 - acc: 0.0050    - ETA: 3s - loss: -405.4782 - acc: 0.01 - ETA: 2s - loss: -306.5682 - acc: 0.01 - ETA: 2s - loss: -245.4354 - acc: 0.01 - ETA: 1s - loss: -204.8263 - acc: 0.01 - ETA: 1s - loss: -175.8182 - acc: 0.01 - ETA: 0s - loss: -153.8936 - acc: 0.01 - ETA: 0s - loss: -137.0393 - acc: 0.01 - 5s 5ms/step - loss: -130.7925 - acc: 0.0201\n",
      "Epoch 398/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1540.4387 - acc: 0.0000e+0 - ETA: 3s - loss: -771.0586 - acc: 0.0100    - ETA: 3s - loss: -514.9316 - acc: 0.01 - ETA: 2s - loss: -386.2285 - acc: 0.01 - ETA: 2s - loss: -309.0777 - acc: 0.02 - ETA: 1s - loss: -257.5053 - acc: 0.02 - ETA: 1s - loss: -221.2375 - acc: 0.02 - ETA: 0s - loss: -193.8637 - acc: 0.02 - ETA: 0s - loss: -172.9074 - acc: 0.02 - 5s 5ms/step - loss: -220.8899 - acc: 0.0201\n",
      "Epoch 399/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.7495 - acc: 0.01 - ETA: 3s - loss: -4.2041 - acc: 0.01 - ETA: 3s - loss: -3.2558 - acc: 0.02 - ETA: 2s - loss: -2.7968 - acc: 0.02 - ETA: 2s - loss: -3.0654 - acc: 0.02 - ETA: 1s - loss: -3.0400 - acc: 0.02 - ETA: 1s - loss: -2.6609 - acc: 0.02 - ETA: 0s - loss: -134.4417 - acc: 0.02 - ETA: 0s - loss: -119.5071 - acc: 0.02 - 5s 5ms/step - loss: -114.1101 - acc: 0.0201\n",
      "Epoch 400/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4953 - acc: 0.04 - ETA: 3s - loss: -0.8527 - acc: 0.03 - ETA: 3s - loss: -3.2298 - acc: 0.02 - ETA: 2s - loss: -2.4656 - acc: 0.02 - ETA: 2s - loss: -2.1984 - acc: 0.02 - ETA: 1s - loss: -2.2781 - acc: 0.02 - ETA: 1s - loss: -187.5505 - acc: 0.02 - ETA: 0s - loss: -165.1885 - acc: 0.01 - ETA: 0s - loss: -146.9786 - acc: 0.02 - 5s 5ms/step - loss: -140.3614 - acc: 0.0201\n",
      "Epoch 401/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9890 - acc: 0.02 - ETA: 3s - loss: -0.5581 - acc: 0.03 - ETA: 3s - loss: -0.9072 - acc: 0.03 - ETA: 2s - loss: -0.8073 - acc: 0.03 - ETA: 2s - loss: -1.4506 - acc: 0.02 - ETA: 1s - loss: -1.4698 - acc: 0.02 - ETA: 1s - loss: -1.5201 - acc: 0.02 - ETA: 0s - loss: -1.5648 - acc: 0.02 - ETA: 0s - loss: -166.2716 - acc: 0.02 - 5s 5ms/step - loss: -214.5915 - acc: 0.0201\n",
      "Epoch 402/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8887 - acc: 0.02 - ETA: 3s - loss: -1.3438 - acc: 0.03 - ETA: 3s - loss: -1.4399 - acc: 0.02 - ETA: 2s - loss: -1.4271 - acc: 0.02 - ETA: 2s - loss: -1.2978 - acc: 0.02 - ETA: 1s - loss: -2.5416 - acc: 0.02 - ETA: 1s - loss: -2.6369 - acc: 0.01 - ETA: 0s - loss: -2.6650 - acc: 0.01 - ETA: 0s - loss: -2.6781 - acc: 0.01 - 5s 5ms/step - loss: -2.5681 - acc: 0.0170\n",
      "Epoch 403/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0848 - acc: 0.02 - ETA: 3s - loss: -2.3636 - acc: 0.01 - ETA: 3s - loss: -1.8581 - acc: 0.01 - ETA: 2s - loss: -1.5403 - acc: 0.01 - ETA: 2s - loss: -1.4347 - acc: 0.01 - ETA: 1s - loss: -1.3489 - acc: 0.01 - ETA: 1s - loss: -1.2997 - acc: 0.01 - ETA: 0s - loss: -1.4438 - acc: 0.01 - ETA: 0s - loss: -1.7851 - acc: 0.01 - 5s 5ms/step - loss: -1.7897 - acc: 0.0170\n",
      "Epoch 404/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4484 - acc: 0.01 - ETA: 3s - loss: -670.4382 - acc: 0.00 - ETA: 3s - loss: -448.0078 - acc: 0.01 - ETA: 2s - loss: -738.1971 - acc: 0.00 - ETA: 2s - loss: -590.6526 - acc: 0.01 - ETA: 1s - loss: -492.3850 - acc: 0.01 - ETA: 1s - loss: -422.0755 - acc: 0.01 - ETA: 0s - loss: -529.1663 - acc: 0.01 - ETA: 0s - loss: -470.3929 - acc: 0.01 - 5s 5ms/step - loss: -448.9995 - acc: 0.0170\n",
      "Epoch 405/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1126.0382 - acc: 0.0000e+0 - ETA: 3s - loss: -563.6311 - acc: 0.0100    - ETA: 3s - loss: -889.7313 - acc: 0.00 - ETA: 2s - loss: -667.4249 - acc: 0.01 - ETA: 2s - loss: -534.0929 - acc: 0.01 - ETA: 1s - loss: -445.2521 - acc: 0.01 - ETA: 1s - loss: -381.8777 - acc: 0.01 - ETA: 0s - loss: -334.3101 - acc: 0.01 - ETA: 0s - loss: -297.2867 - acc: 0.01 - 5s 5ms/step - loss: -339.5244 - acc: 0.0170\n",
      "Epoch 406/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.5643 - acc: 0.03 - ETA: 3s - loss: -1.1616 - acc: 0.03 - ETA: 3s - loss: -425.7525 - acc: 0.02 - ETA: 2s - loss: -706.4587 - acc: 0.01 - ETA: 2s - loss: -565.6961 - acc: 0.01 - ETA: 1s - loss: -471.4553 - acc: 0.01 - ETA: 1s - loss: -404.1779 - acc: 0.01 - ETA: 0s - loss: -353.8791 - acc: 0.01 - ETA: 0s - loss: -314.5775 - acc: 0.02 - 5s 5ms/step - loss: -356.1237 - acc: 0.0201\n",
      "Epoch 407/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1415.5090 - acc: 0.0000e+0 - ETA: 3s - loss: -710.7696 - acc: 0.0050    - ETA: 3s - loss: -474.2451 - acc: 0.01 - ETA: 2s - loss: -355.7365 - acc: 0.02 - ETA: 2s - loss: -285.0402 - acc: 0.02 - ETA: 1s - loss: -237.5878 - acc: 0.02 - ETA: 1s - loss: -203.9633 - acc: 0.02 - ETA: 0s - loss: -178.6100 - acc: 0.02 - ETA: 0s - loss: -158.8592 - acc: 0.02 - 5s 5ms/step - loss: -234.6376 - acc: 0.0201\n",
      "Epoch 408/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -7.7082 - acc: 0.02 - ETA: 3s - loss: -776.1872 - acc: 0.01 - ETA: 3s - loss: -517.9749 - acc: 0.01 - ETA: 2s - loss: -388.5079 - acc: 0.02 - ETA: 2s - loss: -310.8480 - acc: 0.02 - ETA: 1s - loss: -259.1303 - acc: 0.02 - ETA: 1s - loss: -222.3085 - acc: 0.02 - ETA: 0s - loss: -194.9028 - acc: 0.02 - ETA: 0s - loss: -173.3195 - acc: 0.02 - 5s 5ms/step - loss: -242.4787 - acc: 0.0201\n",
      "Epoch 409/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1135.1078 - acc: 0.0000e+0 - ETA: 3s - loss: -573.1619 - acc: 0.0050    - ETA: 3s - loss: -382.2396 - acc: 0.02 - ETA: 2s - loss: -286.8160 - acc: 0.02 - ETA: 2s - loss: -230.2263 - acc: 0.02 - ETA: 1s - loss: -192.1523 - acc: 0.02 - ETA: 1s - loss: -164.8752 - acc: 0.02 - ETA: 0s - loss: -286.3902 - acc: 0.02 - ETA: 0s - loss: -254.8017 - acc: 0.02 - 5s 5ms/step - loss: -290.9323 - acc: 0.0191\n",
      "Epoch 410/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.1622 - acc: 0.020 - ETA: 3s - loss: -1.6273 - acc: 0.02 - ETA: 3s - loss: -1.4191 - acc: 0.02 - ETA: 2s - loss: -1.5228 - acc: 0.02 - ETA: 2s - loss: -214.0132 - acc: 0.01 - ETA: 1s - loss: -178.5701 - acc: 0.01 - ETA: 1s - loss: -153.3461 - acc: 0.01 - ETA: 0s - loss: -134.3127 - acc: 0.01 - ETA: 0s - loss: -119.7013 - acc: 0.01 - 5s 5ms/step - loss: -161.8646 - acc: 0.0170\n",
      "Epoch 411/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9983 - acc: 0.01 - ETA: 3s - loss: -2.0867 - acc: 0.01 - ETA: 3s - loss: -1.7174 - acc: 0.01 - ETA: 2s - loss: -1.9486 - acc: 0.01 - ETA: 2s - loss: -2.0163 - acc: 0.01 - ETA: 1s - loss: -1.8048 - acc: 0.01 - ETA: 1s - loss: -1.8536 - acc: 0.01 - ETA: 0s - loss: -1.7449 - acc: 0.01 - ETA: 0s - loss: -1.7469 - acc: 0.01 - 5s 5ms/step - loss: -65.1037 - acc: 0.0170\n",
      "Epoch 412/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.7345 - acc: 0.01 - ETA: 3s - loss: -2.2716 - acc: 0.02 - ETA: 3s - loss: -1.7243 - acc: 0.02 - ETA: 2s - loss: -1.5052 - acc: 0.02 - ETA: 2s - loss: -1.8292 - acc: 0.02 - ETA: 1s - loss: -1.9601 - acc: 0.02 - ETA: 1s - loss: -153.4440 - acc: 0.01 - ETA: 0s - loss: -134.3363 - acc: 0.01 - ETA: 0s - loss: -121.4960 - acc: 0.01 - 5s 5ms/step - loss: -116.0384 - acc: 0.0170\n",
      "Epoch 413/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4918 - acc: 0.03 - ETA: 3s - loss: -0.4575 - acc: 0.02 - ETA: 3s - loss: -0.5357 - acc: 0.02 - ETA: 2s - loss: -314.5368 - acc: 0.01 - ETA: 2s - loss: -255.5711 - acc: 0.01 - ETA: 1s - loss: -213.1713 - acc: 0.01 - ETA: 1s - loss: -183.4675 - acc: 0.01 - ETA: 0s - loss: -160.5362 - acc: 0.01 - ETA: 0s - loss: -142.8665 - acc: 0.01 - 5s 5ms/step - loss: -136.5193 - acc: 0.0170\n",
      "Epoch 414/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -12.9488 - acc: 0.010 - ETA: 3s - loss: -6.8429 - acc: 0.010 - ETA: 3s - loss: -539.4901 - acc: 0.00 - ETA: 2s - loss: -404.8778 - acc: 0.01 - ETA: 2s - loss: -324.1202 - acc: 0.01 - ETA: 1s - loss: -270.1607 - acc: 0.01 - ETA: 1s - loss: -231.7075 - acc: 0.01 - ETA: 0s - loss: -202.7766 - acc: 0.02 - ETA: 0s - loss: -322.9188 - acc: 0.01 - 5s 5ms/step - loss: -338.3393 - acc: 0.0170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 415/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5486 - acc: 0.03 - ETA: 3s - loss: -708.5361 - acc: 0.01 - ETA: 3s - loss: -472.9504 - acc: 0.01 - ETA: 2s - loss: -355.1535 - acc: 0.01 - ETA: 2s - loss: -284.3466 - acc: 0.01 - ETA: 1s - loss: -237.2541 - acc: 0.01 - ETA: 1s - loss: -207.8782 - acc: 0.01 - ETA: 0s - loss: -182.0360 - acc: 0.01 - ETA: 0s - loss: -161.8666 - acc: 0.01 - 5s 5ms/step - loss: -210.2788 - acc: 0.0170\n",
      "Epoch 416/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -989.9213 - acc: 0.0000e+ - ETA: 3s - loss: -495.0726 - acc: 0.0150   - ETA: 3s - loss: -330.7651 - acc: 0.01 - ETA: 2s - loss: -248.4642 - acc: 0.02 - ETA: 2s - loss: -199.2401 - acc: 0.01 - ETA: 1s - loss: -391.6338 - acc: 0.01 - ETA: 1s - loss: -336.0027 - acc: 0.01 - ETA: 0s - loss: -294.3326 - acc: 0.01 - ETA: 0s - loss: -261.7321 - acc: 0.01 - 5s 5ms/step - loss: -249.8078 - acc: 0.0170\n",
      "Epoch 417/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.9026 - acc: 0.01 - ETA: 3s - loss: -493.5209 - acc: 0.00 - ETA: 3s - loss: -329.5495 - acc: 0.01 - ETA: 2s - loss: -247.4475 - acc: 0.01 - ETA: 2s - loss: -198.3121 - acc: 0.01 - ETA: 1s - loss: -165.4152 - acc: 0.01 - ETA: 1s - loss: -142.1713 - acc: 0.01 - ETA: 0s - loss: -124.5057 - acc: 0.01 - ETA: 0s - loss: -219.4844 - acc: 0.01 - 5s 5ms/step - loss: -209.4962 - acc: 0.0170\n",
      "Epoch 418/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1399.4432 - acc: 0.0000e+0 - ETA: 3s - loss: -700.6491 - acc: 0.0150    - ETA: 3s - loss: -468.0979 - acc: 0.01 - ETA: 2s - loss: -351.0567 - acc: 0.02 - ETA: 2s - loss: -491.9473 - acc: 0.02 - ETA: 1s - loss: -634.9286 - acc: 0.01 - ETA: 1s - loss: -544.3806 - acc: 0.01 - ETA: 0s - loss: -476.3830 - acc: 0.02 - ETA: 0s - loss: -423.5275 - acc: 0.02 - 5s 5ms/step - loss: -434.4522 - acc: 0.0201\n",
      "Epoch 419/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.2204 - acc: 0.01 - ETA: 3s - loss: -2.6709 - acc: 0.01 - ETA: 3s - loss: -2.1423 - acc: 0.01 - ETA: 2s - loss: -1.9725 - acc: 0.01 - ETA: 2s - loss: -2.2043 - acc: 0.01 - ETA: 1s - loss: -1.9230 - acc: 0.01 - ETA: 1s - loss: -1.7026 - acc: 0.02 - ETA: 0s - loss: -1.5320 - acc: 0.02 - ETA: 0s - loss: -1.6610 - acc: 0.02 - 5s 5ms/step - loss: -2.1458 - acc: 0.0201\n",
      "Epoch 420/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0755 - acc: 0.03 - ETA: 3s - loss: -1.7969 - acc: 0.03 - ETA: 3s - loss: -1.9184 - acc: 0.03 - ETA: 2s - loss: -1.6279 - acc: 0.03 - ETA: 2s - loss: -1.2725 - acc: 0.03 - ETA: 1s - loss: -1.2023 - acc: 0.02 - ETA: 1s - loss: -212.4893 - acc: 0.02 - ETA: 0s - loss: -318.2314 - acc: 0.02 - ETA: 0s - loss: -283.0369 - acc: 0.02 - 5s 5ms/step - loss: -300.1901 - acc: 0.0201\n",
      "Epoch 421/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3747 - acc: 0.05 - ETA: 3s - loss: -0.7526 - acc: 0.04 - ETA: 3s - loss: -490.9398 - acc: 0.03 - ETA: 2s - loss: -368.2554 - acc: 0.03 - ETA: 2s - loss: -616.4436 - acc: 0.02 - ETA: 1s - loss: -514.2105 - acc: 0.02 - ETA: 1s - loss: -440.9641 - acc: 0.02 - ETA: 0s - loss: -386.7062 - acc: 0.02 - ETA: 0s - loss: -343.7454 - acc: 0.02 - 5s 5ms/step - loss: -348.6047 - acc: 0.0201\n",
      "Epoch 422/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6067 - acc: 0.02 - ETA: 3s - loss: -0.7861 - acc: 0.02 - ETA: 3s - loss: -1.5739 - acc: 0.02 - ETA: 2s - loss: -1.7960 - acc: 0.02 - ETA: 2s - loss: -2.0609 - acc: 0.02 - ETA: 1s - loss: -1.8359 - acc: 0.02 - ETA: 1s - loss: -1.8068 - acc: 0.02 - ETA: 0s - loss: -1.6104 - acc: 0.02 - ETA: 0s - loss: -110.4607 - acc: 0.02 - 5s 5ms/step - loss: -105.6993 - acc: 0.0201\n",
      "Epoch 423/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2960 - acc: 0.02 - ETA: 3s - loss: -1.8545 - acc: 0.02 - ETA: 3s - loss: -1.5495 - acc: 0.02 - ETA: 2s - loss: -1.2787 - acc: 0.02 - ETA: 2s - loss: -1.0740 - acc: 0.02 - ETA: 1s - loss: -0.9996 - acc: 0.02 - ETA: 1s - loss: -1.4448 - acc: 0.02 - ETA: 0s - loss: -1.4853 - acc: 0.02 - ETA: 0s - loss: -2.1496 - acc: 0.02 - 5s 5ms/step - loss: -65.5310 - acc: 0.0201\n",
      "Epoch 424/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9976 - acc: 0.01 - ETA: 3s - loss: -2.0601 - acc: 0.01 - ETA: 3s - loss: -327.4522 - acc: 0.00 - ETA: 2s - loss: -245.7780 - acc: 0.01 - ETA: 2s - loss: -196.8843 - acc: 0.01 - ETA: 1s - loss: -164.3201 - acc: 0.01 - ETA: 1s - loss: -141.3932 - acc: 0.01 - ETA: 0s - loss: -123.7733 - acc: 0.01 - ETA: 0s - loss: -257.4058 - acc: 0.01 - 5s 5ms/step - loss: -245.6645 - acc: 0.0201\n",
      "Epoch 425/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1575 - acc: 0.04 - ETA: 3s - loss: -0.2456 - acc: 0.04 - ETA: 3s - loss: -0.6357 - acc: 0.03 - ETA: 2s - loss: -0.7938 - acc: 0.02 - ETA: 2s - loss: -1.0676 - acc: 0.02 - ETA: 1s - loss: -1.1040 - acc: 0.02 - ETA: 1s - loss: -191.4626 - acc: 0.02 - ETA: 0s - loss: -168.0707 - acc: 0.02 - ETA: 0s - loss: -149.6953 - acc: 0.02 - 5s 5ms/step - loss: -198.5955 - acc: 0.0201\n",
      "Epoch 426/500\n",
      "943/943 [==============================] - ETA: 4s - loss: 0.2750 - acc: 0.040 - ETA: 3s - loss: -0.6836 - acc: 0.04 - ETA: 3s - loss: -354.5151 - acc: 0.02 - ETA: 2s - loss: -265.8514 - acc: 0.03 - ETA: 2s - loss: -377.6846 - acc: 0.02 - ETA: 1s - loss: -315.4716 - acc: 0.02 - ETA: 1s - loss: -271.5515 - acc: 0.02 - ETA: 0s - loss: -238.2627 - acc: 0.01 - ETA: 0s - loss: -211.9353 - acc: 0.02 - 5s 5ms/step - loss: -202.3803 - acc: 0.0201\n",
      "Epoch 427/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0648 - acc: 0.04 - ETA: 3s - loss: -0.9593 - acc: 0.03 - ETA: 3s - loss: -491.6962 - acc: 0.02 - ETA: 2s - loss: -368.9172 - acc: 0.02 - ETA: 2s - loss: -295.2799 - acc: 0.02 - ETA: 1s - loss: -492.2254 - acc: 0.02 - ETA: 1s - loss: -422.3138 - acc: 0.02 - ETA: 0s - loss: -369.6729 - acc: 0.02 - ETA: 0s - loss: -328.6872 - acc: 0.02 - 5s 5ms/step - loss: -333.4359 - acc: 0.0201\n",
      "Epoch 428/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9153 - acc: 0.03 - ETA: 3s - loss: -5.2140 - acc: 0.02 - ETA: 3s - loss: -3.6825 - acc: 0.02 - ETA: 2s - loss: -3.5813 - acc: 0.02 - ETA: 2s - loss: -3.0737 - acc: 0.02 - ETA: 1s - loss: -191.8185 - acc: 0.01 - ETA: 1s - loss: -164.6687 - acc: 0.01 - ETA: 0s - loss: -144.4564 - acc: 0.01 - ETA: 0s - loss: -128.3715 - acc: 0.02 - 5s 5ms/step - loss: -185.4732 - acc: 0.0201\n",
      "Epoch 429/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.4214 - acc: 0.02 - ETA: 3s - loss: -2.4133 - acc: 0.02 - ETA: 3s - loss: -1.9722 - acc: 0.02 - ETA: 2s - loss: -2.2748 - acc: 0.02 - ETA: 2s - loss: -1.8480 - acc: 0.02 - ETA: 1s - loss: -224.8569 - acc: 0.02 - ETA: 1s - loss: -192.9561 - acc: 0.02 - ETA: 0s - loss: -168.8761 - acc: 0.02 - ETA: 0s - loss: -150.2517 - acc: 0.02 - 5s 5ms/step - loss: -164.0573 - acc: 0.0201\n",
      "Epoch 430/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -6.4855 - acc: 0.01 - ETA: 3s - loss: -4.3506 - acc: 0.01 - ETA: 3s - loss: -380.6322 - acc: 0.00 - ETA: 2s - loss: -285.4719 - acc: 0.01 - ETA: 2s - loss: -228.6837 - acc: 0.01 - ETA: 1s - loss: -190.7830 - acc: 0.01 - ETA: 1s - loss: -163.7045 - acc: 0.01 - ETA: 0s - loss: -143.3275 - acc: 0.02 - ETA: 0s - loss: -127.7210 - acc: 0.02 - 5s 5ms/step - loss: -177.3918 - acc: 0.0201\n",
      "Epoch 431/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.6587 - acc: 0.01 - ETA: 3s - loss: -830.3283 - acc: 0.00 - ETA: 3s - loss: -553.5395 - acc: 0.01 - ETA: 2s - loss: -415.6363 - acc: 0.02 - ETA: 2s - loss: -332.7411 - acc: 0.02 - ETA: 1s - loss: -398.6266 - acc: 0.01 - ETA: 1s - loss: -343.1180 - acc: 0.01 - ETA: 0s - loss: -300.3086 - acc: 0.02 - ETA: 0s - loss: -267.1680 - acc: 0.02 - 5s 5ms/step - loss: -255.0415 - acc: 0.0201\n",
      "Epoch 432/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0227 - acc: 0.02 - ETA: 3s - loss: -1.7820 - acc: 0.01 - ETA: 3s - loss: -577.3031 - acc: 0.01 - ETA: 2s - loss: -433.0616 - acc: 0.01 - ETA: 2s - loss: -346.8877 - acc: 0.01 - ETA: 1s - loss: -290.6178 - acc: 0.01 - ETA: 1s - loss: -249.2037 - acc: 0.02 - ETA: 0s - loss: -218.4349 - acc: 0.02 - ETA: 0s - loss: -194.2838 - acc: 0.02 - 5s 5ms/step - loss: -233.2910 - acc: 0.0201\n",
      "Epoch 433/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.6788 - acc: 0.02 - ETA: 3s - loss: -1.6414 - acc: 0.02 - ETA: 3s - loss: -1.8784 - acc: 0.01 - ETA: 2s - loss: -1.9282 - acc: 0.01 - ETA: 2s - loss: -1.6604 - acc: 0.02 - ETA: 1s - loss: -215.2991 - acc: 0.01 - ETA: 1s - loss: -184.5863 - acc: 0.01 - ETA: 0s - loss: -161.7138 - acc: 0.01 - ETA: 0s - loss: -143.9546 - acc: 0.02 - 5s 5ms/step - loss: -137.4339 - acc: 0.0201\n",
      "Epoch 434/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.0639 - acc: 0.02 - ETA: 3s - loss: -2.4256 - acc: 0.01 - ETA: 3s - loss: -1.7408 - acc: 0.03 - ETA: 2s - loss: -1.4803 - acc: 0.03 - ETA: 2s - loss: -270.2615 - acc: 0.02 - ETA: 1s - loss: -225.4454 - acc: 0.02 - ETA: 1s - loss: -195.7622 - acc: 0.02 - ETA: 0s - loss: -173.7055 - acc: 0.02 - ETA: 0s - loss: -154.4743 - acc: 0.02 - 5s 5ms/step - loss: -147.3897 - acc: 0.0201\n",
      "Epoch 435/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1671.5366 - acc: 0.0000e+0 - ETA: 3s - loss: -835.7424 - acc: 0.0150    - ETA: 3s - loss: -558.0812 - acc: 0.01 - ETA: 2s - loss: -420.0662 - acc: 0.01 - ETA: 2s - loss: -336.0966 - acc: 0.01 - ETA: 1s - loss: -280.4070 - acc: 0.01 - ETA: 1s - loss: -240.6042 - acc: 0.01 - ETA: 0s - loss: -210.5851 - acc: 0.02 - ETA: 0s - loss: -187.4329 - acc: 0.02 - 5s 5ms/step - loss: -179.2365 - acc: 0.0201\n",
      "Epoch 436/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.1939 - acc: 0.01 - ETA: 3s - loss: -1.3424 - acc: 0.02 - ETA: 3s - loss: -12.2382 - acc: 0.013 - ETA: 2s - loss: -9.6357 - acc: 0.015 - ETA: 2s - loss: -7.9275 - acc: 0.01 - ETA: 1s - loss: -6.7621 - acc: 0.01 - ETA: 1s - loss: -5.7875 - acc: 0.01 - ETA: 0s - loss: -5.3219 - acc: 0.01 - ETA: 0s - loss: -4.8500 - acc: 0.01 - 5s 5ms/step - loss: -5.4305 - acc: 0.0170\n",
      "Epoch 437/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.1137 - acc: 0.01 - ETA: 3s - loss: -650.7646 - acc: 0.00 - ETA: 3s - loss: -907.4222 - acc: 0.00 - ETA: 2s - loss: -680.5150 - acc: 0.00 - ETA: 2s - loss: -544.6334 - acc: 0.01 - ETA: 1s - loss: -454.1311 - acc: 0.01 - ETA: 1s - loss: -389.3251 - acc: 0.01 - ETA: 0s - loss: -340.8335 - acc: 0.01 - ETA: 0s - loss: -303.4025 - acc: 0.01 - 5s 5ms/step - loss: -289.5753 - acc: 0.0170\n",
      "Epoch 438/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.1108 - acc: 0.01 - ETA: 3s - loss: -3.2186 - acc: 0.01 - ETA: 3s - loss: -2.9498 - acc: 0.01 - ETA: 2s - loss: -3.1204 - acc: 0.01 - ETA: 2s - loss: -2.7279 - acc: 0.01 - ETA: 1s - loss: -2.6840 - acc: 0.01 - ETA: 1s - loss: -2.4221 - acc: 0.01 - ETA: 0s - loss: -2.0991 - acc: 0.01 - ETA: 0s - loss: -2.7550 - acc: 0.01 - 5s 5ms/step - loss: -2.6415 - acc: 0.0170\n",
      "Epoch 439/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8609 - acc: 0.03 - ETA: 3s - loss: -2.2487 - acc: 0.02 - ETA: 3s - loss: -1.9217 - acc: 0.02 - ETA: 2s - loss: -1.7822 - acc: 0.02 - ETA: 2s - loss: -228.7908 - acc: 0.01 - ETA: 1s - loss: -193.7129 - acc: 0.01 - ETA: 1s - loss: -166.3411 - acc: 0.01 - ETA: 0s - loss: -145.6568 - acc: 0.01 - ETA: 0s - loss: -129.7674 - acc: 0.01 - 5s 5ms/step - loss: -123.8566 - acc: 0.0170\n",
      "Epoch 440/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.6415 - acc: 0.01 - ETA: 3s - loss: -1.8076 - acc: 0.02 - ETA: 3s - loss: -1.3505 - acc: 0.02 - ETA: 2s - loss: -2.4408 - acc: 0.02 - ETA: 2s - loss: -2.6737 - acc: 0.01 - ETA: 1s - loss: -2.6726 - acc: 0.01 - ETA: 1s - loss: -2.2681 - acc: 0.01 - ETA: 0s - loss: -114.0209 - acc: 0.01 - ETA: 0s - loss: -101.7087 - acc: 0.01 - 5s 5ms/step - loss: -97.0988 - acc: 0.0170\n",
      "Epoch 441/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.2589 - acc: 0.01 - ETA: 3s - loss: -3.2915 - acc: 0.02 - ETA: 3s - loss: -2.4944 - acc: 0.02 - ETA: 2s - loss: -2.4407 - acc: 0.02 - ETA: 2s - loss: -2.2281 - acc: 0.02 - ETA: 1s - loss: -2.5453 - acc: 0.01 - ETA: 1s - loss: -2.3295 - acc: 0.01 - ETA: 0s - loss: -152.1623 - acc: 0.01 - ETA: 0s - loss: -135.4422 - acc: 0.01 - 5s 5ms/step - loss: -129.2786 - acc: 0.0170\n",
      "Epoch 442/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.8238 - acc: 0.01 - ETA: 3s - loss: -11.7623 - acc: 0.010 - ETA: 3s - loss: -9.5450 - acc: 0.010 - ETA: 2s - loss: -7.5059 - acc: 0.01 - ETA: 2s - loss: -6.0856 - acc: 0.01 - ETA: 1s - loss: -5.1578 - acc: 0.01 - ETA: 1s - loss: -6.6859 - acc: 0.01 - ETA: 0s - loss: -6.2007 - acc: 0.01 - ETA: 0s - loss: -5.7173 - acc: 0.01 - 5s 5ms/step - loss: -5.4545 - acc: 0.0170\n",
      "Epoch 443/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1321.8958 - acc: 0.0000e+0 - ETA: 3s - loss: -660.9791 - acc: 0.0250    - ETA: 3s - loss: -889.9015 - acc: 0.01 - ETA: 2s - loss: -667.6092 - acc: 0.02 - ETA: 2s - loss: -534.8606 - acc: 0.01 - ETA: 1s - loss: -446.3492 - acc: 0.01 - ETA: 1s - loss: -486.8496 - acc: 0.01 - ETA: 0s - loss: -426.1930 - acc: 0.01 - ETA: 0s - loss: -378.8940 - acc: 0.01 - 5s 5ms/step - loss: -361.6510 - acc: 0.0170\n",
      "Epoch 444/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.6620 - acc: 0.01 - ETA: 3s - loss: -413.9458 - acc: 0.00 - ETA: 3s - loss: -276.2931 - acc: 0.01 - ETA: 2s - loss: -208.1961 - acc: 0.01 - ETA: 2s - loss: -166.5795 - acc: 0.01 - ETA: 1s - loss: -139.1563 - acc: 0.01 - ETA: 1s - loss: -119.6246 - acc: 0.01 - ETA: 0s - loss: -104.8888 - acc: 0.01 - ETA: 0s - loss: -93.2960 - acc: 0.0178 - 5s 5ms/step - loss: -152.3170 - acc: 0.0170\n",
      "Epoch 445/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.2995 - acc: 0.01 - ETA: 3s - loss: -0.8320 - acc: 0.03 - ETA: 3s - loss: -0.6055 - acc: 0.03 - ETA: 2s - loss: -0.7609 - acc: 0.02 - ETA: 2s - loss: -227.4687 - acc: 0.02 - ETA: 1s - loss: -191.4091 - acc: 0.02 - ETA: 1s - loss: -164.2087 - acc: 0.02 - ETA: 0s - loss: -337.4123 - acc: 0.01 - ETA: 0s - loss: -300.1451 - acc: 0.01 - 5s 5ms/step - loss: -286.5148 - acc: 0.0170\n",
      "Epoch 446/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4027 - acc: 0.03 - ETA: 3s - loss: -4.8512 - acc: 0.02 - ETA: 3s - loss: -4.7878 - acc: 0.01 - ETA: 2s - loss: -4.6232 - acc: 0.01 - ETA: 2s - loss: -4.1422 - acc: 0.01 - ETA: 1s - loss: -3.5932 - acc: 0.01 - ETA: 1s - loss: -3.6194 - acc: 0.01 - ETA: 0s - loss: -3.1280 - acc: 0.01 - ETA: 0s - loss: -2.9882 - acc: 0.01 - 5s 5ms/step - loss: -2.9944 - acc: 0.0170\n",
      "Epoch 447/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.9680 - acc: 0.01 - ETA: 3s - loss: -0.8169 - acc: 0.02 - ETA: 3s - loss: -0.6803 - acc: 0.02 - ETA: 2s - loss: -2.1237 - acc: 0.02 - ETA: 2s - loss: -2.2287 - acc: 0.01 - ETA: 1s - loss: -2.2532 - acc: 0.01 - ETA: 1s - loss: -2.4501 - acc: 0.01 - ETA: 0s - loss: -2.5186 - acc: 0.01 - ETA: 0s - loss: -2.3370 - acc: 0.01 - 5s 5ms/step - loss: -32.4566 - acc: 0.0170\n",
      "Epoch 448/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.1006 - acc: 0.02 - ETA: 3s - loss: -1.6257 - acc: 0.02 - ETA: 3s - loss: -1.1657 - acc: 0.02 - ETA: 2s - loss: -1.2096 - acc: 0.02 - ETA: 2s - loss: -1.1870 - acc: 0.02 - ETA: 1s - loss: -5.1398 - acc: 0.02 - ETA: 1s - loss: -165.9029 - acc: 0.01 - ETA: 0s - loss: -145.2661 - acc: 0.01 - ETA: 0s - loss: -129.3598 - acc: 0.01 - 5s 5ms/step - loss: -153.6791 - acc: 0.0170\n",
      "Epoch 449/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1417.9373 - acc: 0.0000e+0 - ETA: 3s - loss: -709.8787 - acc: 0.0150    - ETA: 3s - loss: -473.3896 - acc: 0.01 - ETA: 2s - loss: -355.0410 - acc: 0.01 - ETA: 2s - loss: -284.2697 - acc: 0.01 - ETA: 1s - loss: -237.3271 - acc: 0.01 - ETA: 1s - loss: -204.0115 - acc: 0.01 - ETA: 0s - loss: -181.3297 - acc: 0.01 - ETA: 0s - loss: -161.2495 - acc: 0.01 - 5s 5ms/step - loss: -201.4875 - acc: 0.0170\n",
      "Epoch 450/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.2147 - acc: 0.01 - ETA: 3s - loss: -2.9742 - acc: 0.01 - ETA: 3s - loss: -2.5749 - acc: 0.01 - ETA: 2s - loss: -2.4124 - acc: 0.01 - ETA: 2s - loss: -2.3069 - acc: 0.01 - ETA: 1s - loss: -2.1239 - acc: 0.01 - ETA: 1s - loss: -2.3961 - acc: 0.01 - ETA: 0s - loss: -2.0857 - acc: 0.01 - ETA: 0s - loss: -2.0752 - acc: 0.01 - 5s 5ms/step - loss: -41.2079 - acc: 0.0170\n",
      "Epoch 451/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943/943 [==============================] - ETA: 4s - loss: 0.1683 - acc: 0.040 - ETA: 3s - loss: -0.4808 - acc: 0.03 - ETA: 3s - loss: -0.8444 - acc: 0.03 - ETA: 2s - loss: -1.1624 - acc: 0.03 - ETA: 2s - loss: -4.8813 - acc: 0.02 - ETA: 1s - loss: -203.2778 - acc: 0.02 - ETA: 1s - loss: -174.4298 - acc: 0.02 - ETA: 0s - loss: -152.9110 - acc: 0.01 - ETA: 0s - loss: -136.2426 - acc: 0.01 - 5s 5ms/step - loss: -130.3291 - acc: 0.0170\n",
      "Epoch 452/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.1716 - acc: 0.02 - ETA: 3s - loss: -709.9486 - acc: 0.01 - ETA: 3s - loss: -474.0653 - acc: 0.01 - ETA: 2s - loss: -356.6622 - acc: 0.01 - ETA: 2s - loss: -285.7409 - acc: 0.01 - ETA: 1s - loss: -238.3213 - acc: 0.01 - ETA: 1s - loss: -204.3908 - acc: 0.01 - ETA: 0s - loss: -178.9143 - acc: 0.01 - ETA: 0s - loss: -159.2453 - acc: 0.01 - 5s 5ms/step - loss: -151.9780 - acc: 0.0170\n",
      "Epoch 453/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.0029 - acc: 0.01 - ETA: 3s - loss: -0.8128 - acc: 0.01 - ETA: 3s - loss: -11.4014 - acc: 0.010 - ETA: 2s - loss: -9.0181 - acc: 0.010 - ETA: 2s - loss: -7.5733 - acc: 0.01 - ETA: 1s - loss: -6.7858 - acc: 0.01 - ETA: 1s - loss: -99.1472 - acc: 0.011 - ETA: 0s - loss: -87.0644 - acc: 0.013 - ETA: 0s - loss: -77.5674 - acc: 0.015 - 5s 5ms/step - loss: -74.0090 - acc: 0.0159\n",
      "Epoch 454/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.4105 - acc: 0.02 - ETA: 3s - loss: -1.1010 - acc: 0.02 - ETA: 3s - loss: -1.9384 - acc: 0.02 - ETA: 2s - loss: -2.3036 - acc: 0.01 - ETA: 2s - loss: -2.1624 - acc: 0.01 - ETA: 1s - loss: -179.1343 - acc: 0.01 - ETA: 1s - loss: -154.1265 - acc: 0.01 - ETA: 0s - loss: -135.2534 - acc: 0.01 - ETA: 0s - loss: -120.2365 - acc: 0.01 - 5s 5ms/step - loss: -114.7476 - acc: 0.0159\n",
      "Epoch 455/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -22.5867 - acc: 0.010 - ETA: 3s - loss: -541.5516 - acc: 0.00 - ETA: 3s - loss: -361.2338 - acc: 0.01 - ETA: 2s - loss: -271.0954 - acc: 0.01 - ETA: 2s - loss: -217.1258 - acc: 0.01 - ETA: 1s - loss: -181.0814 - acc: 0.01 - ETA: 1s - loss: -155.7272 - acc: 0.01 - ETA: 0s - loss: -137.7163 - acc: 0.01 - ETA: 0s - loss: -122.4489 - acc: 0.01 - 5s 5ms/step - loss: -172.4820 - acc: 0.0159\n",
      "Epoch 456/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.2707 - acc: 0.02 - ETA: 3s - loss: -3.0691 - acc: 0.02 - ETA: 3s - loss: -324.6816 - acc: 0.01 - ETA: 2s - loss: -243.8313 - acc: 0.01 - ETA: 2s - loss: -195.1057 - acc: 0.01 - ETA: 1s - loss: -313.1405 - acc: 0.01 - ETA: 1s - loss: -268.6208 - acc: 0.01 - ETA: 0s - loss: -235.1218 - acc: 0.01 - ETA: 0s - loss: -209.1141 - acc: 0.01 - 5s 5ms/step - loss: -282.3306 - acc: 0.0159\n",
      "Epoch 457/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -60.4257 - acc: 0.0000e+0 - ETA: 3s - loss: -31.6127 - acc: 0.0050    - ETA: 3s - loss: -21.4531 - acc: 0.013 - ETA: 2s - loss: -16.4980 - acc: 0.015 - ETA: 2s - loss: -13.2212 - acc: 0.016 - ETA: 1s - loss: -11.5200 - acc: 0.016 - ETA: 1s - loss: -9.9576 - acc: 0.018 - ETA: 0s - loss: -9.0383 - acc: 0.01 - ETA: 0s - loss: -8.4385 - acc: 0.01 - 5s 5ms/step - loss: -63.8146 - acc: 0.0159\n",
      "Epoch 458/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1540.4877 - acc: 0.0000e+0 - ETA: 3s - loss: -771.3168 - acc: 0.0100    - ETA: 3s - loss: -589.6513 - acc: 0.00 - ETA: 2s - loss: -442.8606 - acc: 0.00 - ETA: 2s - loss: -354.7279 - acc: 0.00 - ETA: 1s - loss: -295.8379 - acc: 0.00 - ETA: 1s - loss: -253.9524 - acc: 0.01 - ETA: 0s - loss: -223.9468 - acc: 0.01 - ETA: 0s - loss: -199.2206 - acc: 0.01 - 5s 5ms/step - loss: -190.1332 - acc: 0.0159\n",
      "Epoch 459/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.2558 - acc: 0.01 - ETA: 3s - loss: -1.7600 - acc: 0.02 - ETA: 3s - loss: -1.6628 - acc: 0.01 - ETA: 2s - loss: -2.7286 - acc: 0.01 - ETA: 2s - loss: -2.8510 - acc: 0.01 - ETA: 1s - loss: -2.4733 - acc: 0.01 - ETA: 1s - loss: -2.6276 - acc: 0.01 - ETA: 0s - loss: -2.9874 - acc: 0.01 - ETA: 0s - loss: -2.9875 - acc: 0.01 - 5s 5ms/step - loss: -2.8844 - acc: 0.0159\n",
      "Epoch 460/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9170 - acc: 0.01 - ETA: 3s - loss: -2.5202 - acc: 0.01 - ETA: 3s - loss: -111.7351 - acc: 0.00 - ETA: 2s - loss: -83.9496 - acc: 0.0125 - ETA: 2s - loss: -67.5091 - acc: 0.014 - ETA: 1s - loss: -57.0109 - acc: 0.013 - ETA: 1s - loss: -49.2547 - acc: 0.012 - ETA: 0s - loss: -43.1760 - acc: 0.013 - ETA: 0s - loss: -38.4524 - acc: 0.016 - 5s 5ms/step - loss: -99.8842 - acc: 0.0159\n",
      "Epoch 461/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.5547 - acc: 0.02 - ETA: 3s - loss: -1.6403 - acc: 0.02 - ETA: 3s - loss: -2.1436 - acc: 0.02 - ETA: 2s - loss: -120.3397 - acc: 0.01 - ETA: 2s - loss: -97.0949 - acc: 0.0140 - ETA: 1s - loss: -80.8827 - acc: 0.013 - ETA: 1s - loss: -69.3595 - acc: 0.017 - ETA: 0s - loss: -61.4972 - acc: 0.016 - ETA: 0s - loss: -55.1321 - acc: 0.015 - 5s 5ms/step - loss: -52.6507 - acc: 0.0159\n",
      "Epoch 462/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8837 - acc: 0.02 - ETA: 3s - loss: -0.6300 - acc: 0.02 - ETA: 3s - loss: -0.7058 - acc: 0.02 - ETA: 2s - loss: -1.8076 - acc: 0.02 - ETA: 2s - loss: -1.5321 - acc: 0.02 - ETA: 1s - loss: -178.0195 - acc: 0.01 - ETA: 1s - loss: -153.0752 - acc: 0.01 - ETA: 0s - loss: -134.3383 - acc: 0.01 - ETA: 0s - loss: -119.8575 - acc: 0.01 - 5s 5ms/step - loss: -184.8492 - acc: 0.0159\n",
      "Epoch 463/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.1297 - acc: 0.03 - ETA: 3s - loss: -267.6005 - acc: 0.01 - ETA: 3s - loss: -178.3831 - acc: 0.02 - ETA: 2s - loss: -139.4267 - acc: 0.01 - ETA: 2s - loss: -290.5505 - acc: 0.01 - ETA: 1s - loss: -242.2546 - acc: 0.01 - ETA: 1s - loss: -207.6578 - acc: 0.01 - ETA: 0s - loss: -182.3064 - acc: 0.01 - ETA: 0s - loss: -162.4539 - acc: 0.01 - 5s 5ms/step - loss: -202.4658 - acc: 0.0159\n",
      "Epoch 464/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.0587 - acc: 0.02 - ETA: 3s - loss: -2.3488 - acc: 0.01 - ETA: 3s - loss: -1.8486 - acc: 0.01 - ETA: 2s - loss: -266.8544 - acc: 0.01 - ETA: 2s - loss: -220.3903 - acc: 0.01 - ETA: 1s - loss: -183.7353 - acc: 0.01 - ETA: 1s - loss: -157.5559 - acc: 0.01 - ETA: 0s - loss: -138.2979 - acc: 0.01 - ETA: 0s - loss: -123.2689 - acc: 0.01 - 5s 5ms/step - loss: -117.6605 - acc: 0.0170\n",
      "Epoch 465/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -92.6511 - acc: 0.0000e+0 - ETA: 3s - loss: -46.5596 - acc: 0.0200    - ETA: 3s - loss: -31.3296 - acc: 0.030 - ETA: 2s - loss: -24.0485 - acc: 0.025 - ETA: 2s - loss: -19.8518 - acc: 0.022 - ETA: 1s - loss: -16.5093 - acc: 0.020 - ETA: 1s - loss: -229.5471 - acc: 0.01 - ETA: 0s - loss: -201.0166 - acc: 0.01 - ETA: 0s - loss: -179.6742 - acc: 0.01 - 5s 5ms/step - loss: -210.6483 - acc: 0.0170\n",
      "Epoch 466/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1063.5269 - acc: 0.0000e+0 - ETA: 3s - loss: -532.1345 - acc: 0.0150    - ETA: 3s - loss: -355.8910 - acc: 0.01 - ETA: 2s - loss: -267.0296 - acc: 0.02 - ETA: 2s - loss: -214.0867 - acc: 0.02 - ETA: 1s - loss: -222.8424 - acc: 0.01 - ETA: 1s - loss: -191.0699 - acc: 0.01 - ETA: 0s - loss: -167.2918 - acc: 0.02 - ETA: 0s - loss: -266.1970 - acc: 0.01 - 5s 5ms/step - loss: -293.3399 - acc: 0.0170\n",
      "Epoch 467/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.1182 - acc: 0.01 - ETA: 3s - loss: -1.6754 - acc: 0.02 - ETA: 3s - loss: -9.4640 - acc: 0.01 - ETA: 2s - loss: -7.4980 - acc: 0.01 - ETA: 2s - loss: -6.1034 - acc: 0.01 - ETA: 1s - loss: -5.1901 - acc: 0.01 - ETA: 1s - loss: -127.1212 - acc: 0.01 - ETA: 0s - loss: -111.5928 - acc: 0.01 - ETA: 0s - loss: -99.2164 - acc: 0.0167 - 5s 5ms/step - loss: -147.8408 - acc: 0.0159\n",
      "Epoch 468/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -3.4050 - acc: 0.02 - ETA: 3s - loss: -676.6786 - acc: 0.01 - ETA: 3s - loss: -451.3572 - acc: 0.02 - ETA: 2s - loss: -338.4694 - acc: 0.02 - ETA: 2s - loss: -271.7784 - acc: 0.02 - ETA: 1s - loss: -226.4959 - acc: 0.02 - ETA: 1s - loss: -194.8016 - acc: 0.02 - ETA: 0s - loss: -170.6685 - acc: 0.01 - ETA: 0s - loss: -302.0896 - acc: 0.01 - 5s 5ms/step - loss: -317.6747 - acc: 0.0159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.9010 - acc: 0.02 - ETA: 3s - loss: -0.9022 - acc: 0.03 - ETA: 3s - loss: -0.7326 - acc: 0.03 - ETA: 2s - loss: -300.4873 - acc: 0.02 - ETA: 2s - loss: -241.1461 - acc: 0.02 - ETA: 1s - loss: -201.4213 - acc: 0.01 - ETA: 1s - loss: -190.1635 - acc: 0.01 - ETA: 0s - loss: -166.6594 - acc: 0.01 - ETA: 0s - loss: -148.6102 - acc: 0.01 - 5s 5ms/step - loss: -141.9841 - acc: 0.0170\n",
      "Epoch 470/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.2270 - acc: 0.01 - ETA: 3s - loss: -561.5037 - acc: 0.00 - ETA: 3s - loss: -374.4122 - acc: 0.01 - ETA: 2s - loss: -281.1420 - acc: 0.01 - ETA: 2s - loss: -511.0331 - acc: 0.01 - ETA: 1s - loss: -426.2421 - acc: 0.01 - ETA: 1s - loss: -365.3938 - acc: 0.01 - ETA: 0s - loss: -320.5207 - acc: 0.01 - ETA: 0s - loss: -288.5030 - acc: 0.01 - 5s 5ms/step - loss: -275.4308 - acc: 0.0170\n",
      "Epoch 471/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -326.2732 - acc: 0.0000e+ - ETA: 3s - loss: -164.7803 - acc: 0.0050   - ETA: 3s - loss: -110.2865 - acc: 0.01 - ETA: 2s - loss: -314.0944 - acc: 0.01 - ETA: 2s - loss: -252.5507 - acc: 0.01 - ETA: 1s - loss: -210.8633 - acc: 0.01 - ETA: 1s - loss: -180.8625 - acc: 0.01 - ETA: 0s - loss: -158.2330 - acc: 0.01 - ETA: 0s - loss: -140.8711 - acc: 0.01 - 5s 5ms/step - loss: -134.4806 - acc: 0.0170\n",
      "Epoch 472/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1403.9879 - acc: 0.0000e+0 - ETA: 3s - loss: -1222.4237 - acc: 0.0000e+0 - ETA: 3s - loss: -816.1434 - acc: 0.0033    - ETA: 2s - loss: -612.5304 - acc: 0.01 - ETA: 2s - loss: -490.1875 - acc: 0.01 - ETA: 1s - loss: -408.6489 - acc: 0.01 - ETA: 1s - loss: -350.4401 - acc: 0.01 - ETA: 0s - loss: -426.6056 - acc: 0.01 - ETA: 0s - loss: -379.4250 - acc: 0.01 - 5s 5ms/step - loss: -362.1611 - acc: 0.0170\n",
      "Epoch 473/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.7076 - acc: 0.03 - ETA: 3s - loss: -0.9526 - acc: 0.03 - ETA: 3s - loss: -1.3540 - acc: 0.02 - ETA: 2s - loss: -1.9496 - acc: 0.02 - ETA: 2s - loss: -1.8037 - acc: 0.02 - ETA: 1s - loss: -1.8734 - acc: 0.02 - ETA: 1s - loss: -1.8380 - acc: 0.02 - ETA: 0s - loss: -185.1991 - acc: 0.01 - ETA: 0s - loss: -165.1437 - acc: 0.01 - 5s 5ms/step - loss: -157.6133 - acc: 0.0159\n",
      "Epoch 474/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7151 - acc: 0.04 - ETA: 3s - loss: -0.9136 - acc: 0.03 - ETA: 3s - loss: -1.3113 - acc: 0.03 - ETA: 2s - loss: -172.1878 - acc: 0.02 - ETA: 2s - loss: -402.8595 - acc: 0.01 - ETA: 1s - loss: -336.8710 - acc: 0.01 - ETA: 1s - loss: -288.9660 - acc: 0.01 - ETA: 0s - loss: -253.2198 - acc: 0.01 - ETA: 0s - loss: -225.2392 - acc: 0.01 - 5s 5ms/step - loss: -245.0982 - acc: 0.0159\n",
      "Epoch 475/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2662 - acc: 0.04 - ETA: 3s - loss: -1.2839 - acc: 0.03 - ETA: 3s - loss: -1.0450 - acc: 0.03 - ETA: 2s - loss: -1.9664 - acc: 0.02 - ETA: 2s - loss: -1.5427 - acc: 0.02 - ETA: 1s - loss: -1.4837 - acc: 0.02 - ETA: 1s - loss: -245.9735 - acc: 0.02 - ETA: 0s - loss: -231.8972 - acc: 0.01 - ETA: 0s - loss: -219.2645 - acc: 0.01 - 5s 5ms/step - loss: -239.4535 - acc: 0.0159\n",
      "Epoch 476/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.7562 - acc: 0.03 - ETA: 3s - loss: -1.9358 - acc: 0.02 - ETA: 3s - loss: -1.6053 - acc: 0.02 - ETA: 2s - loss: -1.9662 - acc: 0.02 - ETA: 2s - loss: -1.6108 - acc: 0.02 - ETA: 1s - loss: -133.1511 - acc: 0.01 - ETA: 1s - loss: -114.6249 - acc: 0.01 - ETA: 0s - loss: -277.1217 - acc: 0.01 - ETA: 0s - loss: -246.4358 - acc: 0.01 - 5s 5ms/step - loss: -257.6657 - acc: 0.0159\n",
      "Epoch 477/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.4218 - acc: 0.02 - ETA: 3s - loss: -1.2927 - acc: 0.02 - ETA: 3s - loss: -2.9706 - acc: 0.02 - ETA: 2s - loss: -2.2323 - acc: 0.02 - ETA: 2s - loss: -2.0352 - acc: 0.02 - ETA: 1s - loss: -2.0633 - acc: 0.01 - ETA: 1s - loss: -1.9595 - acc: 0.01 - ETA: 0s - loss: -1.8511 - acc: 0.01 - ETA: 0s - loss: -7.6770 - acc: 0.01 - 5s 5ms/step - loss: -77.8543 - acc: 0.0159\n",
      "Epoch 478/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.8654 - acc: 0.01 - ETA: 3s - loss: -2.8003 - acc: 0.01 - ETA: 3s - loss: -415.5454 - acc: 0.01 - ETA: 2s - loss: -311.7999 - acc: 0.01 - ETA: 2s - loss: -377.4946 - acc: 0.01 - ETA: 1s - loss: -314.9213 - acc: 0.01 - ETA: 1s - loss: -270.0882 - acc: 0.01 - ETA: 0s - loss: -405.2996 - acc: 0.01 - ETA: 0s - loss: -360.2368 - acc: 0.01 - 5s 5ms/step - loss: -343.8223 - acc: 0.0159\n",
      "Epoch 479/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.2495 - acc: 0.03 - ETA: 3s - loss: -3.5995 - acc: 0.02 - ETA: 3s - loss: -2.6824 - acc: 0.02 - ETA: 2s - loss: -2.2242 - acc: 0.02 - ETA: 2s - loss: -6.9058 - acc: 0.01 - ETA: 1s - loss: -5.9217 - acc: 0.02 - ETA: 1s - loss: -5.2508 - acc: 0.01 - ETA: 0s - loss: -122.4957 - acc: 0.01 - ETA: 0s - loss: -109.1994 - acc: 0.01 - 5s 5ms/step - loss: -118.9372 - acc: 0.0159\n",
      "Epoch 480/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -810.9259 - acc: 0.0000e+ - ETA: 3s - loss: -405.7054 - acc: 0.0150   - ETA: 3s - loss: -270.9789 - acc: 0.01 - ETA: 2s - loss: -203.2440 - acc: 0.02 - ETA: 2s - loss: -163.8636 - acc: 0.01 - ETA: 1s - loss: -301.4706 - acc: 0.01 - ETA: 1s - loss: -258.5913 - acc: 0.01 - ETA: 0s - loss: -226.4095 - acc: 0.01 - ETA: 0s - loss: -201.2992 - acc: 0.01 - 5s 5ms/step - loss: -262.6461 - acc: 0.0159\n",
      "Epoch 481/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.3862 - acc: 0.01 - ETA: 3s - loss: -1.5419 - acc: 0.02 - ETA: 3s - loss: -1.6308 - acc: 0.01 - ETA: 2s - loss: -1.6681 - acc: 0.01 - ETA: 2s - loss: -2.2521 - acc: 0.01 - ETA: 1s - loss: -2.0508 - acc: 0.01 - ETA: 1s - loss: -231.3181 - acc: 0.01 - ETA: 0s - loss: -202.4295 - acc: 0.01 - ETA: 0s - loss: -180.1655 - acc: 0.01 - 5s 5ms/step - loss: -171.9850 - acc: 0.0159\n",
      "Epoch 482/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.5144 - acc: 0.03 - ETA: 3s - loss: -0.5547 - acc: 0.03 - ETA: 3s - loss: -1.2396 - acc: 0.02 - ETA: 2s - loss: -330.7671 - acc: 0.01 - ETA: 2s - loss: -265.3349 - acc: 0.01 - ETA: 1s - loss: -221.0871 - acc: 0.01 - ETA: 1s - loss: -190.2863 - acc: 0.01 - ETA: 0s - loss: -166.7053 - acc: 0.01 - ETA: 0s - loss: -148.5015 - acc: 0.01 - 5s 5ms/step - loss: -210.5981 - acc: 0.0159\n",
      "Epoch 483/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1397.4376 - acc: 0.0000e+0 - ETA: 3s - loss: -698.7422 - acc: 0.0100    - ETA: 3s - loss: -466.4315 - acc: 0.01 - ETA: 2s - loss: -351.2928 - acc: 0.01 - ETA: 2s - loss: -281.4684 - acc: 0.01 - ETA: 1s - loss: -235.3676 - acc: 0.01 - ETA: 1s - loss: -201.9010 - acc: 0.01 - ETA: 0s - loss: -176.7494 - acc: 0.01 - ETA: 0s - loss: -157.4155 - acc: 0.01 - 5s 5ms/step - loss: -150.2164 - acc: 0.0159\n",
      "Epoch 484/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1.7658 - acc: 0.01 - ETA: 3s - loss: -299.5927 - acc: 0.00 - ETA: 3s - loss: -597.8569 - acc: 0.00 - ETA: 2s - loss: -454.2318 - acc: 0.00 - ETA: 2s - loss: -363.4362 - acc: 0.01 - ETA: 1s - loss: -303.1264 - acc: 0.01 - ETA: 1s - loss: -259.8471 - acc: 0.01 - ETA: 0s - loss: -227.6987 - acc: 0.01 - ETA: 0s - loss: -203.0285 - acc: 0.01 - 5s 5ms/step - loss: -193.7546 - acc: 0.0159\n",
      "Epoch 485/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.1816 - acc: 0.03 - ETA: 3s - loss: -321.6943 - acc: 0.01 - ETA: 3s - loss: -215.1659 - acc: 0.01 - ETA: 2s - loss: -466.9308 - acc: 0.01 - ETA: 2s - loss: -373.8999 - acc: 0.01 - ETA: 1s - loss: -311.6626 - acc: 0.02 - ETA: 1s - loss: -267.4104 - acc: 0.02 - ETA: 0s - loss: -367.0285 - acc: 0.01 - ETA: 0s - loss: -450.5172 - acc: 0.01 - 5s 5ms/step - loss: -493.4732 - acc: 0.0159\n",
      "Epoch 486/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -80.9225 - acc: 0.0000e+0 - ETA: 3s - loss: -40.6296 - acc: 0.0050    - ETA: 3s - loss: -543.4414 - acc: 0.00 - ETA: 2s - loss: -408.0454 - acc: 0.00 - ETA: 2s - loss: -326.4138 - acc: 0.01 - ETA: 1s - loss: -272.6985 - acc: 0.01 - ETA: 1s - loss: -234.4135 - acc: 0.01 - ETA: 0s - loss: -205.4537 - acc: 0.01 - ETA: 0s - loss: -182.6687 - acc: 0.01 - 5s 5ms/step - loss: -174.3720 - acc: 0.0159\n",
      "Epoch 487/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -135.6001 - acc: 0.0000e+ - ETA: 3s - loss: -68.5325 - acc: 0.0150     - ETA: 3s - loss: -46.5677 - acc: 0.013 - ETA: 2s - loss: -34.9769 - acc: 0.015 - ETA: 2s - loss: -28.1860 - acc: 0.016 - ETA: 1s - loss: -23.6564 - acc: 0.015 - ETA: 1s - loss: -20.7318 - acc: 0.014 - ETA: 0s - loss: -217.5151 - acc: 0.01 - ETA: 0s - loss: -193.3926 - acc: 0.01 - 5s 5ms/step - loss: -184.6384 - acc: 0.0159\n",
      "Epoch 488/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.0160 - acc: 0.02 - ETA: 3s - loss: -3.1868 - acc: 0.01 - ETA: 3s - loss: -8.7286 - acc: 0.01 - ETA: 2s - loss: -6.5006 - acc: 0.01 - ETA: 2s - loss: -5.4079 - acc: 0.01 - ETA: 1s - loss: -240.9734 - acc: 0.01 - ETA: 1s - loss: -207.0359 - acc: 0.01 - ETA: 0s - loss: -181.9049 - acc: 0.01 - ETA: 0s - loss: -161.7585 - acc: 0.01 - 5s 5ms/step - loss: -192.6929 - acc: 0.0159\n",
      "Epoch 489/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -4.1354 - acc: 0.01 - ETA: 3s - loss: -2.5467 - acc: 0.01 - ETA: 3s - loss: -2.1672 - acc: 0.01 - ETA: 2s - loss: -55.5880 - acc: 0.012 - ETA: 2s - loss: -44.8325 - acc: 0.012 - ETA: 1s - loss: -37.5743 - acc: 0.013 - ETA: 1s - loss: -32.4323 - acc: 0.014 - ETA: 0s - loss: -28.8378 - acc: 0.013 - ETA: 0s - loss: -25.6986 - acc: 0.015 - 5s 5ms/step - loss: -24.9041 - acc: 0.0159\n",
      "Epoch 490/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.7738 - acc: 0.01 - ETA: 3s - loss: -2.9324 - acc: 0.01 - ETA: 3s - loss: -3.1919 - acc: 0.01 - ETA: 2s - loss: -295.8237 - acc: 0.01 - ETA: 2s - loss: -236.7399 - acc: 0.01 - ETA: 1s - loss: -197.9293 - acc: 0.01 - ETA: 1s - loss: -169.9944 - acc: 0.01 - ETA: 0s - loss: -148.8181 - acc: 0.01 - ETA: 0s - loss: -132.4816 - acc: 0.01 - 5s 5ms/step - loss: -174.2294 - acc: 0.0159\n",
      "Epoch 491/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.8637 - acc: 0.04 - ETA: 3s - loss: -2.2453 - acc: 0.02 - ETA: 3s - loss: -2.5395 - acc: 0.02 - ETA: 2s - loss: -2.4758 - acc: 0.02 - ETA: 2s - loss: -2.5359 - acc: 0.01 - ETA: 1s - loss: -2.2687 - acc: 0.02 - ETA: 1s - loss: -204.5287 - acc: 0.01 - ETA: 0s - loss: -179.1793 - acc: 0.01 - ETA: 0s - loss: -159.4684 - acc: 0.01 - 5s 5ms/step - loss: -152.2094 - acc: 0.0159\n",
      "Epoch 492/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -0.3985 - acc: 0.03 - ETA: 3s - loss: -619.6140 - acc: 0.01 - ETA: 3s - loss: -413.4551 - acc: 0.01 - ETA: 2s - loss: -557.7643 - acc: 0.01 - ETA: 2s - loss: -446.3459 - acc: 0.01 - ETA: 1s - loss: -372.2076 - acc: 0.01 - ETA: 1s - loss: -319.4793 - acc: 0.01 - ETA: 0s - loss: -279.6229 - acc: 0.01 - ETA: 0s - loss: -249.0816 - acc: 0.01 - 5s 5ms/step - loss: -237.8291 - acc: 0.0159\n",
      "Epoch 493/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.3490 - acc: 0.01 - ETA: 3s - loss: -5.0032 - acc: 0.01 - ETA: 3s - loss: -305.5352 - acc: 0.00 - ETA: 2s - loss: -229.1695 - acc: 0.01 - ETA: 2s - loss: -183.7200 - acc: 0.01 - ETA: 1s - loss: -153.3027 - acc: 0.01 - ETA: 1s - loss: -131.6009 - acc: 0.01 - ETA: 0s - loss: -115.5951 - acc: 0.01 - ETA: 0s - loss: -102.9382 - acc: 0.01 - 5s 5ms/step - loss: -98.3160 - acc: 0.0159\n",
      "Epoch 494/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.2165 - acc: 0.01 - ETA: 3s - loss: -1.3830 - acc: 0.01 - ETA: 3s - loss: -1.1728 - acc: 0.02 - ETA: 2s - loss: -1.3695 - acc: 0.02 - ETA: 2s - loss: -149.0355 - acc: 0.01 - ETA: 1s - loss: -124.6570 - acc: 0.01 - ETA: 1s - loss: -106.9922 - acc: 0.01 - ETA: 0s - loss: -96.9144 - acc: 0.0150 - ETA: 0s - loss: -86.2406 - acc: 0.015 - 5s 5ms/step - loss: -82.2750 - acc: 0.0159\n",
      "Epoch 495/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1316.2611 - acc: 0.0000e+0 - ETA: 3s - loss: -660.8028 - acc: 0.0050    - ETA: 3s - loss: -440.7637 - acc: 0.01 - ETA: 2s - loss: -330.7223 - acc: 0.01 - ETA: 2s - loss: -265.0204 - acc: 0.01 - ETA: 1s - loss: -220.9021 - acc: 0.01 - ETA: 1s - loss: -190.2454 - acc: 0.01 - ETA: 0s - loss: -166.4953 - acc: 0.01 - ETA: 0s - loss: -278.1572 - acc: 0.01 - 5s 5ms/step - loss: -265.4853 - acc: 0.0159\n",
      "Epoch 496/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -1057.9370 - acc: 0.0000e+0 - ETA: 3s - loss: -530.5360 - acc: 0.0050    - ETA: 3s - loss: -354.4226 - acc: 0.00 - ETA: 2s - loss: -266.1714 - acc: 0.01 - ETA: 2s - loss: -213.5616 - acc: 0.01 - ETA: 1s - loss: -178.2101 - acc: 0.01 - ETA: 1s - loss: -224.8902 - acc: 0.01 - ETA: 0s - loss: -196.9552 - acc: 0.01 - ETA: 0s - loss: -175.1321 - acc: 0.01 - 5s 5ms/step - loss: -167.1249 - acc: 0.0159\n",
      "Epoch 497/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.2411 - acc: 0.01 - ETA: 3s - loss: -2.6389 - acc: 0.01 - ETA: 3s - loss: -2.0739 - acc: 0.02 - ETA: 2s - loss: -1.6395 - acc: 0.01 - ETA: 2s - loss: -2.3495 - acc: 0.01 - ETA: 1s - loss: -2.0991 - acc: 0.01 - ETA: 1s - loss: -2.2174 - acc: 0.01 - ETA: 0s - loss: -2.2267 - acc: 0.01 - ETA: 0s - loss: -2.0168 - acc: 0.01 - 5s 5ms/step - loss: -57.8684 - acc: 0.0159\n",
      "Epoch 498/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -5.7337 - acc: 0.01 - ETA: 3s - loss: -3.5392 - acc: 0.01 - ETA: 3s - loss: -2.6989 - acc: 0.01 - ETA: 2s - loss: -2.2730 - acc: 0.01 - ETA: 2s - loss: -215.8153 - acc: 0.01 - ETA: 1s - loss: -180.5346 - acc: 0.01 - ETA: 1s - loss: -155.1307 - acc: 0.01 - ETA: 0s - loss: -136.0434 - acc: 0.01 - ETA: 0s - loss: -120.9391 - acc: 0.01 - 5s 5ms/step - loss: -115.7479 - acc: 0.0159\n",
      "Epoch 499/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.2031 - acc: 0.01 - ETA: 3s - loss: -649.6937 - acc: 0.00 - ETA: 3s - loss: -433.1576 - acc: 0.01 - ETA: 2s - loss: -325.6064 - acc: 0.01 - ETA: 2s - loss: -260.4288 - acc: 0.01 - ETA: 1s - loss: -415.8349 - acc: 0.01 - ETA: 1s - loss: -356.5181 - acc: 0.01 - ETA: 0s - loss: -312.2626 - acc: 0.01 - ETA: 0s - loss: -277.7808 - acc: 0.01 - 5s 5ms/step - loss: -353.8065 - acc: 0.0159\n",
      "Epoch 500/500\n",
      "943/943 [==============================] - ETA: 4s - loss: -2.1779 - acc: 0.03 - ETA: 3s - loss: -642.8977 - acc: 0.01 - ETA: 3s - loss: -430.7926 - acc: 0.01 - ETA: 2s - loss: -323.6188 - acc: 0.01 - ETA: 2s - loss: -259.2545 - acc: 0.01 - ETA: 1s - loss: -216.3917 - acc: 0.01 - ETA: 1s - loss: -185.6747 - acc: 0.01 - ETA: 0s - loss: -162.7414 - acc: 0.01 - ETA: 0s - loss: -144.7112 - acc: 0.01 - 5s 6ms/step - loss: -138.0872 - acc: 0.0159\n",
      "  64/1416 [>.............................] - ETA:  - ETA: 8s"
     ]
    },
    {
     "ename": "InvalidValueError",
     "evalue": "InvalidValueError\n        type(variable) = TensorType(float32, matrix)\n        variable       = Elemwise{true_div,no_inplace}.0\n        type(value)    = <class 'numpy.ndarray'>\n        dtype(value)   = float32\n        shape(value)   = (1, 1)\n        value          = [[nan]]\n        min(value)     = nan\n        max(value)     = nan\n        isfinite       = False\n        client_node    = None\n        hint           = perform output\n        specific_hint  = non-finite elements not allowed\n        context        = ...\n  Elemwise{true_div,no_inplace} [id A] ''   \n   |Dot22 [id B] ''   \n   | |InplaceDimShuffle{1,0} [id C] ''   \n   | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n   | |   |AdvancedSubtensor1 [id E] ''   \n   | |   | |<TensorType(float32, matrix)> [id F]\n   | |   | |Subtensor{int64} [id G] ''   \n   | |   |   |Nonzero [id H] ''   \n   | |   |   | |<TensorType(bool, vector)> [id I]\n   | |   |   |Constant{0} [id J]\n   | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n   | |     |InplaceDimShuffle{x,0} [id L] ''   \n   | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n   | |     |   |AdvancedSubtensor1 [id E] ''   \n   | |     |Elemwise{Cast{float32}} [id N] ''   \n   | |       |InplaceDimShuffle{x,x} [id O] ''   \n   | |         |Shape_i{1} [id P] ''   \n   | |           |Nonzero [id H] ''   \n   | |Elemwise{Sub}[(0, 0)] [id D] ''   \n   |Elemwise{Add}[(0, 1)] [id Q] ''   \n     |TensorConstant{(1, 1) of -1.0} [id R]\n     |Elemwise{Cast{float32}} [id N] ''   \n\n        \nApply node that caused the error: for{cpu,scan_fn}(Shape_i{0}.0, Elemwise{eq,no_inplace}.0, Shape_i{0}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0)\nToposort index: 35\nInputs types: [TensorType(int64, scalar), TensorType(bool, matrix), TensorType(int64, scalar), TensorType(float32, matrix)]\nInputs shapes: [(), (2, 32), (), (32, 1)]\nInputs strides: [(), (32, 1), (), (4, 4)]\nInputs values: [array(2, dtype=int64), 'not shown', array(2, dtype=int64), 'not shown']\nOutputs clients: [[Sum{axis=[0], acc_dtype=float64}(for{cpu,scan_fn}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\debugmode.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2127\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2128\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2129\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\debugmode.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1867\u001b[0m                                                         \u001b[0mhint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'perform output'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1868\u001b[1;33m                                                         specific_hint=hint2)\n\u001b[0m\u001b[0;32m   1869\u001b[0m                         \u001b[0mwarn_inp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDebugMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn_input_not_reused\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidValueError\u001b[0m: InvalidValueError\n        type(variable) = TensorType(float32, matrix)\n        variable       = Elemwise{true_div,no_inplace}.0\n        type(value)    = <class 'numpy.ndarray'>\n        dtype(value)   = float32\n        shape(value)   = (1, 1)\n        value          = [[nan]]\n        min(value)     = nan\n        max(value)     = nan\n        isfinite       = False\n        client_node    = None\n        hint           = perform output\n        specific_hint  = non-finite elements not allowed\n        context        = ...\n  Elemwise{true_div,no_inplace} [id A] ''   \n   |Dot22 [id B] ''   \n   | |InplaceDimShuffle{1,0} [id C] ''   \n   | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n   | |   |AdvancedSubtensor1 [id E] ''   \n   | |   | |<TensorType(float32, matrix)> [id F]\n   | |   | |Subtensor{int64} [id G] ''   \n   | |   |   |Nonzero [id H] ''   \n   | |   |   | |<TensorType(bool, vector)> [id I]\n   | |   |   |Constant{0} [id J]\n   | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n   | |     |InplaceDimShuffle{x,0} [id L] ''   \n   | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n   | |     |   |AdvancedSubtensor1 [id E] ''   \n   | |     |Elemwise{Cast{float32}} [id N] ''   \n   | |       |InplaceDimShuffle{x,x} [id O] ''   \n   | |         |Shape_i{1} [id P] ''   \n   | |           |Nonzero [id H] ''   \n   | |Elemwise{Sub}[(0, 0)] [id D] ''   \n   |Elemwise{Add}[(0, 1)] [id Q] ''   \n     |TensorConstant{(1, 1) of -1.0} [id R]\n     |Elemwise{Cast{float32}} [id N] ''   \n\n        ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-adec3f214c98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mautomation_script\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_imly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'linear_discrimant_analysis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\automation_script.py\u001b[0m in \u001b[0;36mrun_imly\u001b[1;34m(dataset_info, model_name, X, Y, test_size, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     \u001b[0mkeras_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# Create plot and write to s3 bucket #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1111\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m                                          steps=steps)\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m     def predict(self, x,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    918\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\debugmode.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2126\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_isfinite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2127\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2128\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2129\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2130\u001b[0m                     \u001b[1;31m# put back the filter_checks_isfinite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\debugmode.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1866\u001b[0m                                 raise InvalidValueError(r, storage_map[r][0],\n\u001b[0;32m   1867\u001b[0m                                                         \u001b[0mhint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'perform output'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1868\u001b[1;33m                                                         specific_hint=hint2)\n\u001b[0m\u001b[0;32m   1869\u001b[0m                         \u001b[0mwarn_inp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDebugMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn_input_not_reused\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1870\u001b[0m                         py_inplace_outs = _check_inputs(\n",
      "\u001b[1;31mInvalidValueError\u001b[0m: InvalidValueError\n        type(variable) = TensorType(float32, matrix)\n        variable       = Elemwise{true_div,no_inplace}.0\n        type(value)    = <class 'numpy.ndarray'>\n        dtype(value)   = float32\n        shape(value)   = (1, 1)\n        value          = [[nan]]\n        min(value)     = nan\n        max(value)     = nan\n        isfinite       = False\n        client_node    = None\n        hint           = perform output\n        specific_hint  = non-finite elements not allowed\n        context        = ...\n  Elemwise{true_div,no_inplace} [id A] ''   \n   |Dot22 [id B] ''   \n   | |InplaceDimShuffle{1,0} [id C] ''   \n   | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n   | |   |AdvancedSubtensor1 [id E] ''   \n   | |   | |<TensorType(float32, matrix)> [id F]\n   | |   | |Subtensor{int64} [id G] ''   \n   | |   |   |Nonzero [id H] ''   \n   | |   |   | |<TensorType(bool, vector)> [id I]\n   | |   |   |Constant{0} [id J]\n   | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n   | |     |InplaceDimShuffle{x,0} [id L] ''   \n   | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n   | |     |   |AdvancedSubtensor1 [id E] ''   \n   | |     |Elemwise{Cast{float32}} [id N] ''   \n   | |       |InplaceDimShuffle{x,x} [id O] ''   \n   | |         |Shape_i{1} [id P] ''   \n   | |           |Nonzero [id H] ''   \n   | |Elemwise{Sub}[(0, 0)] [id D] ''   \n   |Elemwise{Add}[(0, 1)] [id Q] ''   \n     |TensorConstant{(1, 1) of -1.0} [id R]\n     |Elemwise{Cast{float32}} [id N] ''   \n\n        \nApply node that caused the error: for{cpu,scan_fn}(Shape_i{0}.0, Elemwise{eq,no_inplace}.0, Shape_i{0}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0)\nToposort index: 35\nInputs types: [TensorType(int64, scalar), TensorType(bool, matrix), TensorType(int64, scalar), TensorType(float32, matrix)]\nInputs shapes: [(), (2, 32), (), (32, 1)]\nInputs strides: [(), (32, 1), (), (4, 4)]\nInputs values: [array(2, dtype=int64), 'not shown', array(2, dtype=int64), 'not shown']\nOutputs clients: [[Sum{axis=[0], acc_dtype=float64}(for{cpu,scan_fn}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_name = \"uci_ad_lda\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/ad.data.csv\"\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "# Check for columns that contain missing values #\n",
    "\n",
    "data = data.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Label encoding #\n",
    "\n",
    "lb = LabelEncoder()\n",
    "Y = lb.fit_transform(data.iloc[:, -1])\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "\n",
    "# Normalize the X values #\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "automation_script.run_imly(dataset_info, 'linear_discrimant_analysis', X, Y, 0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test bed ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hyperas(Logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils.validation import column_or_1d\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import copy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import datasets\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from automation_script import get_dataset_info\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.datasets import make_regression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano.tensor as T\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from theano.compile.ops import as_op\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [10, 30]),\n",
      "        'epochs': hp.choice('epochs', [100, 170]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: Make sure to have every relevant import statement included here and return data as\n",
      "  6: used in model function below. This function is separated from model() so that hyperopt\n",
      "  7: won't reload data for each evaluation run.\n",
      "  8: '''\n",
      "  9: url = \"../data/iris.csv\"\n",
      " 10: data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
      " 11: class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
      " 12: data.iloc[:,-1] = index\n",
      " 13: data = data.loc[data[4] != 2]\n",
      " 14: X = data.iloc[:,:-1]\n",
      " 15: Y = data.iloc[:,-1]\n",
      " 16: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
      " 17: \n",
      " 18: \n",
      " 19: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13: \n",
      "  14:     model = Sequential()\n",
      "  15:     model.add(Dense(1, input_dim=4, activation='sigmoid'))\n",
      "  16: \n",
      "  17:     model.compile(loss='binary_crossentropy', optimizer=space['optimizer'],\n",
      "  18:                  metrics=['accuracy'])\n",
      "  19: \n",
      "  20:     model.fit(x_train, y_train,\n",
      "  21:               batch_size=space['batch_size'],\n",
      "  22:               epochs=space['epochs'],\n",
      "  23:               verbose=2,\n",
      "  24:               validation_data=(x_test, y_test))\n",
      "  25:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  26:     print('Test accuracy:', acc)\n",
      "  27:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  28: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 0.8220 - acc: 0.5500 - val_loss: 0.8823 - val_acc: 0.4667\n",
      "Epoch 2/170\n",
      " - 0s - loss: 0.8120 - acc: 0.5500 - val_loss: 0.8704 - val_acc: 0.4667\n",
      "Epoch 3/170\n",
      " - 0s - loss: 0.8036 - acc: 0.5500 - val_loss: 0.8589 - val_acc: 0.4667\n",
      "Epoch 4/170\n",
      " - 0s - loss: 0.7932 - acc: 0.5500 - val_loss: 0.8484 - val_acc: 0.4667\n",
      "Epoch 5/170\n",
      " - 0s - loss: 0.7865 - acc: 0.5500 - val_loss: 0.8375 - val_acc: 0.4667\n",
      "Epoch 6/170\n",
      " - 0s - loss: 0.7778 - acc: 0.5500 - val_loss: 0.8276 - val_acc: 0.4667\n",
      "Epoch 7/170\n",
      " - 0s - loss: 0.7692 - acc: 0.5500 - val_loss: 0.8183 - val_acc: 0.4667\n",
      "Epoch 8/170\n",
      " - 0s - loss: 0.7628 - acc: 0.5500 - val_loss: 0.8088 - val_acc: 0.4667\n",
      "Epoch 9/170\n",
      " - 0s - loss: 0.7558 - acc: 0.5500 - val_loss: 0.7994 - val_acc: 0.4667\n",
      "Epoch 10/170\n",
      " - 0s - loss: 0.7486 - acc: 0.5500 - val_loss: 0.7904 - val_acc: 0.4667\n",
      "Epoch 11/170\n",
      " - 0s - loss: 0.7421 - acc: 0.5500 - val_loss: 0.7815 - val_acc: 0.4667\n",
      "Epoch 12/170\n",
      " - 0s - loss: 0.7358 - acc: 0.5500 - val_loss: 0.7729 - val_acc: 0.4667\n",
      "Epoch 13/170\n",
      " - 0s - loss: 0.7287 - acc: 0.5500 - val_loss: 0.7648 - val_acc: 0.4667\n",
      "Epoch 14/170\n",
      " - 0s - loss: 0.7218 - acc: 0.5500 - val_loss: 0.7575 - val_acc: 0.4667\n",
      "Epoch 15/170\n",
      " - 0s - loss: 0.7161 - acc: 0.5500 - val_loss: 0.7503 - val_acc: 0.4667\n",
      "Epoch 16/170\n",
      " - 0s - loss: 0.7110 - acc: 0.5500 - val_loss: 0.7427 - val_acc: 0.4667\n",
      "Epoch 17/170\n",
      " - 0s - loss: 0.7061 - acc: 0.5500 - val_loss: 0.7350 - val_acc: 0.4667\n",
      "Epoch 18/170\n",
      " - 0s - loss: 0.6991 - acc: 0.5500 - val_loss: 0.7284 - val_acc: 0.4667\n",
      "Epoch 19/170\n",
      " - 0s - loss: 0.6941 - acc: 0.5500 - val_loss: 0.7217 - val_acc: 0.4667\n",
      "Epoch 20/170\n",
      " - 0s - loss: 0.6882 - acc: 0.5500 - val_loss: 0.7157 - val_acc: 0.4667\n",
      "Epoch 21/170\n",
      " - 0s - loss: 0.6837 - acc: 0.5500 - val_loss: 0.7094 - val_acc: 0.4667\n",
      "Epoch 22/170\n",
      " - 0s - loss: 0.6786 - acc: 0.5500 - val_loss: 0.7031 - val_acc: 0.4667\n",
      "Epoch 23/170\n",
      " - 0s - loss: 0.6736 - acc: 0.5500 - val_loss: 0.6971 - val_acc: 0.4667\n",
      "Epoch 24/170\n",
      " - 0s - loss: 0.6687 - acc: 0.5500 - val_loss: 0.6913 - val_acc: 0.4667\n",
      "Epoch 25/170\n",
      " - 0s - loss: 0.6638 - acc: 0.5500 - val_loss: 0.6857 - val_acc: 0.4667\n",
      "Epoch 26/170\n",
      " - 0s - loss: 0.6591 - acc: 0.5500 - val_loss: 0.6802 - val_acc: 0.4667\n",
      "Epoch 27/170\n",
      " - 0s - loss: 0.6545 - acc: 0.5500 - val_loss: 0.6748 - val_acc: 0.4667\n",
      "Epoch 28/170\n",
      " - 0s - loss: 0.6508 - acc: 0.5500 - val_loss: 0.6690 - val_acc: 0.4667\n",
      "Epoch 29/170\n",
      " - 0s - loss: 0.6462 - acc: 0.5500 - val_loss: 0.6633 - val_acc: 0.4667\n",
      "Epoch 30/170\n",
      " - 0s - loss: 0.6415 - acc: 0.5500 - val_loss: 0.6581 - val_acc: 0.4667\n",
      "Epoch 31/170\n",
      " - 0s - loss: 0.6367 - acc: 0.5500 - val_loss: 0.6533 - val_acc: 0.4667\n",
      "Epoch 32/170\n",
      " - 0s - loss: 0.6325 - acc: 0.5500 - val_loss: 0.6484 - val_acc: 0.4667\n",
      "Epoch 33/170\n",
      " - 0s - loss: 0.6286 - acc: 0.5500 - val_loss: 0.6434 - val_acc: 0.4667\n",
      "Epoch 34/170\n",
      " - 0s - loss: 0.6244 - acc: 0.5500 - val_loss: 0.6385 - val_acc: 0.4667\n",
      "Epoch 35/170\n",
      " - 0s - loss: 0.6199 - acc: 0.5750 - val_loss: 0.6342 - val_acc: 0.4667\n",
      "Epoch 36/170\n",
      " - 0s - loss: 0.6162 - acc: 0.5750 - val_loss: 0.6295 - val_acc: 0.4667\n",
      "Epoch 37/170\n",
      " - 0s - loss: 0.6121 - acc: 0.5750 - val_loss: 0.6248 - val_acc: 0.4667\n",
      "Epoch 38/170\n",
      " - 0s - loss: 0.6080 - acc: 0.5750 - val_loss: 0.6205 - val_acc: 0.4667\n",
      "Epoch 39/170\n",
      " - 0s - loss: 0.6042 - acc: 0.5750 - val_loss: 0.6161 - val_acc: 0.4833\n",
      "Epoch 40/170\n",
      " - 0s - loss: 0.6004 - acc: 0.6000 - val_loss: 0.6116 - val_acc: 0.5000\n",
      "Epoch 41/170\n",
      " - 0s - loss: 0.5967 - acc: 0.6000 - val_loss: 0.6071 - val_acc: 0.5000\n",
      "Epoch 42/170\n",
      " - 0s - loss: 0.5926 - acc: 0.6000 - val_loss: 0.6029 - val_acc: 0.5000\n",
      "Epoch 43/170\n",
      " - 0s - loss: 0.5895 - acc: 0.6000 - val_loss: 0.5983 - val_acc: 0.5167\n",
      "Epoch 44/170\n",
      " - 0s - loss: 0.5852 - acc: 0.6250 - val_loss: 0.5942 - val_acc: 0.5167\n",
      "Epoch 45/170\n",
      " - 0s - loss: 0.5819 - acc: 0.6250 - val_loss: 0.5899 - val_acc: 0.5333\n",
      "Epoch 46/170\n",
      " - 0s - loss: 0.5778 - acc: 0.6500 - val_loss: 0.5861 - val_acc: 0.5333\n",
      "Epoch 47/170\n",
      " - 0s - loss: 0.5741 - acc: 0.6500 - val_loss: 0.5825 - val_acc: 0.5333\n",
      "Epoch 48/170\n",
      " - 0s - loss: 0.5710 - acc: 0.6500 - val_loss: 0.5784 - val_acc: 0.5333\n",
      "Epoch 49/170\n",
      " - 0s - loss: 0.5672 - acc: 0.6500 - val_loss: 0.5748 - val_acc: 0.5333\n",
      "Epoch 50/170\n",
      " - 0s - loss: 0.5636 - acc: 0.6500 - val_loss: 0.5713 - val_acc: 0.5667\n",
      "Epoch 51/170\n",
      " - 0s - loss: 0.5603 - acc: 0.6500 - val_loss: 0.5676 - val_acc: 0.5667\n",
      "Epoch 52/170\n",
      " - 0s - loss: 0.5569 - acc: 0.6500 - val_loss: 0.5639 - val_acc: 0.5667\n",
      "Epoch 53/170\n",
      " - 0s - loss: 0.5538 - acc: 0.6500 - val_loss: 0.5601 - val_acc: 0.5833\n",
      "Epoch 54/170\n",
      " - 0s - loss: 0.5503 - acc: 0.6750 - val_loss: 0.5564 - val_acc: 0.5833\n",
      "Epoch 55/170\n",
      " - 0s - loss: 0.5468 - acc: 0.6750 - val_loss: 0.5531 - val_acc: 0.6167\n",
      "Epoch 56/170\n",
      " - 0s - loss: 0.5438 - acc: 0.6750 - val_loss: 0.5494 - val_acc: 0.6167\n",
      "Epoch 57/170\n",
      " - 0s - loss: 0.5404 - acc: 0.6750 - val_loss: 0.5460 - val_acc: 0.6167\n",
      "Epoch 58/170\n",
      " - 0s - loss: 0.5371 - acc: 0.6750 - val_loss: 0.5427 - val_acc: 0.6167\n",
      "Epoch 59/170\n",
      " - 0s - loss: 0.5341 - acc: 0.6750 - val_loss: 0.5391 - val_acc: 0.6333\n",
      "Epoch 60/170\n",
      " - 0s - loss: 0.5308 - acc: 0.6750 - val_loss: 0.5357 - val_acc: 0.6333\n",
      "Epoch 61/170\n",
      " - 0s - loss: 0.5277 - acc: 0.6750 - val_loss: 0.5325 - val_acc: 0.6333\n",
      "Epoch 62/170\n",
      " - 0s - loss: 0.5246 - acc: 0.7000 - val_loss: 0.5293 - val_acc: 0.6333\n",
      "Epoch 63/170\n",
      " - 0s - loss: 0.5216 - acc: 0.7000 - val_loss: 0.5259 - val_acc: 0.6500\n",
      "Epoch 64/170\n",
      " - 0s - loss: 0.5184 - acc: 0.7000 - val_loss: 0.5229 - val_acc: 0.6667\n",
      "Epoch 65/170\n",
      " - 0s - loss: 0.5154 - acc: 0.7000 - val_loss: 0.5196 - val_acc: 0.7000\n",
      "Epoch 66/170\n",
      " - 0s - loss: 0.5123 - acc: 0.7000 - val_loss: 0.5164 - val_acc: 0.7000\n",
      "Epoch 67/170\n",
      " - 0s - loss: 0.5094 - acc: 0.7250 - val_loss: 0.5132 - val_acc: 0.7000\n",
      "Epoch 68/170\n",
      " - 0s - loss: 0.5066 - acc: 0.7250 - val_loss: 0.5100 - val_acc: 0.7167\n",
      "Epoch 69/170\n",
      " - 0s - loss: 0.5035 - acc: 0.7250 - val_loss: 0.5068 - val_acc: 0.7167\n",
      "Epoch 70/170\n",
      " - 0s - loss: 0.5006 - acc: 0.7250 - val_loss: 0.5039 - val_acc: 0.7333\n",
      "Epoch 71/170\n",
      " - 0s - loss: 0.4979 - acc: 0.7500 - val_loss: 0.5006 - val_acc: 0.8000\n",
      "Epoch 72/170\n",
      " - 0s - loss: 0.4949 - acc: 0.7500 - val_loss: 0.4977 - val_acc: 0.8167\n",
      "Epoch 73/170\n",
      " - 0s - loss: 0.4921 - acc: 0.7500 - val_loss: 0.4946 - val_acc: 0.8167\n",
      "Epoch 74/170\n",
      " - 0s - loss: 0.4892 - acc: 0.7750 - val_loss: 0.4918 - val_acc: 0.8167\n",
      "Epoch 75/170\n",
      " - 0s - loss: 0.4865 - acc: 0.7750 - val_loss: 0.4888 - val_acc: 0.8333\n",
      "Epoch 76/170\n",
      " - 0s - loss: 0.4837 - acc: 0.7750 - val_loss: 0.4859 - val_acc: 0.8333\n",
      "Epoch 77/170\n",
      " - 0s - loss: 0.4813 - acc: 0.7750 - val_loss: 0.4828 - val_acc: 0.8500\n",
      "Epoch 78/170\n",
      " - 0s - loss: 0.4783 - acc: 0.8000 - val_loss: 0.4800 - val_acc: 0.8500\n",
      "Epoch 79/170\n",
      " - 0s - loss: 0.4755 - acc: 0.8000 - val_loss: 0.4775 - val_acc: 0.8500\n",
      "Epoch 80/170\n",
      " - 0s - loss: 0.4729 - acc: 0.8000 - val_loss: 0.4748 - val_acc: 0.8500\n",
      "Epoch 81/170\n",
      " - 0s - loss: 0.4703 - acc: 0.8000 - val_loss: 0.4720 - val_acc: 0.8667\n",
      "Epoch 82/170\n",
      " - 0s - loss: 0.4676 - acc: 0.8250 - val_loss: 0.4694 - val_acc: 0.8667\n",
      "Epoch 83/170\n",
      " - 0s - loss: 0.4652 - acc: 0.8250 - val_loss: 0.4666 - val_acc: 0.8667\n",
      "Epoch 84/170\n",
      " - 0s - loss: 0.4625 - acc: 0.8250 - val_loss: 0.4643 - val_acc: 0.8667\n",
      "Epoch 85/170\n",
      " - 0s - loss: 0.4599 - acc: 0.8250 - val_loss: 0.4618 - val_acc: 0.8667\n",
      "Epoch 86/170\n",
      " - 0s - loss: 0.4574 - acc: 0.8250 - val_loss: 0.4590 - val_acc: 0.8667\n",
      "Epoch 87/170\n",
      " - 0s - loss: 0.4550 - acc: 0.8250 - val_loss: 0.4563 - val_acc: 0.9000\n",
      "Epoch 88/170\n",
      " - 0s - loss: 0.4524 - acc: 0.8250 - val_loss: 0.4537 - val_acc: 0.9000\n",
      "Epoch 89/170\n",
      " - 0s - loss: 0.4499 - acc: 0.8250 - val_loss: 0.4512 - val_acc: 0.9000\n",
      "Epoch 90/170\n",
      " - 0s - loss: 0.4475 - acc: 0.8250 - val_loss: 0.4488 - val_acc: 0.9000\n",
      "Epoch 91/170\n",
      " - 0s - loss: 0.4453 - acc: 0.8250 - val_loss: 0.4460 - val_acc: 0.9000\n",
      "Epoch 92/170\n",
      " - 0s - loss: 0.4428 - acc: 0.8250 - val_loss: 0.4434 - val_acc: 0.9000\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.4402 - acc: 0.8250 - val_loss: 0.4411 - val_acc: 0.9000\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.4379 - acc: 0.8500 - val_loss: 0.4386 - val_acc: 0.9333\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.4356 - acc: 0.8750 - val_loss: 0.4362 - val_acc: 0.9500\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.4332 - acc: 0.8750 - val_loss: 0.4340 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/170\n",
      " - 0s - loss: 0.4310 - acc: 0.8750 - val_loss: 0.4319 - val_acc: 0.9500\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.4288 - acc: 0.8750 - val_loss: 0.4295 - val_acc: 0.9500\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.4265 - acc: 0.9000 - val_loss: 0.4271 - val_acc: 0.9500\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.4243 - acc: 0.9250 - val_loss: 0.4247 - val_acc: 0.9500\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.4219 - acc: 0.9500 - val_loss: 0.4225 - val_acc: 0.9500\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.4198 - acc: 0.9500 - val_loss: 0.4202 - val_acc: 0.9500\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.4176 - acc: 0.9500 - val_loss: 0.4179 - val_acc: 0.9500\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.4154 - acc: 0.9500 - val_loss: 0.4157 - val_acc: 0.9500\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.4132 - acc: 0.9500 - val_loss: 0.4136 - val_acc: 0.9667\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.4111 - acc: 0.9500 - val_loss: 0.4115 - val_acc: 0.9667\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.4089 - acc: 0.9500 - val_loss: 0.4094 - val_acc: 0.9667\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.4069 - acc: 0.9500 - val_loss: 0.4074 - val_acc: 0.9667\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.4048 - acc: 0.9750 - val_loss: 0.4054 - val_acc: 0.9667\n",
      "Epoch 110/170\n",
      " - 0s - loss: 0.4027 - acc: 0.9750 - val_loss: 0.4032 - val_acc: 0.9667\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.4006 - acc: 0.9750 - val_loss: 0.4012 - val_acc: 0.9667\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.3986 - acc: 0.9750 - val_loss: 0.3991 - val_acc: 0.9833\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.3965 - acc: 0.9750 - val_loss: 0.3969 - val_acc: 0.9833\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.3945 - acc: 0.9750 - val_loss: 0.3949 - val_acc: 0.9833\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.3925 - acc: 0.9750 - val_loss: 0.3928 - val_acc: 0.9833\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.3905 - acc: 0.9750 - val_loss: 0.3907 - val_acc: 0.9833\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.3886 - acc: 0.9750 - val_loss: 0.3885 - val_acc: 0.9833\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.3865 - acc: 0.9750 - val_loss: 0.3865 - val_acc: 0.9833\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.3846 - acc: 0.9750 - val_loss: 0.3845 - val_acc: 0.9833\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.3826 - acc: 0.9750 - val_loss: 0.3827 - val_acc: 0.9833\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.3807 - acc: 0.9750 - val_loss: 0.3807 - val_acc: 0.9833\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.3788 - acc: 0.9750 - val_loss: 0.3787 - val_acc: 0.9833\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.3769 - acc: 1.0000 - val_loss: 0.3768 - val_acc: 0.9833\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.3752 - acc: 1.0000 - val_loss: 0.3747 - val_acc: 0.9833\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.3731 - acc: 1.0000 - val_loss: 0.3729 - val_acc: 0.9833\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.3713 - acc: 1.0000 - val_loss: 0.3709 - val_acc: 0.9833\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.3694 - acc: 1.0000 - val_loss: 0.3691 - val_acc: 0.9833\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.3676 - acc: 1.0000 - val_loss: 0.3672 - val_acc: 0.9833\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.3658 - acc: 1.0000 - val_loss: 0.3653 - val_acc: 0.9833\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.3639 - acc: 1.0000 - val_loss: 0.3637 - val_acc: 0.9833\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.3623 - acc: 1.0000 - val_loss: 0.3617 - val_acc: 0.9833\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.3604 - acc: 1.0000 - val_loss: 0.3602 - val_acc: 0.9833\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.3586 - acc: 1.0000 - val_loss: 0.3583 - val_acc: 0.9833\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.3570 - acc: 1.0000 - val_loss: 0.3563 - val_acc: 0.9833\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.3551 - acc: 1.0000 - val_loss: 0.3546 - val_acc: 0.9833\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.3533 - acc: 1.0000 - val_loss: 0.3529 - val_acc: 0.9833\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.3516 - acc: 1.0000 - val_loss: 0.3513 - val_acc: 0.9833\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.3499 - acc: 1.0000 - val_loss: 0.3497 - val_acc: 0.9833\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.3483 - acc: 1.0000 - val_loss: 0.3479 - val_acc: 0.9833\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.3465 - acc: 1.0000 - val_loss: 0.3461 - val_acc: 0.9833\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.3449 - acc: 1.0000 - val_loss: 0.3446 - val_acc: 0.9833\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.3432 - acc: 1.0000 - val_loss: 0.3430 - val_acc: 0.9833\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.3415 - acc: 1.0000 - val_loss: 0.3414 - val_acc: 0.9833\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.3399 - acc: 1.0000 - val_loss: 0.3397 - val_acc: 0.9833\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.3383 - acc: 1.0000 - val_loss: 0.3380 - val_acc: 0.9833\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.3366 - acc: 1.0000 - val_loss: 0.3364 - val_acc: 0.9833\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.3352 - acc: 1.0000 - val_loss: 0.3346 - val_acc: 0.9833\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.3335 - acc: 1.0000 - val_loss: 0.3331 - val_acc: 0.9833\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.3318 - acc: 1.0000 - val_loss: 0.3315 - val_acc: 0.9833\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.3303 - acc: 1.0000 - val_loss: 0.3298 - val_acc: 0.9833\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.3287 - acc: 1.0000 - val_loss: 0.3282 - val_acc: 0.9833\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.3272 - acc: 1.0000 - val_loss: 0.3266 - val_acc: 0.9833\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.3256 - acc: 1.0000 - val_loss: 0.3250 - val_acc: 0.9833\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.3240 - acc: 1.0000 - val_loss: 0.3235 - val_acc: 0.9833\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.3225 - acc: 1.0000 - val_loss: 0.3220 - val_acc: 0.9833\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.3210 - acc: 1.0000 - val_loss: 0.3207 - val_acc: 0.9833\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.3195 - acc: 1.0000 - val_loss: 0.3192 - val_acc: 0.9833\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.3180 - acc: 1.0000 - val_loss: 0.3176 - val_acc: 0.9833\n",
      "Epoch 159/170\n",
      " - 0s - loss: 0.3166 - acc: 1.0000 - val_loss: 0.3161 - val_acc: 0.9833\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.3151 - acc: 1.0000 - val_loss: 0.3145 - val_acc: 0.9833\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.3136 - acc: 1.0000 - val_loss: 0.3131 - val_acc: 0.9833\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.3121 - acc: 1.0000 - val_loss: 0.3117 - val_acc: 0.9833\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.3108 - acc: 1.0000 - val_loss: 0.3101 - val_acc: 0.9833\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.3093 - acc: 1.0000 - val_loss: 0.3088 - val_acc: 0.9833\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.3078 - acc: 1.0000 - val_loss: 0.3074 - val_acc: 0.9833\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.3064 - acc: 1.0000 - val_loss: 0.3059 - val_acc: 0.9833\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.3051 - acc: 1.0000 - val_loss: 0.3045 - val_acc: 0.9833\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.3036 - acc: 1.0000 - val_loss: 0.3031 - val_acc: 0.9833\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.3022 - acc: 1.0000 - val_loss: 0.3017 - val_acc: 0.9833\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.3008 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.9833\n",
      "Test accuracy: 0.9833333412806193\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.6025 - acc: 0.5500 - val_loss: 0.5472 - val_acc: 0.5333\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.5933 - acc: 0.5500 - val_loss: 0.5400 - val_acc: 0.5333\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.5852 - acc: 0.5500 - val_loss: 0.5326 - val_acc: 0.5333\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.5768 - acc: 0.5500 - val_loss: 0.5252 - val_acc: 0.5333\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.5678 - acc: 0.5500 - val_loss: 0.5178 - val_acc: 0.5333\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.5597 - acc: 0.5500 - val_loss: 0.5105 - val_acc: 0.5333\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5506 - acc: 0.5500 - val_loss: 0.5033 - val_acc: 0.5333\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5427 - acc: 0.5750 - val_loss: 0.4962 - val_acc: 0.5833\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5344 - acc: 0.5750 - val_loss: 0.4892 - val_acc: 0.5833\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5265 - acc: 0.5750 - val_loss: 0.4824 - val_acc: 0.6000\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5184 - acc: 0.5750 - val_loss: 0.4758 - val_acc: 0.6000\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5101 - acc: 0.5750 - val_loss: 0.4693 - val_acc: 0.6333\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5031 - acc: 0.5750 - val_loss: 0.4628 - val_acc: 0.6500\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.4960 - acc: 0.6000 - val_loss: 0.4566 - val_acc: 0.6500\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.4880 - acc: 0.6750 - val_loss: 0.4507 - val_acc: 0.6500\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.4813 - acc: 0.7000 - val_loss: 0.4449 - val_acc: 0.7000\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.4752 - acc: 0.7000 - val_loss: 0.4392 - val_acc: 0.7000\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.4677 - acc: 0.7000 - val_loss: 0.4339 - val_acc: 0.7000\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.4619 - acc: 0.7000 - val_loss: 0.4287 - val_acc: 0.7167\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.4552 - acc: 0.7250 - val_loss: 0.4236 - val_acc: 0.7167\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.4497 - acc: 0.7250 - val_loss: 0.4186 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      " - 0s - loss: 0.4437 - acc: 0.7250 - val_loss: 0.4138 - val_acc: 0.7500\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.4379 - acc: 0.7250 - val_loss: 0.4092 - val_acc: 0.7500\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.4325 - acc: 0.7250 - val_loss: 0.4047 - val_acc: 0.7500\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.4273 - acc: 0.7250 - val_loss: 0.4005 - val_acc: 0.8167\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.4222 - acc: 0.7250 - val_loss: 0.3964 - val_acc: 0.8167\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.4166 - acc: 0.7250 - val_loss: 0.3924 - val_acc: 0.8167\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.4127 - acc: 0.7500 - val_loss: 0.3885 - val_acc: 0.8167\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.4075 - acc: 0.7500 - val_loss: 0.3848 - val_acc: 0.8167\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.4030 - acc: 0.7500 - val_loss: 0.3812 - val_acc: 0.8500\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.3987 - acc: 0.8250 - val_loss: 0.3776 - val_acc: 0.8667\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.3944 - acc: 0.8750 - val_loss: 0.3742 - val_acc: 0.9000\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.3903 - acc: 0.8750 - val_loss: 0.3710 - val_acc: 0.9000\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.3863 - acc: 0.8750 - val_loss: 0.3679 - val_acc: 0.9333\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.3824 - acc: 0.8750 - val_loss: 0.3649 - val_acc: 0.9500\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.3784 - acc: 0.9250 - val_loss: 0.3620 - val_acc: 0.9833\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.3749 - acc: 0.9250 - val_loss: 0.3591 - val_acc: 0.9833\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.3716 - acc: 0.9250 - val_loss: 0.3563 - val_acc: 0.9833\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.3682 - acc: 0.9500 - val_loss: 0.3537 - val_acc: 0.9833\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.3646 - acc: 0.9500 - val_loss: 0.3512 - val_acc: 0.9833\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.3614 - acc: 0.9500 - val_loss: 0.3488 - val_acc: 0.9833\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.3586 - acc: 0.9500 - val_loss: 0.3465 - val_acc: 0.9833\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.3558 - acc: 0.9500 - val_loss: 0.3443 - val_acc: 0.9833\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.3527 - acc: 0.9500 - val_loss: 0.3423 - val_acc: 0.9833\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.3500 - acc: 0.9500 - val_loss: 0.3402 - val_acc: 0.9833\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.3473 - acc: 0.9500 - val_loss: 0.3383 - val_acc: 0.9833\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.3447 - acc: 0.9500 - val_loss: 0.3363 - val_acc: 0.9833\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.3427 - acc: 0.9500 - val_loss: 0.3344 - val_acc: 0.9833\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.3396 - acc: 0.9750 - val_loss: 0.3327 - val_acc: 0.9833\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.3376 - acc: 0.9750 - val_loss: 0.3310 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.3353 - acc: 0.9750 - val_loss: 0.3294 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.3330 - acc: 0.9750 - val_loss: 0.3278 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.3312 - acc: 0.9750 - val_loss: 0.3263 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.3289 - acc: 1.0000 - val_loss: 0.3249 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.3271 - acc: 1.0000 - val_loss: 0.3236 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.3251 - acc: 1.0000 - val_loss: 0.3223 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.3234 - acc: 1.0000 - val_loss: 0.3210 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.3216 - acc: 1.0000 - val_loss: 0.3198 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.3201 - acc: 1.0000 - val_loss: 0.3186 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.3184 - acc: 1.0000 - val_loss: 0.3175 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.3167 - acc: 1.0000 - val_loss: 0.3165 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.3155 - acc: 1.0000 - val_loss: 0.3155 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.3137 - acc: 1.0000 - val_loss: 0.3146 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.3125 - acc: 1.0000 - val_loss: 0.3136 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.3112 - acc: 1.0000 - val_loss: 0.3127 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.3098 - acc: 1.0000 - val_loss: 0.3119 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.3085 - acc: 1.0000 - val_loss: 0.3110 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.3073 - acc: 1.0000 - val_loss: 0.3102 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.3061 - acc: 1.0000 - val_loss: 0.3094 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.3048 - acc: 1.0000 - val_loss: 0.3086 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.3037 - acc: 1.0000 - val_loss: 0.3079 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.3025 - acc: 1.0000 - val_loss: 0.3072 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.3015 - acc: 1.0000 - val_loss: 0.3065 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.3003 - acc: 1.0000 - val_loss: 0.3059 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2994 - acc: 1.0000 - val_loss: 0.3052 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.2985 - acc: 1.0000 - val_loss: 0.3047 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.2976 - acc: 1.0000 - val_loss: 0.3041 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.2967 - acc: 1.0000 - val_loss: 0.3036 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.2957 - acc: 1.0000 - val_loss: 0.3031 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.2951 - acc: 1.0000 - val_loss: 0.3026 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.2944 - acc: 1.0000 - val_loss: 0.3021 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.2935 - acc: 1.0000 - val_loss: 0.3016 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.2929 - acc: 1.0000 - val_loss: 0.3012 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.2921 - acc: 1.0000 - val_loss: 0.3007 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.2915 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.2908 - acc: 1.0000 - val_loss: 0.2998 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.2901 - acc: 1.0000 - val_loss: 0.2994 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.2895 - acc: 1.0000 - val_loss: 0.2990 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.2889 - acc: 1.0000 - val_loss: 0.2986 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.2884 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.2877 - acc: 1.0000 - val_loss: 0.2978 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.2871 - acc: 1.0000 - val_loss: 0.2974 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.2866 - acc: 1.0000 - val_loss: 0.2970 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.2860 - acc: 1.0000 - val_loss: 0.2966 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.2855 - acc: 1.0000 - val_loss: 0.2962 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.2849 - acc: 1.0000 - val_loss: 0.2958 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.2844 - acc: 1.0000 - val_loss: 0.2954 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.2839 - acc: 1.0000 - val_loss: 0.2951 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2833 - acc: 1.0000 - val_loss: 0.2947 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2828 - acc: 1.0000 - val_loss: 0.2943 - val_acc: 1.0000\n",
      "Test accuracy: 1.0\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 1.3511 - acc: 0.0000e+00 - val_loss: 1.3797 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.3415 - acc: 0.0000e+00 - val_loss: 1.3728 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.3321 - acc: 0.0000e+00 - val_loss: 1.3651 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 1.3223 - acc: 0.0250 - val_loss: 1.3571 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 1.3115 - acc: 0.0250 - val_loss: 1.3488 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.3020 - acc: 0.0250 - val_loss: 1.3407 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 1.2903 - acc: 0.0250 - val_loss: 1.3323 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 1.2808 - acc: 0.0250 - val_loss: 1.3238 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 1.2702 - acc: 0.0250 - val_loss: 1.3153 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 1.2614 - acc: 0.0250 - val_loss: 1.3070 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 1.2522 - acc: 0.0750 - val_loss: 1.2989 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 1.2427 - acc: 0.0500 - val_loss: 1.2902 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 1.2316 - acc: 0.0750 - val_loss: 1.2818 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 1.2253 - acc: 0.0750 - val_loss: 1.2739 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 1.2142 - acc: 0.0750 - val_loss: 1.2657 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.2061 - acc: 0.0750 - val_loss: 1.2573 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      " - 0s - loss: 1.1967 - acc: 0.0750 - val_loss: 1.2488 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.1883 - acc: 0.0750 - val_loss: 1.2404 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.1788 - acc: 0.1000 - val_loss: 1.2319 - val_acc: 0.0167\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.1717 - acc: 0.0750 - val_loss: 1.2234 - val_acc: 0.0167\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.1620 - acc: 0.1000 - val_loss: 1.2151 - val_acc: 0.0333\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.1535 - acc: 0.1000 - val_loss: 1.2066 - val_acc: 0.0500\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.1465 - acc: 0.1250 - val_loss: 1.1988 - val_acc: 0.0667\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.1362 - acc: 0.1250 - val_loss: 1.1905 - val_acc: 0.0833\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.1277 - acc: 0.1250 - val_loss: 1.1823 - val_acc: 0.1000\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.1202 - acc: 0.1250 - val_loss: 1.1739 - val_acc: 0.1333\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.1120 - acc: 0.1250 - val_loss: 1.1655 - val_acc: 0.1333\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.1048 - acc: 0.1250 - val_loss: 1.1569 - val_acc: 0.1333\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.0952 - acc: 0.1250 - val_loss: 1.1485 - val_acc: 0.1333\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.0867 - acc: 0.1250 - val_loss: 1.1402 - val_acc: 0.1333\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.0803 - acc: 0.1750 - val_loss: 1.1323 - val_acc: 0.1667\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.0716 - acc: 0.2000 - val_loss: 1.1243 - val_acc: 0.1667\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.0628 - acc: 0.2000 - val_loss: 1.1158 - val_acc: 0.1667\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.0558 - acc: 0.2000 - val_loss: 1.1073 - val_acc: 0.1833\n",
      "Epoch 35/100\n",
      " - 0s - loss: 1.0486 - acc: 0.2250 - val_loss: 1.0992 - val_acc: 0.2500\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.0402 - acc: 0.2250 - val_loss: 1.0909 - val_acc: 0.2667\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.0335 - acc: 0.2250 - val_loss: 1.0830 - val_acc: 0.2667\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.0239 - acc: 0.2750 - val_loss: 1.0751 - val_acc: 0.2667\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.0177 - acc: 0.2500 - val_loss: 1.0670 - val_acc: 0.2667\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.0093 - acc: 0.3250 - val_loss: 1.0593 - val_acc: 0.3167\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.0015 - acc: 0.3500 - val_loss: 1.0515 - val_acc: 0.3333\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.9939 - acc: 0.3500 - val_loss: 1.0437 - val_acc: 0.3500\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.9875 - acc: 0.3500 - val_loss: 1.0357 - val_acc: 0.3667\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.9783 - acc: 0.3750 - val_loss: 1.0281 - val_acc: 0.3833\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.9713 - acc: 0.3750 - val_loss: 1.0202 - val_acc: 0.4000\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.9650 - acc: 0.4250 - val_loss: 1.0127 - val_acc: 0.4000\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.9567 - acc: 0.4500 - val_loss: 1.0052 - val_acc: 0.4000\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.9504 - acc: 0.4500 - val_loss: 0.9977 - val_acc: 0.4000\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.9422 - acc: 0.4500 - val_loss: 0.9900 - val_acc: 0.4000\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.9351 - acc: 0.4750 - val_loss: 0.9824 - val_acc: 0.4000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.9279 - acc: 0.4750 - val_loss: 0.9749 - val_acc: 0.4167\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.9212 - acc: 0.4750 - val_loss: 0.9673 - val_acc: 0.4167\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.9142 - acc: 0.4750 - val_loss: 0.9598 - val_acc: 0.4167\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.9070 - acc: 0.4750 - val_loss: 0.9522 - val_acc: 0.4167\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.8993 - acc: 0.4750 - val_loss: 0.9450 - val_acc: 0.4167\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.8924 - acc: 0.5250 - val_loss: 0.9378 - val_acc: 0.4167\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.8865 - acc: 0.5250 - val_loss: 0.9305 - val_acc: 0.4333\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.8787 - acc: 0.5250 - val_loss: 0.9232 - val_acc: 0.4333\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.8721 - acc: 0.5250 - val_loss: 0.9158 - val_acc: 0.4333\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.8658 - acc: 0.5250 - val_loss: 0.9086 - val_acc: 0.4333\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.8598 - acc: 0.5250 - val_loss: 0.9016 - val_acc: 0.4500\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.8520 - acc: 0.5250 - val_loss: 0.8945 - val_acc: 0.4500\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.8464 - acc: 0.5250 - val_loss: 0.8878 - val_acc: 0.4500\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.8401 - acc: 0.5500 - val_loss: 0.8808 - val_acc: 0.4667\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.8324 - acc: 0.5500 - val_loss: 0.8737 - val_acc: 0.4667\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.8254 - acc: 0.5500 - val_loss: 0.8669 - val_acc: 0.4667\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.8198 - acc: 0.5500 - val_loss: 0.8599 - val_acc: 0.4667\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.8124 - acc: 0.5500 - val_loss: 0.8532 - val_acc: 0.4667\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.8065 - acc: 0.5500 - val_loss: 0.8464 - val_acc: 0.4667\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.7997 - acc: 0.5500 - val_loss: 0.8399 - val_acc: 0.4667\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.7936 - acc: 0.5500 - val_loss: 0.8334 - val_acc: 0.4667\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.7873 - acc: 0.5500 - val_loss: 0.8268 - val_acc: 0.4667\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.7822 - acc: 0.5500 - val_loss: 0.8204 - val_acc: 0.4667\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.7751 - acc: 0.5500 - val_loss: 0.8138 - val_acc: 0.4667\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.7686 - acc: 0.5500 - val_loss: 0.8072 - val_acc: 0.4667\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.7634 - acc: 0.5500 - val_loss: 0.8006 - val_acc: 0.4667\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.7574 - acc: 0.5500 - val_loss: 0.7942 - val_acc: 0.4667\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.7508 - acc: 0.5500 - val_loss: 0.7879 - val_acc: 0.4667\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.7449 - acc: 0.5500 - val_loss: 0.7816 - val_acc: 0.4667\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.7389 - acc: 0.5500 - val_loss: 0.7754 - val_acc: 0.4667\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.7343 - acc: 0.5500 - val_loss: 0.7695 - val_acc: 0.4667\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.7273 - acc: 0.5500 - val_loss: 0.7631 - val_acc: 0.4667\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.7229 - acc: 0.5500 - val_loss: 0.7572 - val_acc: 0.4667\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.7160 - acc: 0.5500 - val_loss: 0.7511 - val_acc: 0.4667\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7099 - acc: 0.5500 - val_loss: 0.7451 - val_acc: 0.4667\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.7055 - acc: 0.5500 - val_loss: 0.7389 - val_acc: 0.4667\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.6987 - acc: 0.5500 - val_loss: 0.7331 - val_acc: 0.4667\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.6940 - acc: 0.5500 - val_loss: 0.7272 - val_acc: 0.4667\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.6885 - acc: 0.5500 - val_loss: 0.7214 - val_acc: 0.4667\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.6831 - acc: 0.5500 - val_loss: 0.7157 - val_acc: 0.4667\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.6774 - acc: 0.5500 - val_loss: 0.7100 - val_acc: 0.4667\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.6723 - acc: 0.5500 - val_loss: 0.7044 - val_acc: 0.4667\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.6673 - acc: 0.5500 - val_loss: 0.6989 - val_acc: 0.4667\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.6610 - acc: 0.5500 - val_loss: 0.6936 - val_acc: 0.4667\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.6562 - acc: 0.5500 - val_loss: 0.6882 - val_acc: 0.4667\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6507 - acc: 0.5500 - val_loss: 0.6829 - val_acc: 0.4667\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6457 - acc: 0.5500 - val_loss: 0.6775 - val_acc: 0.4667\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6412 - acc: 0.5500 - val_loss: 0.6723 - val_acc: 0.4667\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6355 - acc: 0.5500 - val_loss: 0.6670 - val_acc: 0.4667\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6313 - acc: 0.5500 - val_loss: 0.6618 - val_acc: 0.4667\n",
      "Test accuracy: 0.4666666626930237\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 2.5387 - acc: 0.5500 - val_loss: 2.9085 - val_acc: 0.4667\n",
      "Epoch 2/170\n",
      " - 0s - loss: 2.5212 - acc: 0.5500 - val_loss: 2.8939 - val_acc: 0.4667\n",
      "Epoch 3/170\n",
      " - 0s - loss: 2.5081 - acc: 0.5500 - val_loss: 2.8771 - val_acc: 0.4667\n",
      "Epoch 4/170\n",
      " - 0s - loss: 2.4935 - acc: 0.5500 - val_loss: 2.8592 - val_acc: 0.4667\n",
      "Epoch 5/170\n",
      " - 0s - loss: 2.4774 - acc: 0.5500 - val_loss: 2.8392 - val_acc: 0.4667\n",
      "Epoch 6/170\n",
      " - 0s - loss: 2.4605 - acc: 0.5500 - val_loss: 2.8196 - val_acc: 0.4667\n",
      "Epoch 7/170\n",
      " - 0s - loss: 2.4444 - acc: 0.5500 - val_loss: 2.8016 - val_acc: 0.4667\n",
      "Epoch 8/170\n",
      " - 0s - loss: 2.4270 - acc: 0.5500 - val_loss: 2.7791 - val_acc: 0.4667\n",
      "Epoch 9/170\n",
      " - 0s - loss: 2.4093 - acc: 0.5500 - val_loss: 2.7602 - val_acc: 0.4667\n",
      "Epoch 10/170\n",
      " - 0s - loss: 2.3920 - acc: 0.5500 - val_loss: 2.7392 - val_acc: 0.4667\n",
      "Epoch 11/170\n",
      " - 0s - loss: 2.3736 - acc: 0.5500 - val_loss: 2.7176 - val_acc: 0.4667\n",
      "Epoch 12/170\n",
      " - 0s - loss: 2.3560 - acc: 0.5500 - val_loss: 2.6978 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/170\n",
      " - 0s - loss: 2.3368 - acc: 0.5500 - val_loss: 2.6734 - val_acc: 0.4667\n",
      "Epoch 14/170\n",
      " - 0s - loss: 2.3162 - acc: 0.5500 - val_loss: 2.6503 - val_acc: 0.4667\n",
      "Epoch 15/170\n",
      " - 0s - loss: 2.2964 - acc: 0.5500 - val_loss: 2.6275 - val_acc: 0.4667\n",
      "Epoch 16/170\n",
      " - 0s - loss: 2.2765 - acc: 0.5500 - val_loss: 2.6043 - val_acc: 0.4667\n",
      "Epoch 17/170\n",
      " - 0s - loss: 2.2569 - acc: 0.5500 - val_loss: 2.5820 - val_acc: 0.4667\n",
      "Epoch 18/170\n",
      " - 0s - loss: 2.2378 - acc: 0.5500 - val_loss: 2.5600 - val_acc: 0.4667\n",
      "Epoch 19/170\n",
      " - 0s - loss: 2.2197 - acc: 0.5500 - val_loss: 2.5398 - val_acc: 0.4667\n",
      "Epoch 20/170\n",
      " - 0s - loss: 2.2008 - acc: 0.5500 - val_loss: 2.5170 - val_acc: 0.4667\n",
      "Epoch 21/170\n",
      " - 0s - loss: 2.1810 - acc: 0.5500 - val_loss: 2.4939 - val_acc: 0.4667\n",
      "Epoch 22/170\n",
      " - 0s - loss: 2.1607 - acc: 0.5500 - val_loss: 2.4699 - val_acc: 0.4667\n",
      "Epoch 23/170\n",
      " - 0s - loss: 2.1403 - acc: 0.5500 - val_loss: 2.4468 - val_acc: 0.4667\n",
      "Epoch 24/170\n",
      " - 0s - loss: 2.1208 - acc: 0.5500 - val_loss: 2.4247 - val_acc: 0.4667\n",
      "Epoch 25/170\n",
      " - 0s - loss: 2.1011 - acc: 0.5500 - val_loss: 2.4015 - val_acc: 0.4667\n",
      "Epoch 26/170\n",
      " - 0s - loss: 2.0807 - acc: 0.5500 - val_loss: 2.3773 - val_acc: 0.4667\n",
      "Epoch 27/170\n",
      " - 0s - loss: 2.0606 - acc: 0.5500 - val_loss: 2.3550 - val_acc: 0.4667\n",
      "Epoch 28/170\n",
      " - 0s - loss: 2.0415 - acc: 0.5500 - val_loss: 2.3330 - val_acc: 0.4667\n",
      "Epoch 29/170\n",
      " - 0s - loss: 2.0224 - acc: 0.5500 - val_loss: 2.3110 - val_acc: 0.4667\n",
      "Epoch 30/170\n",
      " - 0s - loss: 2.0034 - acc: 0.5500 - val_loss: 2.2891 - val_acc: 0.4667\n",
      "Epoch 31/170\n",
      " - 0s - loss: 1.9843 - acc: 0.5500 - val_loss: 2.2670 - val_acc: 0.4667\n",
      "Epoch 32/170\n",
      " - 0s - loss: 1.9654 - acc: 0.5500 - val_loss: 2.2452 - val_acc: 0.4667\n",
      "Epoch 33/170\n",
      " - 0s - loss: 1.9470 - acc: 0.5500 - val_loss: 2.2243 - val_acc: 0.4667\n",
      "Epoch 34/170\n",
      " - 0s - loss: 1.9290 - acc: 0.5500 - val_loss: 2.2037 - val_acc: 0.4667\n",
      "Epoch 35/170\n",
      " - 0s - loss: 1.9113 - acc: 0.5500 - val_loss: 2.1833 - val_acc: 0.4667\n",
      "Epoch 36/170\n",
      " - 0s - loss: 1.8935 - acc: 0.5500 - val_loss: 2.1628 - val_acc: 0.4667\n",
      "Epoch 37/170\n",
      " - 0s - loss: 1.8753 - acc: 0.5500 - val_loss: 2.1413 - val_acc: 0.4667\n",
      "Epoch 38/170\n",
      " - 0s - loss: 1.8568 - acc: 0.5500 - val_loss: 2.1200 - val_acc: 0.4667\n",
      "Epoch 39/170\n",
      " - 0s - loss: 1.8380 - acc: 0.5500 - val_loss: 2.0977 - val_acc: 0.4667\n",
      "Epoch 40/170\n",
      " - 0s - loss: 1.8192 - acc: 0.5500 - val_loss: 2.0762 - val_acc: 0.4667\n",
      "Epoch 41/170\n",
      " - 0s - loss: 1.8001 - acc: 0.5500 - val_loss: 2.0537 - val_acc: 0.4667\n",
      "Epoch 42/170\n",
      " - 0s - loss: 1.7817 - acc: 0.5500 - val_loss: 2.0331 - val_acc: 0.4667\n",
      "Epoch 43/170\n",
      " - 0s - loss: 1.7635 - acc: 0.5500 - val_loss: 2.0119 - val_acc: 0.4667\n",
      "Epoch 44/170\n",
      " - 0s - loss: 1.7447 - acc: 0.5500 - val_loss: 1.9896 - val_acc: 0.4667\n",
      "Epoch 45/170\n",
      " - 0s - loss: 1.7255 - acc: 0.5500 - val_loss: 1.9674 - val_acc: 0.4667\n",
      "Epoch 46/170\n",
      " - 0s - loss: 1.7063 - acc: 0.5500 - val_loss: 1.9449 - val_acc: 0.4667\n",
      "Epoch 47/170\n",
      " - 0s - loss: 1.6875 - acc: 0.5500 - val_loss: 1.9237 - val_acc: 0.4667\n",
      "Epoch 48/170\n",
      " - 0s - loss: 1.6687 - acc: 0.5500 - val_loss: 1.9015 - val_acc: 0.4667\n",
      "Epoch 49/170\n",
      " - 0s - loss: 1.6494 - acc: 0.5500 - val_loss: 1.8787 - val_acc: 0.4667\n",
      "Epoch 50/170\n",
      " - 0s - loss: 1.6309 - acc: 0.5500 - val_loss: 1.8585 - val_acc: 0.4667\n",
      "Epoch 51/170\n",
      " - 0s - loss: 1.6126 - acc: 0.5500 - val_loss: 1.8366 - val_acc: 0.4667\n",
      "Epoch 52/170\n",
      " - 0s - loss: 1.5938 - acc: 0.5500 - val_loss: 1.8148 - val_acc: 0.4667\n",
      "Epoch 53/170\n",
      " - 0s - loss: 1.5760 - acc: 0.5500 - val_loss: 1.7949 - val_acc: 0.4667\n",
      "Epoch 54/170\n",
      " - 0s - loss: 1.5590 - acc: 0.5500 - val_loss: 1.7755 - val_acc: 0.4667\n",
      "Epoch 55/170\n",
      " - 0s - loss: 1.5417 - acc: 0.5500 - val_loss: 1.7551 - val_acc: 0.4667\n",
      "Epoch 56/170\n",
      " - 0s - loss: 1.5247 - acc: 0.5500 - val_loss: 1.7359 - val_acc: 0.4667\n",
      "Epoch 57/170\n",
      " - 0s - loss: 1.5089 - acc: 0.5500 - val_loss: 1.7181 - val_acc: 0.4667\n",
      "Epoch 58/170\n",
      " - 0s - loss: 1.4915 - acc: 0.5500 - val_loss: 1.6958 - val_acc: 0.4667\n",
      "Epoch 59/170\n",
      " - 0s - loss: 1.4733 - acc: 0.5500 - val_loss: 1.6758 - val_acc: 0.4667\n",
      "Epoch 60/170\n",
      " - 0s - loss: 1.4559 - acc: 0.5500 - val_loss: 1.6553 - val_acc: 0.4667\n",
      "Epoch 61/170\n",
      " - 0s - loss: 1.4393 - acc: 0.5500 - val_loss: 1.6368 - val_acc: 0.4667\n",
      "Epoch 62/170\n",
      " - 0s - loss: 1.4230 - acc: 0.5500 - val_loss: 1.6176 - val_acc: 0.4667\n",
      "Epoch 63/170\n",
      " - 0s - loss: 1.4066 - acc: 0.5500 - val_loss: 1.5986 - val_acc: 0.4667\n",
      "Epoch 64/170\n",
      " - 0s - loss: 1.3903 - acc: 0.5500 - val_loss: 1.5797 - val_acc: 0.4667\n",
      "Epoch 65/170\n",
      " - 0s - loss: 1.3737 - acc: 0.5500 - val_loss: 1.5599 - val_acc: 0.4667\n",
      "Epoch 66/170\n",
      " - 0s - loss: 1.3574 - acc: 0.5500 - val_loss: 1.5414 - val_acc: 0.4667\n",
      "Epoch 67/170\n",
      " - 0s - loss: 1.3408 - acc: 0.5500 - val_loss: 1.5211 - val_acc: 0.4667\n",
      "Epoch 68/170\n",
      " - 0s - loss: 1.3239 - acc: 0.5500 - val_loss: 1.5019 - val_acc: 0.4667\n",
      "Epoch 69/170\n",
      " - 0s - loss: 1.3075 - acc: 0.5500 - val_loss: 1.4826 - val_acc: 0.4667\n",
      "Epoch 70/170\n",
      " - 0s - loss: 1.2912 - acc: 0.5500 - val_loss: 1.4635 - val_acc: 0.4667\n",
      "Epoch 71/170\n",
      " - 0s - loss: 1.2751 - acc: 0.5500 - val_loss: 1.4449 - val_acc: 0.4667\n",
      "Epoch 72/170\n",
      " - 0s - loss: 1.2601 - acc: 0.5500 - val_loss: 1.4280 - val_acc: 0.4667\n",
      "Epoch 73/170\n",
      " - 0s - loss: 1.2444 - acc: 0.5500 - val_loss: 1.4081 - val_acc: 0.4667\n",
      "Epoch 74/170\n",
      " - 0s - loss: 1.2281 - acc: 0.5500 - val_loss: 1.3899 - val_acc: 0.4667\n",
      "Epoch 75/170\n",
      " - 0s - loss: 1.2129 - acc: 0.5500 - val_loss: 1.3722 - val_acc: 0.4667\n",
      "Epoch 76/170\n",
      " - 0s - loss: 1.1980 - acc: 0.5500 - val_loss: 1.3546 - val_acc: 0.4667\n",
      "Epoch 77/170\n",
      " - 0s - loss: 1.1828 - acc: 0.5500 - val_loss: 1.3364 - val_acc: 0.4667\n",
      "Epoch 78/170\n",
      " - 0s - loss: 1.1684 - acc: 0.5500 - val_loss: 1.3204 - val_acc: 0.4667\n",
      "Epoch 79/170\n",
      " - 0s - loss: 1.1550 - acc: 0.5500 - val_loss: 1.3046 - val_acc: 0.4667\n",
      "Epoch 80/170\n",
      " - 0s - loss: 1.1422 - acc: 0.5500 - val_loss: 1.2900 - val_acc: 0.4667\n",
      "Epoch 81/170\n",
      " - 0s - loss: 1.1292 - acc: 0.5500 - val_loss: 1.2742 - val_acc: 0.4667\n",
      "Epoch 82/170\n",
      " - 0s - loss: 1.1161 - acc: 0.5500 - val_loss: 1.2589 - val_acc: 0.4667\n",
      "Epoch 83/170\n",
      " - 0s - loss: 1.1033 - acc: 0.5500 - val_loss: 1.2436 - val_acc: 0.4667\n",
      "Epoch 84/170\n",
      " - 0s - loss: 1.0920 - acc: 0.5500 - val_loss: 1.2311 - val_acc: 0.4667\n",
      "Epoch 85/170\n",
      " - 0s - loss: 1.0811 - acc: 0.5500 - val_loss: 1.2184 - val_acc: 0.4667\n",
      "Epoch 86/170\n",
      " - 0s - loss: 1.0694 - acc: 0.5500 - val_loss: 1.2036 - val_acc: 0.4667\n",
      "Epoch 87/170\n",
      " - 0s - loss: 1.0588 - acc: 0.5500 - val_loss: 1.1922 - val_acc: 0.4667\n",
      "Epoch 88/170\n",
      " - 0s - loss: 1.0479 - acc: 0.5500 - val_loss: 1.1782 - val_acc: 0.4667\n",
      "Epoch 89/170\n",
      " - 0s - loss: 1.0363 - acc: 0.5500 - val_loss: 1.1640 - val_acc: 0.4667\n",
      "Epoch 90/170\n",
      " - 0s - loss: 1.0262 - acc: 0.5500 - val_loss: 1.1532 - val_acc: 0.4667\n",
      "Epoch 91/170\n",
      " - 0s - loss: 1.0164 - acc: 0.5500 - val_loss: 1.1413 - val_acc: 0.4667\n",
      "Epoch 92/170\n",
      " - 0s - loss: 1.0067 - acc: 0.5500 - val_loss: 1.1298 - val_acc: 0.4667\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.9968 - acc: 0.5500 - val_loss: 1.1173 - val_acc: 0.4667\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.9867 - acc: 0.5500 - val_loss: 1.1054 - val_acc: 0.4667\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.9767 - acc: 0.5500 - val_loss: 1.0928 - val_acc: 0.4667\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.9671 - acc: 0.5500 - val_loss: 1.0819 - val_acc: 0.4667\n",
      "Epoch 97/170\n",
      " - 0s - loss: 0.9583 - acc: 0.5500 - val_loss: 1.0714 - val_acc: 0.4667\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.9495 - acc: 0.5500 - val_loss: 1.0605 - val_acc: 0.4667\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.9403 - acc: 0.5500 - val_loss: 1.0486 - val_acc: 0.4667\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.9318 - acc: 0.5500 - val_loss: 1.0394 - val_acc: 0.4667\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.9240 - acc: 0.5500 - val_loss: 1.0299 - val_acc: 0.4667\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.9165 - acc: 0.5500 - val_loss: 1.0209 - val_acc: 0.4667\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.9088 - acc: 0.5500 - val_loss: 1.0113 - val_acc: 0.4667\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.9008 - acc: 0.5500 - val_loss: 1.0005 - val_acc: 0.4667\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.8926 - acc: 0.5500 - val_loss: 0.9912 - val_acc: 0.4667\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.8850 - acc: 0.5500 - val_loss: 0.9816 - val_acc: 0.4667\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.8782 - acc: 0.5500 - val_loss: 0.9739 - val_acc: 0.4667\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.8715 - acc: 0.5500 - val_loss: 0.9654 - val_acc: 0.4667\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.8648 - acc: 0.5500 - val_loss: 0.9572 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/170\n",
      " - 0s - loss: 0.8588 - acc: 0.5500 - val_loss: 0.9503 - val_acc: 0.4667\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.8525 - acc: 0.5500 - val_loss: 0.9421 - val_acc: 0.4667\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.8462 - acc: 0.5500 - val_loss: 0.9346 - val_acc: 0.4667\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.8404 - acc: 0.5500 - val_loss: 0.9276 - val_acc: 0.4667\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.8355 - acc: 0.5500 - val_loss: 0.9221 - val_acc: 0.4667\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.8298 - acc: 0.5500 - val_loss: 0.9147 - val_acc: 0.4667\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.8243 - acc: 0.5500 - val_loss: 0.9084 - val_acc: 0.4667\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.8187 - acc: 0.5500 - val_loss: 0.9007 - val_acc: 0.4667\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.8129 - acc: 0.5500 - val_loss: 0.8940 - val_acc: 0.4667\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.8075 - acc: 0.5500 - val_loss: 0.8874 - val_acc: 0.4667\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.8021 - acc: 0.5500 - val_loss: 0.8804 - val_acc: 0.4667\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.7973 - acc: 0.5500 - val_loss: 0.8750 - val_acc: 0.4667\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.7922 - acc: 0.5500 - val_loss: 0.8684 - val_acc: 0.4667\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.7878 - acc: 0.5500 - val_loss: 0.8638 - val_acc: 0.4667\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.7830 - acc: 0.5500 - val_loss: 0.8567 - val_acc: 0.4667\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.7778 - acc: 0.5500 - val_loss: 0.8513 - val_acc: 0.4667\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.7733 - acc: 0.5500 - val_loss: 0.8462 - val_acc: 0.4667\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.7690 - acc: 0.5500 - val_loss: 0.8411 - val_acc: 0.4667\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.7647 - acc: 0.5500 - val_loss: 0.8358 - val_acc: 0.4667\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.7602 - acc: 0.5500 - val_loss: 0.8300 - val_acc: 0.4667\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.7557 - acc: 0.5500 - val_loss: 0.8247 - val_acc: 0.4667\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.7515 - acc: 0.5500 - val_loss: 0.8199 - val_acc: 0.4667\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.7472 - acc: 0.5500 - val_loss: 0.8147 - val_acc: 0.4667\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.7430 - acc: 0.5500 - val_loss: 0.8093 - val_acc: 0.4667\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.7387 - acc: 0.5500 - val_loss: 0.8043 - val_acc: 0.4667\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.7347 - acc: 0.5500 - val_loss: 0.7999 - val_acc: 0.4667\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.7307 - acc: 0.5500 - val_loss: 0.7951 - val_acc: 0.4667\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.7268 - acc: 0.5500 - val_loss: 0.7907 - val_acc: 0.4667\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.7232 - acc: 0.5500 - val_loss: 0.7868 - val_acc: 0.4667\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.7194 - acc: 0.5500 - val_loss: 0.7824 - val_acc: 0.4667\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.7157 - acc: 0.5500 - val_loss: 0.7769 - val_acc: 0.4667\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.7113 - acc: 0.5500 - val_loss: 0.7721 - val_acc: 0.4667\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.7073 - acc: 0.5500 - val_loss: 0.7675 - val_acc: 0.4667\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.7037 - acc: 0.5500 - val_loss: 0.7621 - val_acc: 0.4667\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.6997 - acc: 0.5500 - val_loss: 0.7583 - val_acc: 0.4667\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.6960 - acc: 0.5500 - val_loss: 0.7534 - val_acc: 0.4667\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.6921 - acc: 0.5500 - val_loss: 0.7488 - val_acc: 0.4667\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.6887 - acc: 0.5500 - val_loss: 0.7452 - val_acc: 0.4667\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.6853 - acc: 0.5500 - val_loss: 0.7417 - val_acc: 0.4667\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.6818 - acc: 0.5500 - val_loss: 0.7381 - val_acc: 0.4667\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.6788 - acc: 0.5500 - val_loss: 0.7351 - val_acc: 0.4667\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.6758 - acc: 0.5500 - val_loss: 0.7324 - val_acc: 0.4667\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.6725 - acc: 0.5500 - val_loss: 0.7289 - val_acc: 0.4667\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.6693 - acc: 0.5500 - val_loss: 0.7255 - val_acc: 0.4667\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.6661 - acc: 0.5500 - val_loss: 0.7222 - val_acc: 0.4667\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.6630 - acc: 0.5500 - val_loss: 0.7186 - val_acc: 0.4667\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.6597 - acc: 0.5500 - val_loss: 0.7145 - val_acc: 0.4667\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.6564 - acc: 0.5500 - val_loss: 0.7111 - val_acc: 0.4667\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.6532 - acc: 0.5500 - val_loss: 0.7077 - val_acc: 0.4667\n",
      "Epoch 159/170\n",
      " - 0s - loss: 0.6500 - acc: 0.5500 - val_loss: 0.7041 - val_acc: 0.4667\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.6468 - acc: 0.5500 - val_loss: 0.7004 - val_acc: 0.4667\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.6438 - acc: 0.5500 - val_loss: 0.6973 - val_acc: 0.4667\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.6407 - acc: 0.5500 - val_loss: 0.6940 - val_acc: 0.4667\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.6377 - acc: 0.5500 - val_loss: 0.6908 - val_acc: 0.4667\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.6346 - acc: 0.5500 - val_loss: 0.6873 - val_acc: 0.4667\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.6316 - acc: 0.5500 - val_loss: 0.6843 - val_acc: 0.4667\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.6286 - acc: 0.5500 - val_loss: 0.6808 - val_acc: 0.4667\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.6255 - acc: 0.5500 - val_loss: 0.6770 - val_acc: 0.4667\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.6225 - acc: 0.5500 - val_loss: 0.6740 - val_acc: 0.4667\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.6195 - acc: 0.5500 - val_loss: 0.6702 - val_acc: 0.4667\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.6163 - acc: 0.5500 - val_loss: 0.6668 - val_acc: 0.4667\n",
      "Test accuracy: 0.4666666626930237\n",
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 2.2162 - acc: 0.5500 - val_loss: 2.5267 - val_acc: 0.4667\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.1981 - acc: 0.5500 - val_loss: 2.5105 - val_acc: 0.4667\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.1840 - acc: 0.5500 - val_loss: 2.4935 - val_acc: 0.4667\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.1692 - acc: 0.5500 - val_loss: 2.4754 - val_acc: 0.4667\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.1533 - acc: 0.5500 - val_loss: 2.4562 - val_acc: 0.4667\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.1367 - acc: 0.5500 - val_loss: 2.4365 - val_acc: 0.4667\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.1195 - acc: 0.5500 - val_loss: 2.4160 - val_acc: 0.4667\n",
      "Epoch 8/100\n",
      " - 0s - loss: 2.1021 - acc: 0.5500 - val_loss: 2.3960 - val_acc: 0.4667\n",
      "Epoch 9/100\n",
      " - 0s - loss: 2.0851 - acc: 0.5500 - val_loss: 2.3764 - val_acc: 0.4667\n",
      "Epoch 10/100\n",
      " - 0s - loss: 2.0669 - acc: 0.5500 - val_loss: 2.3537 - val_acc: 0.4667\n",
      "Epoch 11/100\n",
      " - 0s - loss: 2.0481 - acc: 0.5500 - val_loss: 2.3328 - val_acc: 0.4667\n",
      "Epoch 12/100\n",
      " - 0s - loss: 2.0288 - acc: 0.5500 - val_loss: 2.3090 - val_acc: 0.4667\n",
      "Epoch 13/100\n",
      " - 0s - loss: 2.0089 - acc: 0.5500 - val_loss: 2.2868 - val_acc: 0.4667\n",
      "Epoch 14/100\n",
      " - 0s - loss: 1.9901 - acc: 0.5500 - val_loss: 2.2655 - val_acc: 0.4667\n",
      "Epoch 15/100\n",
      " - 0s - loss: 1.9708 - acc: 0.5500 - val_loss: 2.2423 - val_acc: 0.4667\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.9515 - acc: 0.5500 - val_loss: 2.2207 - val_acc: 0.4667\n",
      "Epoch 17/100\n",
      " - 0s - loss: 1.9337 - acc: 0.5500 - val_loss: 2.2010 - val_acc: 0.4667\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.9163 - acc: 0.5500 - val_loss: 2.1807 - val_acc: 0.4667\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.8977 - acc: 0.5500 - val_loss: 2.1584 - val_acc: 0.4667\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.8785 - acc: 0.5500 - val_loss: 2.1362 - val_acc: 0.4667\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.8593 - acc: 0.5500 - val_loss: 2.1139 - val_acc: 0.4667\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.8399 - acc: 0.5500 - val_loss: 2.0915 - val_acc: 0.4667\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.8209 - acc: 0.5500 - val_loss: 2.0699 - val_acc: 0.4667\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.8023 - acc: 0.5500 - val_loss: 2.0484 - val_acc: 0.4667\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.7831 - acc: 0.5500 - val_loss: 2.0259 - val_acc: 0.4667\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.7637 - acc: 0.5500 - val_loss: 2.0035 - val_acc: 0.4667\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.7443 - acc: 0.5500 - val_loss: 1.9811 - val_acc: 0.4667\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.7249 - acc: 0.5500 - val_loss: 1.9586 - val_acc: 0.4667\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.7068 - acc: 0.5500 - val_loss: 1.9389 - val_acc: 0.4667\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.6903 - acc: 0.5500 - val_loss: 1.9205 - val_acc: 0.4667\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.6733 - acc: 0.5500 - val_loss: 1.9006 - val_acc: 0.4667\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.6545 - acc: 0.5500 - val_loss: 1.8768 - val_acc: 0.4667\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.6351 - acc: 0.5500 - val_loss: 1.8558 - val_acc: 0.4667\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.6160 - acc: 0.5500 - val_loss: 1.8329 - val_acc: 0.4667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      " - 0s - loss: 1.5965 - acc: 0.5500 - val_loss: 1.8108 - val_acc: 0.4667\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.5771 - acc: 0.5500 - val_loss: 1.7880 - val_acc: 0.4667\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.5567 - acc: 0.5500 - val_loss: 1.7634 - val_acc: 0.4667\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.5362 - acc: 0.5500 - val_loss: 1.7409 - val_acc: 0.4667\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.5170 - acc: 0.5500 - val_loss: 1.7189 - val_acc: 0.4667\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.4987 - acc: 0.5500 - val_loss: 1.6985 - val_acc: 0.4667\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.4813 - acc: 0.5500 - val_loss: 1.6788 - val_acc: 0.4667\n",
      "Epoch 42/100\n",
      " - 0s - loss: 1.4636 - acc: 0.5500 - val_loss: 1.6582 - val_acc: 0.4667\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.4451 - acc: 0.5500 - val_loss: 1.6362 - val_acc: 0.4667\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.4260 - acc: 0.5500 - val_loss: 1.6140 - val_acc: 0.4667\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.4076 - acc: 0.5500 - val_loss: 1.5935 - val_acc: 0.4667\n",
      "Epoch 46/100\n",
      " - 0s - loss: 1.3899 - acc: 0.5500 - val_loss: 1.5732 - val_acc: 0.4667\n",
      "Epoch 47/100\n",
      " - 0s - loss: 1.3725 - acc: 0.5500 - val_loss: 1.5533 - val_acc: 0.4667\n",
      "Epoch 48/100\n",
      " - 0s - loss: 1.3552 - acc: 0.5500 - val_loss: 1.5334 - val_acc: 0.4667\n",
      "Epoch 49/100\n",
      " - 0s - loss: 1.3384 - acc: 0.5500 - val_loss: 1.5144 - val_acc: 0.4667\n",
      "Epoch 50/100\n",
      " - 0s - loss: 1.3213 - acc: 0.5500 - val_loss: 1.4941 - val_acc: 0.4667\n",
      "Epoch 51/100\n",
      " - 0s - loss: 1.3046 - acc: 0.5500 - val_loss: 1.4757 - val_acc: 0.4667\n",
      "Epoch 52/100\n",
      " - 0s - loss: 1.2885 - acc: 0.5500 - val_loss: 1.4571 - val_acc: 0.4667\n",
      "Epoch 53/100\n",
      " - 0s - loss: 1.2726 - acc: 0.5500 - val_loss: 1.4390 - val_acc: 0.4667\n",
      "Epoch 54/100\n",
      " - 0s - loss: 1.2558 - acc: 0.5500 - val_loss: 1.4185 - val_acc: 0.4667\n",
      "Epoch 55/100\n",
      " - 0s - loss: 1.2381 - acc: 0.5500 - val_loss: 1.3979 - val_acc: 0.4667\n",
      "Epoch 56/100\n",
      " - 0s - loss: 1.2211 - acc: 0.5500 - val_loss: 1.3790 - val_acc: 0.4667\n",
      "Epoch 57/100\n",
      " - 0s - loss: 1.2047 - acc: 0.5500 - val_loss: 1.3602 - val_acc: 0.4667\n",
      "Epoch 58/100\n",
      " - 0s - loss: 1.1877 - acc: 0.5500 - val_loss: 1.3397 - val_acc: 0.4667\n",
      "Epoch 59/100\n",
      " - 0s - loss: 1.1714 - acc: 0.5500 - val_loss: 1.3219 - val_acc: 0.4667\n",
      "Epoch 60/100\n",
      " - 0s - loss: 1.1556 - acc: 0.5500 - val_loss: 1.3036 - val_acc: 0.4667\n",
      "Epoch 61/100\n",
      " - 0s - loss: 1.1395 - acc: 0.5500 - val_loss: 1.2846 - val_acc: 0.4667\n",
      "Epoch 62/100\n",
      " - 0s - loss: 1.1240 - acc: 0.5500 - val_loss: 1.2674 - val_acc: 0.4667\n",
      "Epoch 63/100\n",
      " - 0s - loss: 1.1084 - acc: 0.5500 - val_loss: 1.2489 - val_acc: 0.4667\n",
      "Epoch 64/100\n",
      " - 0s - loss: 1.0927 - acc: 0.5500 - val_loss: 1.2310 - val_acc: 0.4667\n",
      "Epoch 65/100\n",
      " - 0s - loss: 1.0770 - acc: 0.5500 - val_loss: 1.2125 - val_acc: 0.4667\n",
      "Epoch 66/100\n",
      " - 0s - loss: 1.0612 - acc: 0.5500 - val_loss: 1.1942 - val_acc: 0.4667\n",
      "Epoch 67/100\n",
      " - 0s - loss: 1.0454 - acc: 0.5500 - val_loss: 1.1759 - val_acc: 0.4667\n",
      "Epoch 68/100\n",
      " - 0s - loss: 1.0292 - acc: 0.5500 - val_loss: 1.1565 - val_acc: 0.4667\n",
      "Epoch 69/100\n",
      " - 0s - loss: 1.0141 - acc: 0.5500 - val_loss: 1.1404 - val_acc: 0.4667\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.9991 - acc: 0.5500 - val_loss: 1.1220 - val_acc: 0.4667\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.9832 - acc: 0.5500 - val_loss: 1.1031 - val_acc: 0.4667\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.9679 - acc: 0.5500 - val_loss: 1.0864 - val_acc: 0.4667\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.9538 - acc: 0.5500 - val_loss: 1.0703 - val_acc: 0.4667\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.9396 - acc: 0.5500 - val_loss: 1.0535 - val_acc: 0.4667\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.9258 - acc: 0.5500 - val_loss: 1.0379 - val_acc: 0.4667\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.9126 - acc: 0.5500 - val_loss: 1.0226 - val_acc: 0.4667\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.8987 - acc: 0.5500 - val_loss: 1.0055 - val_acc: 0.4667\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.8844 - acc: 0.5500 - val_loss: 0.9892 - val_acc: 0.4667\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.8722 - acc: 0.5500 - val_loss: 0.9760 - val_acc: 0.4667\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.8593 - acc: 0.5500 - val_loss: 0.9597 - val_acc: 0.4667\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.8461 - acc: 0.5500 - val_loss: 0.9450 - val_acc: 0.4667\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.8337 - acc: 0.5500 - val_loss: 0.9305 - val_acc: 0.4667\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.8219 - acc: 0.5500 - val_loss: 0.9171 - val_acc: 0.4667\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.8100 - acc: 0.5500 - val_loss: 0.9027 - val_acc: 0.4667\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7980 - acc: 0.5500 - val_loss: 0.8884 - val_acc: 0.4667\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.7867 - acc: 0.5500 - val_loss: 0.8758 - val_acc: 0.4667\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.7763 - acc: 0.5500 - val_loss: 0.8638 - val_acc: 0.4667\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.7669 - acc: 0.5500 - val_loss: 0.8532 - val_acc: 0.4667\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.7572 - acc: 0.5500 - val_loss: 0.8414 - val_acc: 0.4667\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.7476 - acc: 0.5500 - val_loss: 0.8303 - val_acc: 0.4667\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.7377 - acc: 0.5500 - val_loss: 0.8176 - val_acc: 0.4667\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.7280 - acc: 0.5500 - val_loss: 0.8069 - val_acc: 0.4667\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.7187 - acc: 0.5500 - val_loss: 0.7955 - val_acc: 0.4667\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.7093 - acc: 0.5500 - val_loss: 0.7842 - val_acc: 0.4667\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.7000 - acc: 0.5500 - val_loss: 0.7731 - val_acc: 0.4667\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6911 - acc: 0.5500 - val_loss: 0.7625 - val_acc: 0.4667\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6826 - acc: 0.5500 - val_loss: 0.7525 - val_acc: 0.4667\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6747 - acc: 0.5500 - val_loss: 0.7431 - val_acc: 0.4667\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6669 - acc: 0.5500 - val_loss: 0.7338 - val_acc: 0.4667\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6597 - acc: 0.5500 - val_loss: 0.7254 - val_acc: 0.4667\n",
      "Test accuracy: 0.4666666626930237\n",
      "Evalutation of best performing model:\n",
      "60/60 [==============================] - 0s 17us/step\n",
      "[0.2943368494510651, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    Make sure to have every relevant import statement included here and return data as\n",
    "    used in model function below. This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    url = \"../data/iris.csv\"\n",
    "    data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "    class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "    data.iloc[:,-1] = index\n",
    "    data = data.loc[data[4] != 2]\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=4, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer={{choice(['adam', 'nadam'])}},\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([10, 30])}},\n",
    "              epochs={{choice([100, 170])}},\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='experiment')\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hyperas(Linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils.validation import column_or_1d\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import copy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import datasets\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from automation_script import get_dataset_info\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.datasets import make_regression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano.tensor as T\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from theano.compile.ops import as_op\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [10, 30]),\n",
      "        'epochs': hp.choice('epochs', [100, 170]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: Make sure to have every relevant import statement included here and return data as\n",
      "  6: used in model function below. This function is separated from model() so that hyperopt\n",
      "  7: won't reload data for each evaluation run.\n",
      "  8: '''\n",
      "  9: from os import path\n",
      " 10: import pandas as pd\n",
      " 11: from sklearn import preprocessing\n",
      " 12: from sklearn.preprocessing import StandardScaler\n",
      " 13: \n",
      " 14: url = \"../data/diabetes.csv\"\n",
      " 15: data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
      " 16: sc = StandardScaler()\n",
      " 17: data = sc.fit_transform(data)\n",
      " 18: data = pd.DataFrame(data)\n",
      " 19: \n",
      " 20: \n",
      " 21: X = data.iloc[:,:-1]\n",
      " 22: Y = data.iloc[:,-1]\n",
      " 23: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
      " 24: \n",
      " 25: \n",
      " 26: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13: \n",
      "  14:     model = Sequential()\n",
      "  15:     model.add(Dense(1, input_dim=10, activation='linear'))\n",
      "  16: \n",
      "  17:     model.compile(loss='mse', optimizer=space['optimizer'])\n",
      "  18: \n",
      "  19:     model.fit(x_train, y_train,\n",
      "  20:               batch_size=space['batch_size'],\n",
      "  21:               epochs=space['epochs'],\n",
      "  22:               verbose=2,\n",
      "  23:               validation_data=(x_test, y_test))\n",
      "  24:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  25:     print('Test accuracy:', acc)\n",
      "  26:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  27: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 1.6659 - acc: 0.0000e+00 - val_loss: 1.5142 - val_acc: 0.0000e+00\n",
      "Epoch 2/170\n",
      " - 0s - loss: 1.6094 - acc: 0.0000e+00 - val_loss: 1.4674 - val_acc: 0.0000e+00\n",
      "Epoch 3/170\n",
      " - 0s - loss: 1.5604 - acc: 0.0000e+00 - val_loss: 1.4252 - val_acc: 0.0000e+00\n",
      "Epoch 4/170\n",
      " - 0s - loss: 1.5204 - acc: 0.0000e+00 - val_loss: 1.3813 - val_acc: 0.0000e+00\n",
      "Epoch 5/170\n",
      " - 0s - loss: 1.4753 - acc: 0.0000e+00 - val_loss: 1.3462 - val_acc: 0.0000e+00\n",
      "Epoch 6/170\n",
      " - 0s - loss: 1.4360 - acc: 0.0000e+00 - val_loss: 1.3165 - val_acc: 0.0000e+00\n",
      "Epoch 7/170\n",
      " - 0s - loss: 1.3996 - acc: 0.0000e+00 - val_loss: 1.2852 - val_acc: 0.0000e+00\n",
      "Epoch 8/170\n",
      " - 0s - loss: 1.3632 - acc: 0.0000e+00 - val_loss: 1.2518 - val_acc: 0.0000e+00\n",
      "Epoch 9/170\n",
      " - 0s - loss: 1.3302 - acc: 0.0000e+00 - val_loss: 1.2223 - val_acc: 0.0000e+00\n",
      "Epoch 10/170\n",
      " - 0s - loss: 1.2957 - acc: 0.0000e+00 - val_loss: 1.1963 - val_acc: 0.0000e+00\n",
      "Epoch 11/170\n",
      " - 0s - loss: 1.2650 - acc: 0.0000e+00 - val_loss: 1.1693 - val_acc: 0.0000e+00\n",
      "Epoch 12/170\n",
      " - 0s - loss: 1.2356 - acc: 0.0000e+00 - val_loss: 1.1414 - val_acc: 0.0000e+00\n",
      "Epoch 13/170\n",
      " - 0s - loss: 1.2079 - acc: 0.0000e+00 - val_loss: 1.1171 - val_acc: 0.0000e+00\n",
      "Epoch 14/170\n",
      " - 0s - loss: 1.1778 - acc: 0.0000e+00 - val_loss: 1.0964 - val_acc: 0.0000e+00\n",
      "Epoch 15/170\n",
      " - 0s - loss: 1.1536 - acc: 0.0000e+00 - val_loss: 1.0736 - val_acc: 0.0000e+00\n",
      "Epoch 16/170\n",
      " - 0s - loss: 1.1273 - acc: 0.0000e+00 - val_loss: 1.0521 - val_acc: 0.0000e+00\n",
      "Epoch 17/170\n",
      " - 0s - loss: 1.1030 - acc: 0.0000e+00 - val_loss: 1.0333 - val_acc: 0.0000e+00\n",
      "Epoch 18/170\n",
      " - 0s - loss: 1.0797 - acc: 0.0000e+00 - val_loss: 1.0138 - val_acc: 0.0000e+00\n",
      "Epoch 19/170\n",
      " - 0s - loss: 1.0573 - acc: 0.0000e+00 - val_loss: 0.9942 - val_acc: 0.0000e+00\n",
      "Epoch 20/170\n",
      " - 0s - loss: 1.0365 - acc: 0.0000e+00 - val_loss: 0.9776 - val_acc: 0.0000e+00\n",
      "Epoch 21/170\n",
      " - 0s - loss: 1.0154 - acc: 0.0000e+00 - val_loss: 0.9591 - val_acc: 0.0000e+00\n",
      "Epoch 22/170\n",
      " - 0s - loss: 0.9958 - acc: 0.0000e+00 - val_loss: 0.9389 - val_acc: 0.0000e+00\n",
      "Epoch 23/170\n",
      " - 0s - loss: 0.9760 - acc: 0.0000e+00 - val_loss: 0.9232 - val_acc: 0.0000e+00\n",
      "Epoch 24/170\n",
      " - 0s - loss: 0.9569 - acc: 0.0000e+00 - val_loss: 0.9071 - val_acc: 0.0000e+00\n",
      "Epoch 25/170\n",
      " - 0s - loss: 0.9396 - acc: 0.0000e+00 - val_loss: 0.8925 - val_acc: 0.0000e+00\n",
      "Epoch 26/170\n",
      " - 0s - loss: 0.9217 - acc: 0.0000e+00 - val_loss: 0.8776 - val_acc: 0.0000e+00\n",
      "Epoch 27/170\n",
      " - 0s - loss: 0.9045 - acc: 0.0000e+00 - val_loss: 0.8651 - val_acc: 0.0000e+00\n",
      "Epoch 28/170\n",
      " - 0s - loss: 0.8893 - acc: 0.0000e+00 - val_loss: 0.8498 - val_acc: 0.0000e+00\n",
      "Epoch 29/170\n",
      " - 0s - loss: 0.8741 - acc: 0.0000e+00 - val_loss: 0.8364 - val_acc: 0.0000e+00\n",
      "Epoch 30/170\n",
      " - 0s - loss: 0.8589 - acc: 0.0000e+00 - val_loss: 0.8254 - val_acc: 0.0000e+00\n",
      "Epoch 31/170\n",
      " - 0s - loss: 0.8440 - acc: 0.0000e+00 - val_loss: 0.8124 - val_acc: 0.0000e+00\n",
      "Epoch 32/170\n",
      " - 0s - loss: 0.8306 - acc: 0.0000e+00 - val_loss: 0.7989 - val_acc: 0.0000e+00\n",
      "Epoch 33/170\n",
      " - 0s - loss: 0.8164 - acc: 0.0000e+00 - val_loss: 0.7878 - val_acc: 0.0000e+00\n",
      "Epoch 34/170\n",
      " - 0s - loss: 0.8037 - acc: 0.0000e+00 - val_loss: 0.7759 - val_acc: 0.0000e+00\n",
      "Epoch 35/170\n",
      " - 0s - loss: 0.7916 - acc: 0.0000e+00 - val_loss: 0.7653 - val_acc: 0.0000e+00\n",
      "Epoch 36/170\n",
      " - 0s - loss: 0.7790 - acc: 0.0000e+00 - val_loss: 0.7573 - val_acc: 0.0000e+00\n",
      "Epoch 37/170\n",
      " - 0s - loss: 0.7675 - acc: 0.0000e+00 - val_loss: 0.7474 - val_acc: 0.0000e+00\n",
      "Epoch 38/170\n",
      " - 0s - loss: 0.7564 - acc: 0.0000e+00 - val_loss: 0.7361 - val_acc: 0.0000e+00\n",
      "Epoch 39/170\n",
      " - 0s - loss: 0.7450 - acc: 0.0000e+00 - val_loss: 0.7267 - val_acc: 0.0000e+00\n",
      "Epoch 40/170\n",
      " - 0s - loss: 0.7349 - acc: 0.0000e+00 - val_loss: 0.7191 - val_acc: 0.0000e+00\n",
      "Epoch 41/170\n",
      " - 0s - loss: 0.7244 - acc: 0.0000e+00 - val_loss: 0.7100 - val_acc: 0.0000e+00\n",
      "Epoch 42/170\n",
      " - 0s - loss: 0.7147 - acc: 0.0000e+00 - val_loss: 0.7014 - val_acc: 0.0000e+00\n",
      "Epoch 43/170\n",
      " - 0s - loss: 0.7063 - acc: 0.0000e+00 - val_loss: 0.6924 - val_acc: 0.0000e+00\n",
      "Epoch 44/170\n",
      " - 0s - loss: 0.6958 - acc: 0.0000e+00 - val_loss: 0.6877 - val_acc: 0.0000e+00\n",
      "Epoch 45/170\n",
      " - 0s - loss: 0.6875 - acc: 0.0000e+00 - val_loss: 0.6810 - val_acc: 0.0000e+00\n",
      "Epoch 46/170\n",
      " - 0s - loss: 0.6791 - acc: 0.0000e+00 - val_loss: 0.6740 - val_acc: 0.0000e+00\n",
      "Epoch 47/170\n",
      " - 0s - loss: 0.6702 - acc: 0.0000e+00 - val_loss: 0.6662 - val_acc: 0.0000e+00\n",
      "Epoch 48/170\n",
      " - 0s - loss: 0.6623 - acc: 0.0000e+00 - val_loss: 0.6592 - val_acc: 0.0000e+00\n",
      "Epoch 49/170\n",
      " - 0s - loss: 0.6549 - acc: 0.0000e+00 - val_loss: 0.6542 - val_acc: 0.0000e+00\n",
      "Epoch 50/170\n",
      " - 0s - loss: 0.6475 - acc: 0.0000e+00 - val_loss: 0.6479 - val_acc: 0.0000e+00\n",
      "Epoch 51/170\n",
      " - 0s - loss: 0.6406 - acc: 0.0000e+00 - val_loss: 0.6424 - val_acc: 0.0000e+00\n",
      "Epoch 52/170\n",
      " - 0s - loss: 0.6333 - acc: 0.0000e+00 - val_loss: 0.6384 - val_acc: 0.0000e+00\n",
      "Epoch 53/170\n",
      " - 0s - loss: 0.6274 - acc: 0.0000e+00 - val_loss: 0.6336 - val_acc: 0.0000e+00\n",
      "Epoch 54/170\n",
      " - 0s - loss: 0.6213 - acc: 0.0000e+00 - val_loss: 0.6271 - val_acc: 0.0000e+00\n",
      "Epoch 55/170\n",
      " - 0s - loss: 0.6146 - acc: 0.0000e+00 - val_loss: 0.6228 - val_acc: 0.0000e+00\n",
      "Epoch 56/170\n",
      " - 0s - loss: 0.6087 - acc: 0.0000e+00 - val_loss: 0.6168 - val_acc: 0.0000e+00\n",
      "Epoch 57/170\n",
      " - 0s - loss: 0.6027 - acc: 0.0000e+00 - val_loss: 0.6127 - val_acc: 0.0000e+00\n",
      "Epoch 58/170\n",
      " - 0s - loss: 0.5972 - acc: 0.0000e+00 - val_loss: 0.6092 - val_acc: 0.0000e+00\n",
      "Epoch 59/170\n",
      " - 0s - loss: 0.5920 - acc: 0.0000e+00 - val_loss: 0.6043 - val_acc: 0.0000e+00\n",
      "Epoch 60/170\n",
      " - 0s - loss: 0.5866 - acc: 0.0000e+00 - val_loss: 0.6005 - val_acc: 0.0000e+00\n",
      "Epoch 61/170\n",
      " - 0s - loss: 0.5813 - acc: 0.0000e+00 - val_loss: 0.5970 - val_acc: 0.0000e+00\n",
      "Epoch 62/170\n",
      " - 0s - loss: 0.5769 - acc: 0.0000e+00 - val_loss: 0.5940 - val_acc: 0.0000e+00\n",
      "Epoch 63/170\n",
      " - 0s - loss: 0.5719 - acc: 0.0000e+00 - val_loss: 0.5907 - val_acc: 0.0000e+00\n",
      "Epoch 64/170\n",
      " - 0s - loss: 0.5675 - acc: 0.0000e+00 - val_loss: 0.5877 - val_acc: 0.0000e+00\n",
      "Epoch 65/170\n",
      " - 0s - loss: 0.5627 - acc: 0.0000e+00 - val_loss: 0.5847 - val_acc: 0.0000e+00\n",
      "Epoch 66/170\n",
      " - 0s - loss: 0.5592 - acc: 0.0000e+00 - val_loss: 0.5815 - val_acc: 0.0000e+00\n",
      "Epoch 67/170\n",
      " - 0s - loss: 0.5546 - acc: 0.0000e+00 - val_loss: 0.5796 - val_acc: 0.0000e+00\n",
      "Epoch 68/170\n",
      " - 0s - loss: 0.5510 - acc: 0.0000e+00 - val_loss: 0.5770 - val_acc: 0.0000e+00\n",
      "Epoch 69/170\n",
      " - 0s - loss: 0.5473 - acc: 0.0000e+00 - val_loss: 0.5758 - val_acc: 0.0000e+00\n",
      "Epoch 70/170\n",
      " - 0s - loss: 0.5439 - acc: 0.0000e+00 - val_loss: 0.5723 - val_acc: 0.0000e+00\n",
      "Epoch 71/170\n",
      " - 0s - loss: 0.5394 - acc: 0.0000e+00 - val_loss: 0.5701 - val_acc: 0.0000e+00\n",
      "Epoch 72/170\n",
      " - 0s - loss: 0.5368 - acc: 0.0000e+00 - val_loss: 0.5685 - val_acc: 0.0000e+00\n",
      "Epoch 73/170\n",
      " - 0s - loss: 0.5329 - acc: 0.0000e+00 - val_loss: 0.5669 - val_acc: 0.0000e+00\n",
      "Epoch 74/170\n",
      " - 0s - loss: 0.5301 - acc: 0.0000e+00 - val_loss: 0.5640 - val_acc: 0.0000e+00\n",
      "Epoch 75/170\n",
      " - 0s - loss: 0.5271 - acc: 0.0000e+00 - val_loss: 0.5629 - val_acc: 0.0000e+00\n",
      "Epoch 76/170\n",
      " - 0s - loss: 0.5238 - acc: 0.0000e+00 - val_loss: 0.5614 - val_acc: 0.0000e+00\n",
      "Epoch 77/170\n",
      " - 0s - loss: 0.5214 - acc: 0.0000e+00 - val_loss: 0.5586 - val_acc: 0.0000e+00\n",
      "Epoch 78/170\n",
      " - 0s - loss: 0.5193 - acc: 0.0000e+00 - val_loss: 0.5578 - val_acc: 0.0000e+00\n",
      "Epoch 79/170\n",
      " - 0s - loss: 0.5163 - acc: 0.0000e+00 - val_loss: 0.5559 - val_acc: 0.0000e+00\n",
      "Epoch 80/170\n",
      " - 0s - loss: 0.5134 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 81/170\n",
      " - 0s - loss: 0.5112 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 82/170\n",
      " - 0s - loss: 0.5083 - acc: 0.0000e+00 - val_loss: 0.5533 - val_acc: 0.0000e+00\n",
      "Epoch 83/170\n",
      " - 0s - loss: 0.5062 - acc: 0.0000e+00 - val_loss: 0.5522 - val_acc: 0.0000e+00\n",
      "Epoch 84/170\n",
      " - 0s - loss: 0.5039 - acc: 0.0000e+00 - val_loss: 0.5512 - val_acc: 0.0000e+00\n",
      "Epoch 85/170\n",
      " - 0s - loss: 0.5019 - acc: 0.0000e+00 - val_loss: 0.5497 - val_acc: 0.0000e+00\n",
      "Epoch 86/170\n",
      " - 0s - loss: 0.5004 - acc: 0.0000e+00 - val_loss: 0.5487 - val_acc: 0.0000e+00\n",
      "Epoch 87/170\n",
      " - 0s - loss: 0.4983 - acc: 0.0000e+00 - val_loss: 0.5486 - val_acc: 0.0000e+00\n",
      "Epoch 88/170\n",
      " - 0s - loss: 0.4968 - acc: 0.0000e+00 - val_loss: 0.5480 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/170\n",
      " - 0s - loss: 0.4939 - acc: 0.0000e+00 - val_loss: 0.5473 - val_acc: 0.0000e+00\n",
      "Epoch 90/170\n",
      " - 0s - loss: 0.4932 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n",
      "Epoch 91/170\n",
      " - 0s - loss: 0.4908 - acc: 0.0000e+00 - val_loss: 0.5469 - val_acc: 0.0000e+00\n",
      "Epoch 92/170\n",
      " - 0s - loss: 0.4895 - acc: 0.0000e+00 - val_loss: 0.5461 - val_acc: 0.0000e+00\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.4880 - acc: 0.0000e+00 - val_loss: 0.5459 - val_acc: 0.0000e+00\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.4863 - acc: 0.0000e+00 - val_loss: 0.5453 - val_acc: 0.0000e+00\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.4846 - acc: 0.0000e+00 - val_loss: 0.5446 - val_acc: 0.0000e+00\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.4838 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 97/170\n",
      " - 0s - loss: 0.4822 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.4813 - acc: 0.0000e+00 - val_loss: 0.5443 - val_acc: 0.0000e+00\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.4800 - acc: 0.0000e+00 - val_loss: 0.5433 - val_acc: 0.0000e+00\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.4791 - acc: 0.0000e+00 - val_loss: 0.5433 - val_acc: 0.0000e+00\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.4773 - acc: 0.0000e+00 - val_loss: 0.5428 - val_acc: 0.0000e+00\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.4764 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.4756 - acc: 0.0000e+00 - val_loss: 0.5430 - val_acc: 0.0000e+00\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.4742 - acc: 0.0000e+00 - val_loss: 0.5425 - val_acc: 0.0000e+00\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.4735 - acc: 0.0000e+00 - val_loss: 0.5430 - val_acc: 0.0000e+00\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.4726 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.4712 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.4709 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.4697 - acc: 0.0000e+00 - val_loss: 0.5435 - val_acc: 0.0000e+00\n",
      "Epoch 110/170\n",
      " - 0s - loss: 0.4694 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.4686 - acc: 0.0000e+00 - val_loss: 0.5434 - val_acc: 0.0000e+00\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.4674 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.4672 - acc: 0.0000e+00 - val_loss: 0.5439 - val_acc: 0.0000e+00\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.4662 - acc: 0.0000e+00 - val_loss: 0.5437 - val_acc: 0.0000e+00\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.4656 - acc: 0.0000e+00 - val_loss: 0.5443 - val_acc: 0.0000e+00\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.4650 - acc: 0.0000e+00 - val_loss: 0.5446 - val_acc: 0.0000e+00\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.4653 - acc: 0.0000e+00 - val_loss: 0.5448 - val_acc: 0.0000e+00\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.4638 - acc: 0.0000e+00 - val_loss: 0.5456 - val_acc: 0.0000e+00\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.4636 - acc: 0.0000e+00 - val_loss: 0.5446 - val_acc: 0.0000e+00\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.4627 - acc: 0.0000e+00 - val_loss: 0.5452 - val_acc: 0.0000e+00\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.4624 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.4623 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.4617 - acc: 0.0000e+00 - val_loss: 0.5464 - val_acc: 0.0000e+00\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.4611 - acc: 0.0000e+00 - val_loss: 0.5469 - val_acc: 0.0000e+00\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.4607 - acc: 0.0000e+00 - val_loss: 0.5468 - val_acc: 0.0000e+00\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.4604 - acc: 0.0000e+00 - val_loss: 0.5479 - val_acc: 0.0000e+00\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.4604 - acc: 0.0000e+00 - val_loss: 0.5478 - val_acc: 0.0000e+00\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.4600 - acc: 0.0000e+00 - val_loss: 0.5489 - val_acc: 0.0000e+00\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.4588 - acc: 0.0000e+00 - val_loss: 0.5486 - val_acc: 0.0000e+00\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.4594 - acc: 0.0000e+00 - val_loss: 0.5487 - val_acc: 0.0000e+00\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.4581 - acc: 0.0000e+00 - val_loss: 0.5495 - val_acc: 0.0000e+00\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.4577 - acc: 0.0000e+00 - val_loss: 0.5498 - val_acc: 0.0000e+00\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.4574 - acc: 0.0000e+00 - val_loss: 0.5498 - val_acc: 0.0000e+00\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.4573 - acc: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.0000e+00\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.4573 - acc: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.0000e+00\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.4568 - acc: 0.0000e+00 - val_loss: 0.5508 - val_acc: 0.0000e+00\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.4572 - acc: 0.0000e+00 - val_loss: 0.5513 - val_acc: 0.0000e+00\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.4562 - acc: 0.0000e+00 - val_loss: 0.5517 - val_acc: 0.0000e+00\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5520 - val_acc: 0.0000e+00\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.4556 - acc: 0.0000e+00 - val_loss: 0.5526 - val_acc: 0.0000e+00\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.4564 - acc: 0.0000e+00 - val_loss: 0.5520 - val_acc: 0.0000e+00\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.4551 - acc: 0.0000e+00 - val_loss: 0.5530 - val_acc: 0.0000e+00\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.4554 - acc: 0.0000e+00 - val_loss: 0.5533 - val_acc: 0.0000e+00\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.4547 - acc: 0.0000e+00 - val_loss: 0.5537 - val_acc: 0.0000e+00\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.4549 - acc: 0.0000e+00 - val_loss: 0.5536 - val_acc: 0.0000e+00\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.4546 - acc: 0.0000e+00 - val_loss: 0.5544 - val_acc: 0.0000e+00\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.4543 - acc: 0.0000e+00 - val_loss: 0.5543 - val_acc: 0.0000e+00\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.4540 - acc: 0.0000e+00 - val_loss: 0.5550 - val_acc: 0.0000e+00\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.4538 - acc: 0.0000e+00 - val_loss: 0.5551 - val_acc: 0.0000e+00\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.4534 - acc: 0.0000e+00 - val_loss: 0.5549 - val_acc: 0.0000e+00\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5556 - val_acc: 0.0000e+00\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.4537 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.4532 - acc: 0.0000e+00 - val_loss: 0.5564 - val_acc: 0.0000e+00\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.4530 - acc: 0.0000e+00 - val_loss: 0.5563 - val_acc: 0.0000e+00\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.4528 - acc: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.0000e+00\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.4527 - acc: 0.0000e+00 - val_loss: 0.5571 - val_acc: 0.0000e+00\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.4527 - acc: 0.0000e+00 - val_loss: 0.5576 - val_acc: 0.0000e+00\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.4531 - acc: 0.0000e+00 - val_loss: 0.5575 - val_acc: 0.0000e+00\n",
      "Epoch 159/170\n",
      " - 0s - loss: 0.4524 - acc: 0.0000e+00 - val_loss: 0.5576 - val_acc: 0.0000e+00\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.4525 - acc: 0.0000e+00 - val_loss: 0.5582 - val_acc: 0.0000e+00\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.4524 - acc: 0.0000e+00 - val_loss: 0.5587 - val_acc: 0.0000e+00\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5590 - val_acc: 0.0000e+00\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.4522 - acc: 0.0000e+00 - val_loss: 0.5585 - val_acc: 0.0000e+00\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.4525 - acc: 0.0000e+00 - val_loss: 0.5586 - val_acc: 0.0000e+00\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.4520 - acc: 0.0000e+00 - val_loss: 0.5595 - val_acc: 0.0000e+00\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.4516 - acc: 0.0000e+00 - val_loss: 0.5607 - val_acc: 0.0000e+00\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.4520 - acc: 0.0000e+00 - val_loss: 0.5597 - val_acc: 0.0000e+00\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5604 - val_acc: 0.0000e+00\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.4513 - acc: 0.0000e+00 - val_loss: 0.5608 - val_acc: 0.0000e+00\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.4514 - acc: 0.0000e+00 - val_loss: 0.5606 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 2.8030 - acc: 0.0000e+00 - val_loss: 2.1154 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.7350 - acc: 0.0000e+00 - val_loss: 2.0676 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 2.6646 - acc: 0.0000e+00 - val_loss: 2.0222 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 2.6021 - acc: 0.0000e+00 - val_loss: 1.9775 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.5360 - acc: 0.0000e+00 - val_loss: 1.9353 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      " - 0s - loss: 2.4758 - acc: 0.0000e+00 - val_loss: 1.8944 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.4168 - acc: 0.0000e+00 - val_loss: 1.8551 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 2.3611 - acc: 0.0000e+00 - val_loss: 1.8173 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 2.3044 - acc: 0.0000e+00 - val_loss: 1.7816 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 2.2538 - acc: 0.0000e+00 - val_loss: 1.7467 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 2.2037 - acc: 0.0000e+00 - val_loss: 1.7129 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 2.1562 - acc: 0.0000e+00 - val_loss: 1.6803 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 2.1104 - acc: 0.0000e+00 - val_loss: 1.6489 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 2.0637 - acc: 0.0000e+00 - val_loss: 1.6192 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 2.0216 - acc: 0.0000e+00 - val_loss: 1.5903 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 1.9791 - acc: 0.0000e+00 - val_loss: 1.5629 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 1.9408 - acc: 0.0000e+00 - val_loss: 1.5359 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 1.9026 - acc: 0.0000e+00 - val_loss: 1.5097 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 1.8641 - acc: 0.0000e+00 - val_loss: 1.4847 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 1.8284 - acc: 0.0000e+00 - val_loss: 1.4602 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.7940 - acc: 0.0000e+00 - val_loss: 1.4367 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.7600 - acc: 0.0000e+00 - val_loss: 1.4140 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.7283 - acc: 0.0000e+00 - val_loss: 1.3919 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.6965 - acc: 0.0000e+00 - val_loss: 1.3706 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.6650 - acc: 0.0000e+00 - val_loss: 1.3502 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.6369 - acc: 0.0000e+00 - val_loss: 1.3297 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.6074 - acc: 0.0000e+00 - val_loss: 1.3102 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.5803 - acc: 0.0000e+00 - val_loss: 1.2910 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.5517 - acc: 0.0000e+00 - val_loss: 1.2730 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.5260 - acc: 0.0000e+00 - val_loss: 1.2553 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.5021 - acc: 0.0000e+00 - val_loss: 1.2372 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.4757 - acc: 0.0000e+00 - val_loss: 1.2205 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 1.4530 - acc: 0.0000e+00 - val_loss: 1.2035 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 1.4278 - acc: 0.0000e+00 - val_loss: 1.1875 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 1.4066 - acc: 0.0000e+00 - val_loss: 1.1712 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.3831 - acc: 0.0000e+00 - val_loss: 1.1558 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.3615 - acc: 0.0000e+00 - val_loss: 1.1407 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.3401 - acc: 0.0000e+00 - val_loss: 1.1260 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.3203 - acc: 0.0000e+00 - val_loss: 1.1116 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.2988 - acc: 0.0000e+00 - val_loss: 1.0979 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.2791 - acc: 0.0000e+00 - val_loss: 1.0845 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 1.2606 - acc: 0.0000e+00 - val_loss: 1.0711 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.2420 - acc: 0.0000e+00 - val_loss: 1.0582 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.2234 - acc: 0.0000e+00 - val_loss: 1.0457 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.2054 - acc: 0.0000e+00 - val_loss: 1.0335 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 1.1883 - acc: 0.0000e+00 - val_loss: 1.0212 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 1.1713 - acc: 0.0000e+00 - val_loss: 1.0095 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 1.1542 - acc: 0.0000e+00 - val_loss: 0.9979 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 1.1377 - acc: 0.0000e+00 - val_loss: 0.9866 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 1.1219 - acc: 0.0000e+00 - val_loss: 0.9756 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 1.1059 - acc: 0.0000e+00 - val_loss: 0.9649 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 1.0911 - acc: 0.0000e+00 - val_loss: 0.9542 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 1.0765 - acc: 0.0000e+00 - val_loss: 0.9437 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 1.0609 - acc: 0.0000e+00 - val_loss: 0.9340 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 1.0476 - acc: 0.0000e+00 - val_loss: 0.9241 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 1.0332 - acc: 0.0000e+00 - val_loss: 0.9146 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 1.0202 - acc: 0.0000e+00 - val_loss: 0.9051 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 1.0069 - acc: 0.0000e+00 - val_loss: 0.8962 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.9937 - acc: 0.0000e+00 - val_loss: 0.8874 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.9803 - acc: 0.0000e+00 - val_loss: 0.8789 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.9683 - acc: 0.0000e+00 - val_loss: 0.8706 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.9562 - acc: 0.0000e+00 - val_loss: 0.8625 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.9448 - acc: 0.0000e+00 - val_loss: 0.8543 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.9329 - acc: 0.0000e+00 - val_loss: 0.8466 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.9219 - acc: 0.0000e+00 - val_loss: 0.8388 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.9097 - acc: 0.0000e+00 - val_loss: 0.8318 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.8995 - acc: 0.0000e+00 - val_loss: 0.8248 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.8899 - acc: 0.0000e+00 - val_loss: 0.8175 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.8786 - acc: 0.0000e+00 - val_loss: 0.8106 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.8684 - acc: 0.0000e+00 - val_loss: 0.8039 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.8590 - acc: 0.0000e+00 - val_loss: 0.7974 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.8493 - acc: 0.0000e+00 - val_loss: 0.7911 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.8394 - acc: 0.0000e+00 - val_loss: 0.7850 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.8304 - acc: 0.0000e+00 - val_loss: 0.7789 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.8212 - acc: 0.0000e+00 - val_loss: 0.7732 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.8126 - acc: 0.0000e+00 - val_loss: 0.7675 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.8035 - acc: 0.0000e+00 - val_loss: 0.7620 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.7949 - acc: 0.0000e+00 - val_loss: 0.7566 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.7867 - acc: 0.0000e+00 - val_loss: 0.7515 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.7795 - acc: 0.0000e+00 - val_loss: 0.7461 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.7708 - acc: 0.0000e+00 - val_loss: 0.7411 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.7635 - acc: 0.0000e+00 - val_loss: 0.7362 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.7555 - acc: 0.0000e+00 - val_loss: 0.7315 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.7487 - acc: 0.0000e+00 - val_loss: 0.7268 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.7410 - acc: 0.0000e+00 - val_loss: 0.7224 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.7338 - acc: 0.0000e+00 - val_loss: 0.7182 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.7271 - acc: 0.0000e+00 - val_loss: 0.7139 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.7208 - acc: 0.0000e+00 - val_loss: 0.7096 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.7134 - acc: 0.0000e+00 - val_loss: 0.7057 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.7073 - acc: 0.0000e+00 - val_loss: 0.7018 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.7008 - acc: 0.0000e+00 - val_loss: 0.6980 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.6951 - acc: 0.0000e+00 - val_loss: 0.6944 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.6889 - acc: 0.0000e+00 - val_loss: 0.6907 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.6830 - acc: 0.0000e+00 - val_loss: 0.6874 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      " - 0s - loss: 0.6772 - acc: 0.0000e+00 - val_loss: 0.6841 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.6718 - acc: 0.0000e+00 - val_loss: 0.6807 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.6665 - acc: 0.0000e+00 - val_loss: 0.6776 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.6608 - acc: 0.0000e+00 - val_loss: 0.6747 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.6558 - acc: 0.0000e+00 - val_loss: 0.6716 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.6508 - acc: 0.0000e+00 - val_loss: 0.6686 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 5.3148 - acc: 0.0000e+00 - val_loss: 3.6639 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.5647 - acc: 0.0000e+00 - val_loss: 3.1159 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 3.8448 - acc: 0.0000e+00 - val_loss: 2.6434 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.2392 - acc: 0.0000e+00 - val_loss: 2.2378 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.7215 - acc: 0.0000e+00 - val_loss: 1.9033 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.2965 - acc: 0.0000e+00 - val_loss: 1.6285 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 1.9373 - acc: 0.0000e+00 - val_loss: 1.4080 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 1.6528 - acc: 0.0000e+00 - val_loss: 1.2294 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 1.4198 - acc: 0.0000e+00 - val_loss: 1.0908 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 1.2332 - acc: 0.0000e+00 - val_loss: 0.9833 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 1.0887 - acc: 0.0000e+00 - val_loss: 0.8992 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.9715 - acc: 0.0000e+00 - val_loss: 0.8368 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.8836 - acc: 0.0000e+00 - val_loss: 0.7885 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.8119 - acc: 0.0000e+00 - val_loss: 0.7533 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.7584 - acc: 0.0000e+00 - val_loss: 0.7261 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.7163 - acc: 0.0000e+00 - val_loss: 0.7058 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.6830 - acc: 0.0000e+00 - val_loss: 0.6909 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.6583 - acc: 0.0000e+00 - val_loss: 0.6789 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.6366 - acc: 0.0000e+00 - val_loss: 0.6702 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.6204 - acc: 0.0000e+00 - val_loss: 0.6629 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.6063 - acc: 0.0000e+00 - val_loss: 0.6562 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5941 - acc: 0.0000e+00 - val_loss: 0.6508 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5840 - acc: 0.0000e+00 - val_loss: 0.6459 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5754 - acc: 0.0000e+00 - val_loss: 0.6416 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5679 - acc: 0.0000e+00 - val_loss: 0.6373 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5601 - acc: 0.0000e+00 - val_loss: 0.6332 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5532 - acc: 0.0000e+00 - val_loss: 0.6294 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5469 - acc: 0.0000e+00 - val_loss: 0.6251 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5417 - acc: 0.0000e+00 - val_loss: 0.6222 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5355 - acc: 0.0000e+00 - val_loss: 0.6188 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5306 - acc: 0.0000e+00 - val_loss: 0.6152 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5253 - acc: 0.0000e+00 - val_loss: 0.6121 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5211 - acc: 0.0000e+00 - val_loss: 0.6086 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5174 - acc: 0.0000e+00 - val_loss: 0.6057 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5129 - acc: 0.0000e+00 - val_loss: 0.6024 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.5092 - acc: 0.0000e+00 - val_loss: 0.6004 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.5058 - acc: 0.0000e+00 - val_loss: 0.5978 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.5030 - acc: 0.0000e+00 - val_loss: 0.5953 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.4991 - acc: 0.0000e+00 - val_loss: 0.5918 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4965 - acc: 0.0000e+00 - val_loss: 0.5892 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4935 - acc: 0.0000e+00 - val_loss: 0.5872 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4911 - acc: 0.0000e+00 - val_loss: 0.5854 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4884 - acc: 0.0000e+00 - val_loss: 0.5830 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4864 - acc: 0.0000e+00 - val_loss: 0.5816 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4844 - acc: 0.0000e+00 - val_loss: 0.5800 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.4823 - acc: 0.0000e+00 - val_loss: 0.5779 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4801 - acc: 0.0000e+00 - val_loss: 0.5761 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4785 - acc: 0.0000e+00 - val_loss: 0.5745 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4763 - acc: 0.0000e+00 - val_loss: 0.5727 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.5707 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.4737 - acc: 0.0000e+00 - val_loss: 0.5691 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.4722 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.4707 - acc: 0.0000e+00 - val_loss: 0.5670 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.4696 - acc: 0.0000e+00 - val_loss: 0.5656 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.4683 - acc: 0.0000e+00 - val_loss: 0.5644 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.4676 - acc: 0.0000e+00 - val_loss: 0.5630 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.4665 - acc: 0.0000e+00 - val_loss: 0.5622 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.4656 - acc: 0.0000e+00 - val_loss: 0.5610 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.4643 - acc: 0.0000e+00 - val_loss: 0.5593 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.4637 - acc: 0.0000e+00 - val_loss: 0.5579 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4627 - acc: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4620 - acc: 0.0000e+00 - val_loss: 0.5552 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4612 - acc: 0.0000e+00 - val_loss: 0.5550 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.4605 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.4596 - acc: 0.0000e+00 - val_loss: 0.5538 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.4602 - acc: 0.0000e+00 - val_loss: 0.5532 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.4583 - acc: 0.0000e+00 - val_loss: 0.5528 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.4586 - acc: 0.0000e+00 - val_loss: 0.5523 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.4575 - acc: 0.0000e+00 - val_loss: 0.5518 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.4570 - acc: 0.0000e+00 - val_loss: 0.5514 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.4568 - acc: 0.0000e+00 - val_loss: 0.5518 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5506 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.4556 - acc: 0.0000e+00 - val_loss: 0.5495 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.5493 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.5489 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.4536 - acc: 0.0000e+00 - val_loss: 0.5484 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.4543 - acc: 0.0000e+00 - val_loss: 0.5478 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.4531 - acc: 0.0000e+00 - val_loss: 0.5473 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5472 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.4532 - acc: 0.0000e+00 - val_loss: 0.5468 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5461 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.4518 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.4518 - acc: 0.0000e+00 - val_loss: 0.5462 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.4518 - acc: 0.0000e+00 - val_loss: 0.5463 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.4508 - acc: 0.0000e+00 - val_loss: 0.5458 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.4505 - acc: 0.0000e+00 - val_loss: 0.5452 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4501 - acc: 0.0000e+00 - val_loss: 0.5451 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.4499 - acc: 0.0000e+00 - val_loss: 0.5444 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4502 - acc: 0.0000e+00 - val_loss: 0.5440 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.4497 - acc: 0.0000e+00 - val_loss: 0.5447 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.4493 - acc: 0.0000e+00 - val_loss: 0.5445 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4490 - acc: 0.0000e+00 - val_loss: 0.5443 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4489 - acc: 0.0000e+00 - val_loss: 0.5444 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4488 - acc: 0.0000e+00 - val_loss: 0.5436 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4485 - acc: 0.0000e+00 - val_loss: 0.5435 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4480 - acc: 0.0000e+00 - val_loss: 0.5431 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4476 - acc: 0.0000e+00 - val_loss: 0.5423 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4478 - acc: 0.0000e+00 - val_loss: 0.5421 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.4477 - acc: 0.0000e+00 - val_loss: 0.5422 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/170\n",
      " - 0s - loss: 1.6608 - acc: 0.0000e+00 - val_loss: 1.1954 - val_acc: 0.0000e+00\n",
      "Epoch 2/170\n",
      " - 0s - loss: 1.6198 - acc: 0.0000e+00 - val_loss: 1.1673 - val_acc: 0.0000e+00\n",
      "Epoch 3/170\n",
      " - 0s - loss: 1.5783 - acc: 0.0000e+00 - val_loss: 1.1391 - val_acc: 0.0000e+00\n",
      "Epoch 4/170\n",
      " - 0s - loss: 1.5354 - acc: 0.0000e+00 - val_loss: 1.1111 - val_acc: 0.0000e+00\n",
      "Epoch 5/170\n",
      " - 0s - loss: 1.4917 - acc: 0.0000e+00 - val_loss: 1.0835 - val_acc: 0.0000e+00\n",
      "Epoch 6/170\n",
      " - 0s - loss: 1.4479 - acc: 0.0000e+00 - val_loss: 1.0568 - val_acc: 0.0000e+00\n",
      "Epoch 7/170\n",
      " - 0s - loss: 1.4080 - acc: 0.0000e+00 - val_loss: 1.0314 - val_acc: 0.0000e+00\n",
      "Epoch 8/170\n",
      " - 0s - loss: 1.3663 - acc: 0.0000e+00 - val_loss: 1.0075 - val_acc: 0.0000e+00\n",
      "Epoch 9/170\n",
      " - 0s - loss: 1.3285 - acc: 0.0000e+00 - val_loss: 0.9844 - val_acc: 0.0000e+00\n",
      "Epoch 10/170\n",
      " - 0s - loss: 1.2912 - acc: 0.0000e+00 - val_loss: 0.9626 - val_acc: 0.0000e+00\n",
      "Epoch 11/170\n",
      " - 0s - loss: 1.2558 - acc: 0.0000e+00 - val_loss: 0.9423 - val_acc: 0.0000e+00\n",
      "Epoch 12/170\n",
      " - 0s - loss: 1.2206 - acc: 0.0000e+00 - val_loss: 0.9232 - val_acc: 0.0000e+00\n",
      "Epoch 13/170\n",
      " - 0s - loss: 1.1898 - acc: 0.0000e+00 - val_loss: 0.9049 - val_acc: 0.0000e+00\n",
      "Epoch 14/170\n",
      " - 0s - loss: 1.1581 - acc: 0.0000e+00 - val_loss: 0.8876 - val_acc: 0.0000e+00\n",
      "Epoch 15/170\n",
      " - 0s - loss: 1.1269 - acc: 0.0000e+00 - val_loss: 0.8714 - val_acc: 0.0000e+00\n",
      "Epoch 16/170\n",
      " - 0s - loss: 1.0976 - acc: 0.0000e+00 - val_loss: 0.8561 - val_acc: 0.0000e+00\n",
      "Epoch 17/170\n",
      " - 0s - loss: 1.0703 - acc: 0.0000e+00 - val_loss: 0.8415 - val_acc: 0.0000e+00\n",
      "Epoch 18/170\n",
      " - 0s - loss: 1.0441 - acc: 0.0000e+00 - val_loss: 0.8275 - val_acc: 0.0000e+00\n",
      "Epoch 19/170\n",
      " - 0s - loss: 1.0197 - acc: 0.0000e+00 - val_loss: 0.8143 - val_acc: 0.0000e+00\n",
      "Epoch 20/170\n",
      " - 0s - loss: 0.9943 - acc: 0.0000e+00 - val_loss: 0.8021 - val_acc: 0.0000e+00\n",
      "Epoch 21/170\n",
      " - 0s - loss: 0.9717 - acc: 0.0000e+00 - val_loss: 0.7905 - val_acc: 0.0000e+00\n",
      "Epoch 22/170\n",
      " - 0s - loss: 0.9492 - acc: 0.0000e+00 - val_loss: 0.7796 - val_acc: 0.0000e+00\n",
      "Epoch 23/170\n",
      " - 0s - loss: 0.9283 - acc: 0.0000e+00 - val_loss: 0.7694 - val_acc: 0.0000e+00\n",
      "Epoch 24/170\n",
      " - 0s - loss: 0.9075 - acc: 0.0000e+00 - val_loss: 0.7597 - val_acc: 0.0000e+00\n",
      "Epoch 25/170\n",
      " - 0s - loss: 0.8876 - acc: 0.0000e+00 - val_loss: 0.7506 - val_acc: 0.0000e+00\n",
      "Epoch 26/170\n",
      " - 0s - loss: 0.8697 - acc: 0.0000e+00 - val_loss: 0.7418 - val_acc: 0.0000e+00\n",
      "Epoch 27/170\n",
      " - 0s - loss: 0.8518 - acc: 0.0000e+00 - val_loss: 0.7335 - val_acc: 0.0000e+00\n",
      "Epoch 28/170\n",
      " - 0s - loss: 0.8337 - acc: 0.0000e+00 - val_loss: 0.7258 - val_acc: 0.0000e+00\n",
      "Epoch 29/170\n",
      " - 0s - loss: 0.8178 - acc: 0.0000e+00 - val_loss: 0.7185 - val_acc: 0.0000e+00\n",
      "Epoch 30/170\n",
      " - 0s - loss: 0.8028 - acc: 0.0000e+00 - val_loss: 0.7116 - val_acc: 0.0000e+00\n",
      "Epoch 31/170\n",
      " - 0s - loss: 0.7874 - acc: 0.0000e+00 - val_loss: 0.7050 - val_acc: 0.0000e+00\n",
      "Epoch 32/170\n",
      " - 0s - loss: 0.7728 - acc: 0.0000e+00 - val_loss: 0.6988 - val_acc: 0.0000e+00\n",
      "Epoch 33/170\n",
      " - 0s - loss: 0.7598 - acc: 0.0000e+00 - val_loss: 0.6930 - val_acc: 0.0000e+00\n",
      "Epoch 34/170\n",
      " - 0s - loss: 0.7465 - acc: 0.0000e+00 - val_loss: 0.6876 - val_acc: 0.0000e+00\n",
      "Epoch 35/170\n",
      " - 0s - loss: 0.7338 - acc: 0.0000e+00 - val_loss: 0.6823 - val_acc: 0.0000e+00\n",
      "Epoch 36/170\n",
      " - 0s - loss: 0.7220 - acc: 0.0000e+00 - val_loss: 0.6771 - val_acc: 0.0000e+00\n",
      "Epoch 37/170\n",
      " - 0s - loss: 0.7107 - acc: 0.0000e+00 - val_loss: 0.6723 - val_acc: 0.0000e+00\n",
      "Epoch 38/170\n",
      " - 0s - loss: 0.6994 - acc: 0.0000e+00 - val_loss: 0.6679 - val_acc: 0.0000e+00\n",
      "Epoch 39/170\n",
      " - 0s - loss: 0.6901 - acc: 0.0000e+00 - val_loss: 0.6636 - val_acc: 0.0000e+00\n",
      "Epoch 40/170\n",
      " - 0s - loss: 0.6789 - acc: 0.0000e+00 - val_loss: 0.6594 - val_acc: 0.0000e+00\n",
      "Epoch 41/170\n",
      " - 0s - loss: 0.6700 - acc: 0.0000e+00 - val_loss: 0.6553 - val_acc: 0.0000e+00\n",
      "Epoch 42/170\n",
      " - 0s - loss: 0.6604 - acc: 0.0000e+00 - val_loss: 0.6515 - val_acc: 0.0000e+00\n",
      "Epoch 43/170\n",
      " - 0s - loss: 0.6513 - acc: 0.0000e+00 - val_loss: 0.6480 - val_acc: 0.0000e+00\n",
      "Epoch 44/170\n",
      " - 0s - loss: 0.6429 - acc: 0.0000e+00 - val_loss: 0.6446 - val_acc: 0.0000e+00\n",
      "Epoch 45/170\n",
      " - 0s - loss: 0.6353 - acc: 0.0000e+00 - val_loss: 0.6414 - val_acc: 0.0000e+00\n",
      "Epoch 46/170\n",
      " - 0s - loss: 0.6268 - acc: 0.0000e+00 - val_loss: 0.6383 - val_acc: 0.0000e+00\n",
      "Epoch 47/170\n",
      " - 0s - loss: 0.6202 - acc: 0.0000e+00 - val_loss: 0.6354 - val_acc: 0.0000e+00\n",
      "Epoch 48/170\n",
      " - 0s - loss: 0.6131 - acc: 0.0000e+00 - val_loss: 0.6325 - val_acc: 0.0000e+00\n",
      "Epoch 49/170\n",
      " - 0s - loss: 0.6055 - acc: 0.0000e+00 - val_loss: 0.6298 - val_acc: 0.0000e+00\n",
      "Epoch 50/170\n",
      " - 0s - loss: 0.5997 - acc: 0.0000e+00 - val_loss: 0.6273 - val_acc: 0.0000e+00\n",
      "Epoch 51/170\n",
      " - 0s - loss: 0.5930 - acc: 0.0000e+00 - val_loss: 0.6248 - val_acc: 0.0000e+00\n",
      "Epoch 52/170\n",
      " - 0s - loss: 0.5877 - acc: 0.0000e+00 - val_loss: 0.6224 - val_acc: 0.0000e+00\n",
      "Epoch 53/170\n",
      " - 0s - loss: 0.5817 - acc: 0.0000e+00 - val_loss: 0.6202 - val_acc: 0.0000e+00\n",
      "Epoch 54/170\n",
      " - 0s - loss: 0.5770 - acc: 0.0000e+00 - val_loss: 0.6180 - val_acc: 0.0000e+00\n",
      "Epoch 55/170\n",
      " - 0s - loss: 0.5716 - acc: 0.0000e+00 - val_loss: 0.6156 - val_acc: 0.0000e+00\n",
      "Epoch 56/170\n",
      " - 0s - loss: 0.5661 - acc: 0.0000e+00 - val_loss: 0.6134 - val_acc: 0.0000e+00\n",
      "Epoch 57/170\n",
      " - 0s - loss: 0.5611 - acc: 0.0000e+00 - val_loss: 0.6114 - val_acc: 0.0000e+00\n",
      "Epoch 58/170\n",
      " - 0s - loss: 0.5568 - acc: 0.0000e+00 - val_loss: 0.6094 - val_acc: 0.0000e+00\n",
      "Epoch 59/170\n",
      " - 0s - loss: 0.5529 - acc: 0.0000e+00 - val_loss: 0.6074 - val_acc: 0.0000e+00\n",
      "Epoch 60/170\n",
      " - 0s - loss: 0.5486 - acc: 0.0000e+00 - val_loss: 0.6057 - val_acc: 0.0000e+00\n",
      "Epoch 61/170\n",
      " - 0s - loss: 0.5443 - acc: 0.0000e+00 - val_loss: 0.6041 - val_acc: 0.0000e+00\n",
      "Epoch 62/170\n",
      " - 0s - loss: 0.5407 - acc: 0.0000e+00 - val_loss: 0.6025 - val_acc: 0.0000e+00\n",
      "Epoch 63/170\n",
      " - 0s - loss: 0.5369 - acc: 0.0000e+00 - val_loss: 0.6009 - val_acc: 0.0000e+00\n",
      "Epoch 64/170\n",
      " - 0s - loss: 0.5335 - acc: 0.0000e+00 - val_loss: 0.5993 - val_acc: 0.0000e+00\n",
      "Epoch 65/170\n",
      " - 0s - loss: 0.5300 - acc: 0.0000e+00 - val_loss: 0.5978 - val_acc: 0.0000e+00\n",
      "Epoch 66/170\n",
      " - 0s - loss: 0.5263 - acc: 0.0000e+00 - val_loss: 0.5965 - val_acc: 0.0000e+00\n",
      "Epoch 67/170\n",
      " - 0s - loss: 0.5231 - acc: 0.0000e+00 - val_loss: 0.5950 - val_acc: 0.0000e+00\n",
      "Epoch 68/170\n",
      " - 0s - loss: 0.5209 - acc: 0.0000e+00 - val_loss: 0.5934 - val_acc: 0.0000e+00\n",
      "Epoch 69/170\n",
      " - 0s - loss: 0.5179 - acc: 0.0000e+00 - val_loss: 0.5922 - val_acc: 0.0000e+00\n",
      "Epoch 70/170\n",
      " - 0s - loss: 0.5144 - acc: 0.0000e+00 - val_loss: 0.5908 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/170\n",
      " - 0s - loss: 0.5124 - acc: 0.0000e+00 - val_loss: 0.5897 - val_acc: 0.0000e+00\n",
      "Epoch 72/170\n",
      " - 0s - loss: 0.5095 - acc: 0.0000e+00 - val_loss: 0.5884 - val_acc: 0.0000e+00\n",
      "Epoch 73/170\n",
      " - 0s - loss: 0.5076 - acc: 0.0000e+00 - val_loss: 0.5872 - val_acc: 0.0000e+00\n",
      "Epoch 74/170\n",
      " - 0s - loss: 0.5050 - acc: 0.0000e+00 - val_loss: 0.5862 - val_acc: 0.0000e+00\n",
      "Epoch 75/170\n",
      " - 0s - loss: 0.5029 - acc: 0.0000e+00 - val_loss: 0.5850 - val_acc: 0.0000e+00\n",
      "Epoch 76/170\n",
      " - 0s - loss: 0.5011 - acc: 0.0000e+00 - val_loss: 0.5838 - val_acc: 0.0000e+00\n",
      "Epoch 77/170\n",
      " - 0s - loss: 0.4985 - acc: 0.0000e+00 - val_loss: 0.5829 - val_acc: 0.0000e+00\n",
      "Epoch 78/170\n",
      " - 0s - loss: 0.4973 - acc: 0.0000e+00 - val_loss: 0.5820 - val_acc: 0.0000e+00\n",
      "Epoch 79/170\n",
      " - 0s - loss: 0.4950 - acc: 0.0000e+00 - val_loss: 0.5809 - val_acc: 0.0000e+00\n",
      "Epoch 80/170\n",
      " - 0s - loss: 0.4927 - acc: 0.0000e+00 - val_loss: 0.5800 - val_acc: 0.0000e+00\n",
      "Epoch 81/170\n",
      " - 0s - loss: 0.4916 - acc: 0.0000e+00 - val_loss: 0.5792 - val_acc: 0.0000e+00\n",
      "Epoch 82/170\n",
      " - 0s - loss: 0.4900 - acc: 0.0000e+00 - val_loss: 0.5783 - val_acc: 0.0000e+00\n",
      "Epoch 83/170\n",
      " - 0s - loss: 0.4878 - acc: 0.0000e+00 - val_loss: 0.5772 - val_acc: 0.0000e+00\n",
      "Epoch 84/170\n",
      " - 0s - loss: 0.4869 - acc: 0.0000e+00 - val_loss: 0.5763 - val_acc: 0.0000e+00\n",
      "Epoch 85/170\n",
      " - 0s - loss: 0.4847 - acc: 0.0000e+00 - val_loss: 0.5756 - val_acc: 0.0000e+00\n",
      "Epoch 86/170\n",
      " - 0s - loss: 0.4837 - acc: 0.0000e+00 - val_loss: 0.5749 - val_acc: 0.0000e+00\n",
      "Epoch 87/170\n",
      " - 0s - loss: 0.4820 - acc: 0.0000e+00 - val_loss: 0.5741 - val_acc: 0.0000e+00\n",
      "Epoch 88/170\n",
      " - 0s - loss: 0.4809 - acc: 0.0000e+00 - val_loss: 0.5735 - val_acc: 0.0000e+00\n",
      "Epoch 89/170\n",
      " - 0s - loss: 0.4797 - acc: 0.0000e+00 - val_loss: 0.5728 - val_acc: 0.0000e+00\n",
      "Epoch 90/170\n",
      " - 0s - loss: 0.4786 - acc: 0.0000e+00 - val_loss: 0.5722 - val_acc: 0.0000e+00\n",
      "Epoch 91/170\n",
      " - 0s - loss: 0.4777 - acc: 0.0000e+00 - val_loss: 0.5713 - val_acc: 0.0000e+00\n",
      "Epoch 92/170\n",
      " - 0s - loss: 0.4763 - acc: 0.0000e+00 - val_loss: 0.5707 - val_acc: 0.0000e+00\n",
      "Epoch 93/170\n",
      " - 0s - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.5700 - val_acc: 0.0000e+00\n",
      "Epoch 94/170\n",
      " - 0s - loss: 0.4746 - acc: 0.0000e+00 - val_loss: 0.5694 - val_acc: 0.0000e+00\n",
      "Epoch 95/170\n",
      " - 0s - loss: 0.4732 - acc: 0.0000e+00 - val_loss: 0.5688 - val_acc: 0.0000e+00\n",
      "Epoch 96/170\n",
      " - 0s - loss: 0.4724 - acc: 0.0000e+00 - val_loss: 0.5682 - val_acc: 0.0000e+00\n",
      "Epoch 97/170\n",
      " - 0s - loss: 0.4717 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 98/170\n",
      " - 0s - loss: 0.4714 - acc: 0.0000e+00 - val_loss: 0.5672 - val_acc: 0.0000e+00\n",
      "Epoch 99/170\n",
      " - 0s - loss: 0.4697 - acc: 0.0000e+00 - val_loss: 0.5665 - val_acc: 0.0000e+00\n",
      "Epoch 100/170\n",
      " - 0s - loss: 0.4689 - acc: 0.0000e+00 - val_loss: 0.5660 - val_acc: 0.0000e+00\n",
      "Epoch 101/170\n",
      " - 0s - loss: 0.4683 - acc: 0.0000e+00 - val_loss: 0.5657 - val_acc: 0.0000e+00\n",
      "Epoch 102/170\n",
      " - 0s - loss: 0.4670 - acc: 0.0000e+00 - val_loss: 0.5652 - val_acc: 0.0000e+00\n",
      "Epoch 103/170\n",
      " - 0s - loss: 0.4667 - acc: 0.0000e+00 - val_loss: 0.5648 - val_acc: 0.0000e+00\n",
      "Epoch 104/170\n",
      " - 0s - loss: 0.4656 - acc: 0.0000e+00 - val_loss: 0.5644 - val_acc: 0.0000e+00\n",
      "Epoch 105/170\n",
      " - 0s - loss: 0.4658 - acc: 0.0000e+00 - val_loss: 0.5640 - val_acc: 0.0000e+00\n",
      "Epoch 106/170\n",
      " - 0s - loss: 0.4647 - acc: 0.0000e+00 - val_loss: 0.5635 - val_acc: 0.0000e+00\n",
      "Epoch 107/170\n",
      " - 0s - loss: 0.4637 - acc: 0.0000e+00 - val_loss: 0.5631 - val_acc: 0.0000e+00\n",
      "Epoch 108/170\n",
      " - 0s - loss: 0.4632 - acc: 0.0000e+00 - val_loss: 0.5627 - val_acc: 0.0000e+00\n",
      "Epoch 109/170\n",
      " - 0s - loss: 0.4625 - acc: 0.0000e+00 - val_loss: 0.5623 - val_acc: 0.0000e+00\n",
      "Epoch 110/170\n",
      " - 0s - loss: 0.4623 - acc: 0.0000e+00 - val_loss: 0.5620 - val_acc: 0.0000e+00\n",
      "Epoch 111/170\n",
      " - 0s - loss: 0.4619 - acc: 0.0000e+00 - val_loss: 0.5615 - val_acc: 0.0000e+00\n",
      "Epoch 112/170\n",
      " - 0s - loss: 0.4617 - acc: 0.0000e+00 - val_loss: 0.5612 - val_acc: 0.0000e+00\n",
      "Epoch 113/170\n",
      " - 0s - loss: 0.4606 - acc: 0.0000e+00 - val_loss: 0.5608 - val_acc: 0.0000e+00\n",
      "Epoch 114/170\n",
      " - 0s - loss: 0.4601 - acc: 0.0000e+00 - val_loss: 0.5604 - val_acc: 0.0000e+00\n",
      "Epoch 115/170\n",
      " - 0s - loss: 0.4597 - acc: 0.0000e+00 - val_loss: 0.5600 - val_acc: 0.0000e+00\n",
      "Epoch 116/170\n",
      " - 0s - loss: 0.4595 - acc: 0.0000e+00 - val_loss: 0.5599 - val_acc: 0.0000e+00\n",
      "Epoch 117/170\n",
      " - 0s - loss: 0.4588 - acc: 0.0000e+00 - val_loss: 0.5596 - val_acc: 0.0000e+00\n",
      "Epoch 118/170\n",
      " - 0s - loss: 0.4590 - acc: 0.0000e+00 - val_loss: 0.5594 - val_acc: 0.0000e+00\n",
      "Epoch 119/170\n",
      " - 0s - loss: 0.4580 - acc: 0.0000e+00 - val_loss: 0.5591 - val_acc: 0.0000e+00\n",
      "Epoch 120/170\n",
      " - 0s - loss: 0.4575 - acc: 0.0000e+00 - val_loss: 0.5587 - val_acc: 0.0000e+00\n",
      "Epoch 121/170\n",
      " - 0s - loss: 0.4583 - acc: 0.0000e+00 - val_loss: 0.5585 - val_acc: 0.0000e+00\n",
      "Epoch 122/170\n",
      " - 0s - loss: 0.4569 - acc: 0.0000e+00 - val_loss: 0.5583 - val_acc: 0.0000e+00\n",
      "Epoch 123/170\n",
      " - 0s - loss: 0.4563 - acc: 0.0000e+00 - val_loss: 0.5581 - val_acc: 0.0000e+00\n",
      "Epoch 124/170\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5578 - val_acc: 0.0000e+00\n",
      "Epoch 125/170\n",
      " - 0s - loss: 0.4555 - acc: 0.0000e+00 - val_loss: 0.5575 - val_acc: 0.0000e+00\n",
      "Epoch 126/170\n",
      " - 0s - loss: 0.4553 - acc: 0.0000e+00 - val_loss: 0.5571 - val_acc: 0.0000e+00\n",
      "Epoch 127/170\n",
      " - 0s - loss: 0.4552 - acc: 0.0000e+00 - val_loss: 0.5568 - val_acc: 0.0000e+00\n",
      "Epoch 128/170\n",
      " - 0s - loss: 0.4547 - acc: 0.0000e+00 - val_loss: 0.5567 - val_acc: 0.0000e+00\n",
      "Epoch 129/170\n",
      " - 0s - loss: 0.4545 - acc: 0.0000e+00 - val_loss: 0.5565 - val_acc: 0.0000e+00\n",
      "Epoch 130/170\n",
      " - 0s - loss: 0.4541 - acc: 0.0000e+00 - val_loss: 0.5564 - val_acc: 0.0000e+00\n",
      "Epoch 131/170\n",
      " - 0s - loss: 0.4539 - acc: 0.0000e+00 - val_loss: 0.5561 - val_acc: 0.0000e+00\n",
      "Epoch 132/170\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5560 - val_acc: 0.0000e+00\n",
      "Epoch 133/170\n",
      " - 0s - loss: 0.4535 - acc: 0.0000e+00 - val_loss: 0.5560 - val_acc: 0.0000e+00\n",
      "Epoch 134/170\n",
      " - 0s - loss: 0.4529 - acc: 0.0000e+00 - val_loss: 0.5559 - val_acc: 0.0000e+00\n",
      "Epoch 135/170\n",
      " - 0s - loss: 0.4528 - acc: 0.0000e+00 - val_loss: 0.5557 - val_acc: 0.0000e+00\n",
      "Epoch 136/170\n",
      " - 0s - loss: 0.4522 - acc: 0.0000e+00 - val_loss: 0.5556 - val_acc: 0.0000e+00\n",
      "Epoch 137/170\n",
      " - 0s - loss: 0.4526 - acc: 0.0000e+00 - val_loss: 0.5555 - val_acc: 0.0000e+00\n",
      "Epoch 138/170\n",
      " - 0s - loss: 0.4524 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 139/170\n",
      " - 0s - loss: 0.4521 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 140/170\n",
      " - 0s - loss: 0.4519 - acc: 0.0000e+00 - val_loss: 0.5554 - val_acc: 0.0000e+00\n",
      "Epoch 141/170\n",
      " - 0s - loss: 0.4515 - acc: 0.0000e+00 - val_loss: 0.5553 - val_acc: 0.0000e+00\n",
      "Epoch 142/170\n",
      " - 0s - loss: 0.4516 - acc: 0.0000e+00 - val_loss: 0.5551 - val_acc: 0.0000e+00\n",
      "Epoch 143/170\n",
      " - 0s - loss: 0.4515 - acc: 0.0000e+00 - val_loss: 0.5549 - val_acc: 0.0000e+00\n",
      "Epoch 144/170\n",
      " - 0s - loss: 0.4512 - acc: 0.0000e+00 - val_loss: 0.5548 - val_acc: 0.0000e+00\n",
      "Epoch 145/170\n",
      " - 0s - loss: 0.4509 - acc: 0.0000e+00 - val_loss: 0.5547 - val_acc: 0.0000e+00\n",
      "Epoch 146/170\n",
      " - 0s - loss: 0.4504 - acc: 0.0000e+00 - val_loss: 0.5544 - val_acc: 0.0000e+00\n",
      "Epoch 147/170\n",
      " - 0s - loss: 0.4505 - acc: 0.0000e+00 - val_loss: 0.5543 - val_acc: 0.0000e+00\n",
      "Epoch 148/170\n",
      " - 0s - loss: 0.4498 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 149/170\n",
      " - 0s - loss: 0.4496 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 150/170\n",
      " - 0s - loss: 0.4496 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 151/170\n",
      " - 0s - loss: 0.4498 - acc: 0.0000e+00 - val_loss: 0.5544 - val_acc: 0.0000e+00\n",
      "Epoch 152/170\n",
      " - 0s - loss: 0.4494 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 153/170\n",
      " - 0s - loss: 0.4491 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 154/170\n",
      " - 0s - loss: 0.4495 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 155/170\n",
      " - 0s - loss: 0.4489 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 156/170\n",
      " - 0s - loss: 0.4492 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n",
      "Epoch 157/170\n",
      " - 0s - loss: 0.4486 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 158/170\n",
      " - 0s - loss: 0.4492 - acc: 0.0000e+00 - val_loss: 0.5542 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/170\n",
      " - 0s - loss: 0.4482 - acc: 0.0000e+00 - val_loss: 0.5541 - val_acc: 0.0000e+00\n",
      "Epoch 160/170\n",
      " - 0s - loss: 0.4480 - acc: 0.0000e+00 - val_loss: 0.5538 - val_acc: 0.0000e+00\n",
      "Epoch 161/170\n",
      " - 0s - loss: 0.4490 - acc: 0.0000e+00 - val_loss: 0.5537 - val_acc: 0.0000e+00\n",
      "Epoch 162/170\n",
      " - 0s - loss: 0.4478 - acc: 0.0000e+00 - val_loss: 0.5535 - val_acc: 0.0000e+00\n",
      "Epoch 163/170\n",
      " - 0s - loss: 0.4477 - acc: 0.0000e+00 - val_loss: 0.5535 - val_acc: 0.0000e+00\n",
      "Epoch 164/170\n",
      " - 0s - loss: 0.4482 - acc: 0.0000e+00 - val_loss: 0.5533 - val_acc: 0.0000e+00\n",
      "Epoch 165/170\n",
      " - 0s - loss: 0.4470 - acc: 0.0000e+00 - val_loss: 0.5532 - val_acc: 0.0000e+00\n",
      "Epoch 166/170\n",
      " - 0s - loss: 0.4468 - acc: 0.0000e+00 - val_loss: 0.5532 - val_acc: 0.0000e+00\n",
      "Epoch 167/170\n",
      " - 0s - loss: 0.4473 - acc: 0.0000e+00 - val_loss: 0.5530 - val_acc: 0.0000e+00\n",
      "Epoch 168/170\n",
      " - 0s - loss: 0.4469 - acc: 0.0000e+00 - val_loss: 0.5531 - val_acc: 0.0000e+00\n",
      "Epoch 169/170\n",
      " - 0s - loss: 0.4471 - acc: 0.0000e+00 - val_loss: 0.5531 - val_acc: 0.0000e+00\n",
      "Epoch 170/170\n",
      " - 0s - loss: 0.4465 - acc: 0.0000e+00 - val_loss: 0.5529 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Train on 176 samples, validate on 266 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.9854 - acc: 0.0000e+00 - val_loss: 1.0209 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.9266 - acc: 0.0000e+00 - val_loss: 0.9717 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.8710 - acc: 0.0000e+00 - val_loss: 0.9245 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.8180 - acc: 0.0000e+00 - val_loss: 0.8819 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.7710 - acc: 0.0000e+00 - val_loss: 0.8439 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.7329 - acc: 0.0000e+00 - val_loss: 0.8104 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6979 - acc: 0.0000e+00 - val_loss: 0.7813 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.6689 - acc: 0.0000e+00 - val_loss: 0.7563 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6426 - acc: 0.0000e+00 - val_loss: 0.7350 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.6217 - acc: 0.0000e+00 - val_loss: 0.7159 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.6018 - acc: 0.0000e+00 - val_loss: 0.6993 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5846 - acc: 0.0000e+00 - val_loss: 0.6846 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5705 - acc: 0.0000e+00 - val_loss: 0.6716 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5564 - acc: 0.0000e+00 - val_loss: 0.6601 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5446 - acc: 0.0000e+00 - val_loss: 0.6497 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5343 - acc: 0.0000e+00 - val_loss: 0.6404 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5245 - acc: 0.0000e+00 - val_loss: 0.6321 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5153 - acc: 0.0000e+00 - val_loss: 0.6249 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5079 - acc: 0.0000e+00 - val_loss: 0.6182 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5003 - acc: 0.0000e+00 - val_loss: 0.6125 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.4939 - acc: 0.0000e+00 - val_loss: 0.6072 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.4890 - acc: 0.0000e+00 - val_loss: 0.6026 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.4835 - acc: 0.0000e+00 - val_loss: 0.5985 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.4791 - acc: 0.0000e+00 - val_loss: 0.5949 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.4749 - acc: 0.0000e+00 - val_loss: 0.5917 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.4711 - acc: 0.0000e+00 - val_loss: 0.5889 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.4675 - acc: 0.0000e+00 - val_loss: 0.5863 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.4650 - acc: 0.0000e+00 - val_loss: 0.5840 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.4624 - acc: 0.0000e+00 - val_loss: 0.5820 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.4604 - acc: 0.0000e+00 - val_loss: 0.5802 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.4578 - acc: 0.0000e+00 - val_loss: 0.5787 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.5774 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.4540 - acc: 0.0000e+00 - val_loss: 0.5761 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.4531 - acc: 0.0000e+00 - val_loss: 0.5750 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.4513 - acc: 0.0000e+00 - val_loss: 0.5741 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.4501 - acc: 0.0000e+00 - val_loss: 0.5734 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.4492 - acc: 0.0000e+00 - val_loss: 0.5727 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.4480 - acc: 0.0000e+00 - val_loss: 0.5721 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.4470 - acc: 0.0000e+00 - val_loss: 0.5716 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4469 - acc: 0.0000e+00 - val_loss: 0.5712 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4456 - acc: 0.0000e+00 - val_loss: 0.5708 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4454 - acc: 0.0000e+00 - val_loss: 0.5703 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4445 - acc: 0.0000e+00 - val_loss: 0.5700 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4448 - acc: 0.0000e+00 - val_loss: 0.5696 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4437 - acc: 0.0000e+00 - val_loss: 0.5693 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.4434 - acc: 0.0000e+00 - val_loss: 0.5692 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4431 - acc: 0.0000e+00 - val_loss: 0.5691 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4427 - acc: 0.0000e+00 - val_loss: 0.5690 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4423 - acc: 0.0000e+00 - val_loss: 0.5689 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.4421 - acc: 0.0000e+00 - val_loss: 0.5688 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.4427 - acc: 0.0000e+00 - val_loss: 0.5685 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.4414 - acc: 0.0000e+00 - val_loss: 0.5683 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.4413 - acc: 0.0000e+00 - val_loss: 0.5682 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.4413 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.4408 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.4408 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.4407 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.4403 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.4411 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.4405 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4401 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4399 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4398 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.4400 - acc: 0.0000e+00 - val_loss: 0.5680 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.4396 - acc: 0.0000e+00 - val_loss: 0.5679 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.4403 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.4400 - acc: 0.0000e+00 - val_loss: 0.5677 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.4404 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.4397 - acc: 0.0000e+00 - val_loss: 0.5678 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.4394 - acc: 0.0000e+00 - val_loss: 0.5677 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.4394 - acc: 0.0000e+00 - val_loss: 0.5676 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5675 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5674 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.4396 - acc: 0.0000e+00 - val_loss: 0.5672 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.4389 - acc: 0.0000e+00 - val_loss: 0.5671 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.4393 - acc: 0.0000e+00 - val_loss: 0.5671 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5671 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.4391 - acc: 0.0000e+00 - val_loss: 0.5669 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.4399 - acc: 0.0000e+00 - val_loss: 0.5669 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5668 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.4390 - acc: 0.0000e+00 - val_loss: 0.5666 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5664 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5661 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.4387 - acc: 0.0000e+00 - val_loss: 0.5663 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5663 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5662 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.4384 - acc: 0.0000e+00 - val_loss: 0.5663 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5661 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.4395 - acc: 0.0000e+00 - val_loss: 0.5659 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4382 - acc: 0.0000e+00 - val_loss: 0.5661 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.4390 - acc: 0.0000e+00 - val_loss: 0.5660 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5660 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5659 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4392 - acc: 0.0000e+00 - val_loss: 0.5659 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4387 - acc: 0.0000e+00 - val_loss: 0.5658 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4381 - acc: 0.0000e+00 - val_loss: 0.5657 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4388 - acc: 0.0000e+00 - val_loss: 0.5654 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.5654 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4387 - acc: 0.0000e+00 - val_loss: 0.5652 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.4379 - acc: 0.0000e+00 - val_loss: 0.5652 - val_acc: 0.0000e+00\n",
      "Test accuracy: 0.0\n",
      "Evalutation of best performing model:\n",
      "266/266 [==============================] - 0s 4us/step\n",
      "[0.5605926444207815, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    Make sure to have every relevant import statement included here and return data as\n",
    "    used in model function below. This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    from os import path\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    url = \"../data/diabetes.csv\"\n",
    "    data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "    sc = StandardScaler()\n",
    "    data = sc.fit_transform(data)\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=10, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer={{choice(['adam', 'nadam'])}})\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([10, 30])}},\n",
    "              epochs={{choice([100, 170])}},\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='experiment')\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Hyperas(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils.validation import column_or_1d\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.losses import lda_loss\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import copy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import datasets\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import experiment_automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from automation_script import get_dataset_info\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from imly import dope\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from utils.correlations import concordance_correlation_coefficient as ccc\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import boto\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from boto.s3.key import Key\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.datasets import make_regression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano.tensor as T\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import theano\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from theano.compile.ops import as_op\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import automation_script\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'nadam']),\n",
      "        'batch_size': hp.choice('batch_size', [10]),\n",
      "        'epochs': hp.choice('epochs', [100]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: Make sure to have every relevant import statement included here and return data as\n",
      "  6: used in model function below. This function is separated from model() so that hyperopt\n",
      "  7: won't reload data for each evaluation run.\n",
      "  8: '''\n",
      "  9: url = \"../data/iris.csv\"\n",
      " 10: data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
      " 11: class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
      " 12: data.iloc[:,-1] = index\n",
      " 13: data = data.loc[data[4] != 2]\n",
      " 14: X = data.iloc[:,:-1]\n",
      " 15: Y = data.iloc[:,-1]\n",
      " 16: x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
      " 17: \n",
      " 18: \n",
      " 19: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     '''\n",
      "  13: \n",
      "  14:     model = Sequential()\n",
      "  15:     model.add(Dense(1, input_dim=4, activation='sigmoid',\n",
      "  16:                    kernel_regularizer=l2(1e-5)))\n",
      "  17: \n",
      "  18:     model.compile(loss=lda_loss(n_components=1, margin=1),\n",
      "  19:                  optimizer=space['optimizer'],\n",
      "  20:                  metrics=['accuracy'])\n",
      "  21: \n",
      "  22:     model.fit(x_train, y_train,\n",
      "  23:               batch_size=space['batch_size'],\n",
      "  24:               epochs=space['epochs'],\n",
      "  25:               verbose=2,\n",
      "  26:               validation_data=(x_test, y_test))\n",
      "  27:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  28:     print('Test accuracy:', acc)\n",
      "  29:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  30: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40 samples, validate on 60 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shakk\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py:150: RuntimeWarning: All-NaN slice encountered\n",
      "  return np.isinf(np.nanmax(arr)) or np.isinf(np.nanmin(arr))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "NaN detected\nNanGuardMode found an error in the output of a node in this variable:\nElemwise{true_div,no_inplace} [id A] ''   \n |Dot22 [id B] ''   \n | |InplaceDimShuffle{1,0} [id C] ''   \n | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n | |   |AdvancedSubtensor1 [id E] ''   \n | |   | |<TensorType(float32, matrix)> [id F]\n | |   | |Subtensor{int64} [id G] ''   \n | |   |   |Nonzero [id H] ''   \n | |   |   | |<TensorType(bool, vector)> [id I]\n | |   |   |Constant{0} [id J]\n | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n | |     |InplaceDimShuffle{x,0} [id L] ''   \n | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n | |     |   |AdvancedSubtensor1 [id E] ''   \n | |     |Elemwise{Cast{float32}} [id N] ''   \n | |       |InplaceDimShuffle{x,x} [id O] ''   \n | |         |Shape_i{1} [id P] ''   \n | |           |Nonzero [id H] ''   \n | |Elemwise{Sub}[(0, 0)] [id D] ''   \n |Elemwise{Add}[(0, 1)] [id Q] ''   \n   |TensorConstant{(1, 1) of -1.0} [id R]\n   |Elemwise{Cast{float32}} [id N] ''   \n\n\n\nApply node that caused the error: Elemwise{true_div,no_inplace}(Dot22.0, Elemwise{Add}[(0, 1)].0)\nToposort index: 13\nInputs types: [TensorType(float32, matrix), TensorType(float32, (True, True))]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[0.]], dtype=float32), array([[0.]], dtype=float32)]\nOutputs clients: [['output']]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: for{cpu,scan_fn}(Shape_i{0}.0, Elemwise{eq,no_inplace}.0, Shape_i{0}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0)\nToposort index: 70\nInputs types: [TensorType(int64, scalar), TensorType(bool, matrix), TensorType(int64, scalar), TensorType(float32, matrix)]\nInputs shapes: [(), (2, 10), (), (10, 1)]\nInputs strides: [(), (10, 1), (), (4, 4)]\nInputs values: [array(2, dtype=int64), 'not shown', array(2, dtype=int64), 'not shown']\nOutputs clients: [[Sum{axis=[0], acc_dtype=float64}(for{cpu,scan_fn}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    489\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_thunk_of_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                         \u001b[1;32mdel\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36mrun_thunk_of_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mstorage_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mnan_check\u001b[1;34m(node, thunk, storage_map, compute_map)\u001b[0m\n\u001b[0;32m    272\u001b[0m                         getattr(var.tag, 'nan_guard_mode_check', True)):\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[0mdo_check_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mdo_check_on\u001b[1;34m(value, nd, var)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pdb'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: NaN detected\nNanGuardMode found an error in the output of a node in this variable:\nElemwise{true_div,no_inplace} [id A] ''   \n |Dot22 [id B] ''   \n | |InplaceDimShuffle{1,0} [id C] ''   \n | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n | |   |AdvancedSubtensor1 [id E] ''   \n | |   | |<TensorType(float32, matrix)> [id F]\n | |   | |Subtensor{int64} [id G] ''   \n | |   |   |Nonzero [id H] ''   \n | |   |   | |<TensorType(bool, vector)> [id I]\n | |   |   |Constant{0} [id J]\n | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n | |     |InplaceDimShuffle{x,0} [id L] ''   \n | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n | |     |   |AdvancedSubtensor1 [id E] ''   \n | |     |Elemwise{Cast{float32}} [id N] ''   \n | |       |InplaceDimShuffle{x,x} [id O] ''   \n | |         |Shape_i{1} [id P] ''   \n | |           |Nonzero [id H] ''   \n | |Elemwise{Sub}[(0, 0)] [id D] ''   \n |Elemwise{Add}[(0, 1)] [id Q] ''   \n   |TensorConstant{(1, 1) of -1.0} [id R]\n   |Elemwise{Cast{float32}} [id N] ''   \n\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    522\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m                             storage_map=storage_map)\n\u001b[0m\u001b[0;32m    524\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcurrent_apply\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    489\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_thunk_of_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                         \u001b[1;32mdel\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36mrun_thunk_of_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mstorage_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mnan_check\u001b[1;34m(node, thunk, storage_map, compute_map)\u001b[0m\n\u001b[0;32m    272\u001b[0m                         getattr(var.tag, 'nan_guard_mode_check', True)):\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[0mdo_check_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mdo_check_on\u001b[1;34m(value, nd, var)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pdb'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: NaN detected\nNanGuardMode found an error in the output of a node in this variable:\nElemwise{true_div,no_inplace} [id A] ''   \n |Dot22 [id B] ''   \n | |InplaceDimShuffle{1,0} [id C] ''   \n | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n | |   |AdvancedSubtensor1 [id E] ''   \n | |   | |<TensorType(float32, matrix)> [id F]\n | |   | |Subtensor{int64} [id G] ''   \n | |   |   |Nonzero [id H] ''   \n | |   |   | |<TensorType(bool, vector)> [id I]\n | |   |   |Constant{0} [id J]\n | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n | |     |InplaceDimShuffle{x,0} [id L] ''   \n | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n | |     |   |AdvancedSubtensor1 [id E] ''   \n | |     |Elemwise{Cast{float32}} [id N] ''   \n | |       |InplaceDimShuffle{x,x} [id O] ''   \n | |         |Shape_i{1} [id P] ''   \n | |           |Nonzero [id H] ''   \n | |Elemwise{Sub}[(0, 0)] [id D] ''   \n |Elemwise{Add}[(0, 1)] [id Q] ''   \n   |TensorConstant{(1, 1) of -1.0} [id R]\n   |Elemwise{Cast{float32}} [id N] ''   \n\n\n\nApply node that caused the error: Elemwise{true_div,no_inplace}(Dot22.0, Elemwise{Add}[(0, 1)].0)\nToposort index: 13\nInputs types: [TensorType(float32, matrix), TensorType(float32, (True, True))]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[0.]], dtype=float32), array([[0.]], dtype=float32)]\nOutputs clients: [['output']]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bd9e63ea320b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m                                       \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                       \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                       notebook_name='experiment')\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                      verbose=verbose)\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[0;32m    131\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m              return_argmin=True),\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m         )\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[0;32m    383\u001b[0m                     max_queue_len=max_queue_len)\n\u001b[0;32m    384\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    918\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 963\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    521\u001b[0m                             \u001b[0mcurrent_apply\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m                             storage_map=storage_map)\n\u001b[0m\u001b[0;32m    524\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcurrent_apply\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                         \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, output_subset)\u001b[0m\n\u001b[0;32m    488\u001b[0m                     \u001b[1;31m# -- Non-lazy case: have inputs, time to compute outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_thunk_of_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_apply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                         \u001b[1;32mdel\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_global_stats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36mrun_thunk_of_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mstorage_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_map\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m             )\n\u001b[0;32m    407\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mnan_check\u001b[1;34m(node, thunk, storage_map, compute_map)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 if (compute_map[var][0] and\n\u001b[0;32m    272\u001b[0m                         getattr(var.tag, 'nan_guard_mode_check', True)):\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[0mdo_check_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mnan_check_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\theano\\compile\\nanguardmode.py\u001b[0m in \u001b[0;36mdo_check_on\u001b[1;34m(value, nd, var)\u001b[0m\n\u001b[0;32m    259\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNanGuardMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pdb'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: NaN detected\nNanGuardMode found an error in the output of a node in this variable:\nElemwise{true_div,no_inplace} [id A] ''   \n |Dot22 [id B] ''   \n | |InplaceDimShuffle{1,0} [id C] ''   \n | | |Elemwise{Sub}[(0, 0)] [id D] ''   \n | |   |AdvancedSubtensor1 [id E] ''   \n | |   | |<TensorType(float32, matrix)> [id F]\n | |   | |Subtensor{int64} [id G] ''   \n | |   |   |Nonzero [id H] ''   \n | |   |   | |<TensorType(bool, vector)> [id I]\n | |   |   |Constant{0} [id J]\n | |   |Elemwise{TrueDiv}[(0, 0)] [id K] ''   \n | |     |InplaceDimShuffle{x,0} [id L] ''   \n | |     | |Sum{axis=[0], acc_dtype=float64} [id M] ''   \n | |     |   |AdvancedSubtensor1 [id E] ''   \n | |     |Elemwise{Cast{float32}} [id N] ''   \n | |       |InplaceDimShuffle{x,x} [id O] ''   \n | |         |Shape_i{1} [id P] ''   \n | |           |Nonzero [id H] ''   \n | |Elemwise{Sub}[(0, 0)] [id D] ''   \n |Elemwise{Add}[(0, 1)] [id Q] ''   \n   |TensorConstant{(1, 1) of -1.0} [id R]\n   |Elemwise{Cast{float32}} [id N] ''   \n\n\n\nApply node that caused the error: Elemwise{true_div,no_inplace}(Dot22.0, Elemwise{Add}[(0, 1)].0)\nToposort index: 13\nInputs types: [TensorType(float32, matrix), TensorType(float32, (True, True))]\nInputs shapes: [(1, 1), (1, 1)]\nInputs strides: [(4, 4), (4, 4)]\nInputs values: [array([[0.]], dtype=float32), array([[0.]], dtype=float32)]\nOutputs clients: [['output']]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: for{cpu,scan_fn}(Shape_i{0}.0, Elemwise{eq,no_inplace}.0, Shape_i{0}.0, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0)\nToposort index: 70\nInputs types: [TensorType(int64, scalar), TensorType(bool, matrix), TensorType(int64, scalar), TensorType(float32, matrix)]\nInputs shapes: [(), (2, 10), (), (10, 1)]\nInputs strides: [(), (10, 1), (), (4, 4)]\nInputs values: [array(2, dtype=int64), 'not shown', array(2, dtype=int64), 'not shown']\nOutputs clients: [[Sum{axis=[0], acc_dtype=float64}(for{cpu,scan_fn}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from utils.losses import lda_loss\n",
    "\n",
    "def data():\n",
    "    '''\n",
    "    Data providing function:\n",
    "\n",
    "    Make sure to have every relevant import statement included here and return data as\n",
    "    used in model function below. This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    '''\n",
    "    url = \"../data/iris.csv\"\n",
    "    data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "    class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "    data.iloc[:,-1] = index\n",
    "    data = data.loc[data[4] != 2]\n",
    "    X = data.iloc[:,:-1]\n",
    "    Y = data.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    '''\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=4, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-5)))\n",
    "\n",
    "    model.compile(loss=lda_loss(n_components=1, margin=1),\n",
    "                 optimizer={{choice(['adam', 'nadam'])}},\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size={{choice([10])}},\n",
    "              epochs={{choice([100])}},\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='experiment')\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = \"../data/iris.csv\"\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def make_model(config):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1, input_dim=4, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-17 22:27:49,461\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-17_22-27-49_12189/logs.\n",
      "2019-02-17 22:27:49,572\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:20473 to respond...\n",
      "2019-02-17 22:27:49,685\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:31160 to respond...\n",
      "2019-02-17 22:27:49,688\tINFO services.py:798 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-02-17 22:27:49,709\tINFO services.py:1360 -- Starting the Plasma object store with 3.285091942 GB memory using /dev/shm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8890/notebooks/ray_ui.ipynb?token=f658dd85001a7ce0f1d9a41dbdb0a3fa7f97664ccf48dc33\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': None,\n",
       " 'redis_address': '192.168.1.4:20473',\n",
       " 'object_store_address': '/tmp/ray/session_2019-02-17_22-27-49_12189/sockets/plasma_store',\n",
       " 'webui_url': 'http://localhost:8890/notebooks/ray_ui.ipynb?token=f658dd85001a7ce0f1d9a41dbdb0a3fa7f97664ccf48dc33',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-02-17_22-27-49_12189/sockets/raylet'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize tune\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the signature of your function to match that of Tune #\n",
    "def train_iris_tune(config, reporter):\n",
    "    model = make_model(None)\n",
    "    model.fit(x_train, y_train)\n",
    "    accuracy = model.evaluate(x_test, y_test)[1]\n",
    "    reporter(mean_accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define experiment config\n",
    "configuration = tune.Experiment(\n",
    "    \"experiment_name\",\n",
    "    run=train_iris_tune,\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    stop={\"mean_accuracy\": 95},  # TODO: Part 1\n",
    "    config={\n",
    "        \"optimizer\": tune.grid_search(['adam', 'nadam'])\n",
    "    }  # TODO: Part 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-17 23:28:22,258\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-17 23:28:22,259\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_iris_tune_1_optimizer=nadam:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_iris_tune_0_optimizer=adam:\tRUNNING\n",
      "\n",
      "Result for train_iris_tune_1_optimizer=nadam:\n",
      "  date: 2019-02-17_23-28-27\n",
      "  done: false\n",
      "  experiment_id: f1a60b6ee62540f98907033cd05fc02b\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.5333333373069763\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 12249\n",
      "  time_since_restore: 1.000760793685913\n",
      "  time_this_iter_s: 1.000760793685913\n",
      "  time_total_s: 1.000760793685913\n",
      "  timestamp: 1550426307\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_0_optimizer=adam:\n",
      "  date: 2019-02-17_23-28-27\n",
      "  done: false\n",
      "  experiment_id: 6f4b0eae168f45eaa5f4cc8ec7e5401c\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.5333333373069763\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 12247\n",
      "  time_since_restore: 1.001235008239746\n",
      "  time_this_iter_s: 1.001235008239746\n",
      "  time_total_s: 1.001235008239746\n",
      "  timestamp: 1550426307\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_iris_tune_1_optimizer=nadam:\n",
      "  date: 2019-02-17_23-28-28\n",
      "  done: true\n",
      "  experiment_id: f1a60b6ee62540f98907033cd05fc02b\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.5333333373069763\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 12249\n",
      "  time_since_restore: 2.0021607875823975\n",
      "  time_this_iter_s: 1.0013999938964844\n",
      "  time_total_s: 2.0021607875823975\n",
      "  timestamp: 1550426308\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "RUNNING trials:\n",
      " - train_iris_tune_0_optimizer=adam:\tRUNNING [pid=12247], 1 s, 1 iter, 0.533 acc\n",
      "TERMINATED trials:\n",
      " - train_iris_tune_1_optimizer=nadam:\tTERMINATED [pid=12249], 2 s, 2 iter, 0.533 acc\n",
      "\n",
      "Result for train_iris_tune_0_optimizer=adam:\n",
      "  date: 2019-02-17_23-28-28\n",
      "  done: true\n",
      "  experiment_id: 6f4b0eae168f45eaa5f4cc8ec7e5401c\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.5333333373069763\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 12247\n",
      "  time_since_restore: 2.002418279647827\n",
      "  time_this_iter_s: 1.001183271408081\n",
      "  time_total_s: 2.002418279647827\n",
      "  timestamp: 1550426308\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_iris_tune_0_optimizer=adam:\tTERMINATED [pid=12247], 2 s, 2 iter, 0.533 acc\n",
      " - train_iris_tune_1_optimizer=nadam:\tTERMINATED [pid=12249], 2 s, 2 iter, 0.533 acc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = tune.run_experiments(configuration, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional features #\n",
    "# Scheduler, search_algo etc #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils from Tune #\n",
    "\n",
    "def get_sorted_trials(trial_list, metric):\n",
    "    return sorted(trial_list, key=lambda trial: trial.last_result.get(metric, 0), reverse=True)\n",
    "\n",
    "\n",
    "def get_best_result(trial_list, metric):\n",
    "    \"\"\"Retrieve the last result from the best trial.\"\"\"\n",
    "    return {metric: get_best_trial(trial_list, metric).last_result[metric]}\n",
    "\n",
    "\n",
    "def get_best_model(model_creator, trial_list, metric):\n",
    "    \"\"\"Restore a model from the best trial.\"\"\"\n",
    "    sorted_trials = get_sorted_trials(trial_list, metric)\n",
    "    for best_trial in sorted_trials:\n",
    "        try:\n",
    "            print(\"Creating model...\")\n",
    "            model = model_creator(best_trial.config)\n",
    "            weights = os.path.join(best_trial.logdir, best_trial.last_result[\"checkpoint\"])\n",
    "            print(\"Loading from\", weights)\n",
    "            model.load_weights(weights)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Loading failed. Trying next model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "name 'os' is not defined\n",
      "Loading failed. Trying next model\n",
      "Creating model...\n",
      "name 'os' is not defined\n",
      "Loading failed. Trying next model\n"
     ]
    }
   ],
   "source": [
    "# Get best model #\n",
    "final_model = get_best_model(make_model, trials, metric=\"mean_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f21b7b360b8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Tune integration with IMLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2019-02-18 19:50:47,754\tINFO node.py:278 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-18_19-50-47_13183/logs.\n",
      "2019-02-18 19:50:47,863\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:39983 to respond...\n",
      "2019-02-18 19:50:47,992\tINFO services.py:396 -- Waiting for redis server at 127.0.0.1:41836 to respond...\n",
      "2019-02-18 19:50:47,999\tINFO services.py:798 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-02-18 19:50:48,044\tINFO services.py:1360 -- Starting the Plasma object store with 3.2850935800000003 GB memory using /dev/shm.\n",
      "/home/shakkeel/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2019-02-18 19:50:48,276\tINFO tune.py:135 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run_experiments()\n",
      "2019-02-18 19:50:48,278\tINFO tune.py:145 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=08bc8dd7f5190f3c8146461eb07a4cbe0fda55a1f4a5171d\n",
      "======================================================================\n",
      "\n",
      "Keras classifier chosen\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "PENDING trials:\n",
      " - train_model_1_optimizer=nadam:\tPENDING\n",
      "RUNNING trials:\n",
      " - train_model_0_optimizer=adam:\tRUNNING\n",
      "\n",
      "Result for train_model_0_optimizer=adam:\n",
      "  date: 2019-02-18_19-50-51\n",
      "  done: false\n",
      "  experiment_id: d30057ca5bcf41239bfa2afc14b96048\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.45\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 13217\n",
      "  time_since_restore: 1.0011470317840576\n",
      "  time_this_iter_s: 1.0011470317840576\n",
      "  time_total_s: 1.0011470317840576\n",
      "  timestamp: 1550499651\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_1_optimizer=nadam:\n",
      "  date: 2019-02-18_19-50-51\n",
      "  done: false\n",
      "  experiment_id: 16116fa5ea1e41eaa7eb00b43970fbf6\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 13216\n",
      "  time_since_restore: 1.0011100769042969\n",
      "  time_this_iter_s: 1.0011100769042969\n",
      "  time_total_s: 1.0011100769042969\n",
      "  timestamp: 1550499651\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  \n",
      "Result for train_model_0_optimizer=adam:\n",
      "  date: 2019-02-18_19-50-52\n",
      "  done: true\n",
      "  experiment_id: d30057ca5bcf41239bfa2afc14b96048\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.45\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 13217\n",
      "  time_since_restore: 2.002741813659668\n",
      "  time_this_iter_s: 1.0015947818756104\n",
      "  time_total_s: 2.002741813659668\n",
      "  timestamp: 1550499652\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "Result for train_model_1_optimizer=nadam:\n",
      "  date: 2019-02-18_19-50-52\n",
      "  done: true\n",
      "  experiment_id: 16116fa5ea1e41eaa7eb00b43970fbf6\n",
      "  hostname: shakkeel-TUF-GAMING-FX504GD-FX80GD\n",
      "  iterations_since_restore: 2\n",
      "  mean_accuracy: 0.0\n",
      "  node_ip: 192.168.1.4\n",
      "  pid: 13216\n",
      "  time_since_restore: 2.0023584365844727\n",
      "  time_this_iter_s: 1.0012483596801758\n",
      "  time_total_s: 2.0023584365844727\n",
      "  timestamp: 1550499652\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/8.2 GB\n",
      "Result logdir: /home/shakkeel/ray_results/experiment_name\n",
      "TERMINATED trials:\n",
      " - train_model_0_optimizer=adam:\tTERMINATED [pid=13217], 2 s, 2 iter, 0.45 acc\n",
      " - train_model_1_optimizer=nadam:\tTERMINATED [pid=13216], 2 s, 2 iter, 0 acc\n",
      "\n",
      "Creating model...\n",
      "'checkpoint' from tuner\n",
      "Loading failed. Trying next model\n",
      "Creating model...\n",
      "'checkpoint' from tuner\n",
      "Loading failed. Trying next model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f9814b829e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imly import dope\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = \"../data/iris.csv\"\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "m = dope(LogisticRegression())\n",
    "m.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 368us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4666666626930237"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Keras MNIST Example')\n",
    "parser.add_argument('--lr', type=float, default=0.1, help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, help='SGD momentum')\n",
    "parser.add_argument('--kernel1', type=int, default=3, help='Size of first kernel')\n",
    "parser.add_argument('--kernel2', type=int, default=3, help='Size of second kernel')\n",
    "parser.add_argument('--poolsize', type=int, default=2, help='Size of Poolin')\n",
    "parser.add_argument('--dropout1', type=float, default=0.25, help='Size of first kernel')\n",
    "parser.add_argument('--hidden', type=int, default=4, help='Size of Hidden Layer')\n",
    "parser.add_argument('--dropout2', type=float, default=0.5, help='Size of first kernel')\n",
    "\n",
    "DEFAULT_ARGS = vars(parser.parse_known_args()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.1,\n",
       " 'momentum': 0.0,\n",
       " 'kernel1': 3,\n",
       " 'kernel2': 3,\n",
       " 'poolsize': 2,\n",
       " 'dropout1': 0.25,\n",
       " 'hidden': 4,\n",
       " 'dropout2': 0.5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-19 18:08:25,085\tERROR worker.py:1632 -- The node with client ID 5626a904d1847aa6a026fc204de5c26fb852e18b has been marked dead because the monitor has missed too many heartbeats from it.\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_ARGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Hyperopt functionality #\n",
    "# WILL IT WORK WITHOUT THE HYPERBAND SCHEDULER? #\n",
    "from hyperopt import hp\n",
    "from ray.tune.suggest import HyperOptSearch\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.001, 0.1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.1, 0.9),\n",
    "    \"hidden\": hp.choice(\"hidden\", np.arange(16, 256, dtype=int)),\n",
    "}\n",
    "\n",
    "## TODO: CREATE A HyperOptObject\n",
    "hyperopt_search = HyperOptSearch(space, reward_attr=\"mean_accuracy\")\n",
    "\n",
    "## TODO: Pass in the object to Tune.\n",
    "good_results = tune.run_experiments(\n",
    "    configuration2, search_alg=hyperopt_search, scheduler=hyperband, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LinearRegression'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "model_name = 'linear_regression'\n",
    "model_mappings = {\n",
    "    'linear_regression': 'LinearRegression',\n",
    "    'logistic_regression': 'LogisticRegression'\n",
    "}\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "for key, value in model_mappings.items():\n",
    "    if key == model_name:\n",
    "        name = value\n",
    "\n",
    "module = __import__('sklearn.linear_model', fromlist=[name])\n",
    "imported_module = getattr(module, name)\n",
    "model = imported_module\n",
    "\n",
    "primal_model = model()\n",
    "\n",
    "# Primal\n",
    "primal_model.fit(x_train, y_train)\n",
    "primal_model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# diabetes = datasets.load_diabetes()\n",
    "# sc = StandardScaler()\n",
    "# diabetes = sc.fit_transform(diabetes)\n",
    "#####\n",
    "# # Use only one feature\n",
    "# diabetes_X = diabetes.data\n",
    "# # sc = StandardScaler()\n",
    "# # diabetes.data = sc.fit_transform(diabetes.data)\n",
    "\n",
    "# X = diabetes.data\n",
    "# Y = diabetes.target\n",
    "#####\n",
    "\n",
    "# X = preprocessing.scale(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "# # Split the data into training/testing sets\n",
    "# x_train = diabetes_X[:-20]\n",
    "# x_test = diabetes_X[-20:]\n",
    "\n",
    "# # Split the targets into training/testing sets\n",
    "# y_train = diabetes.target[:-20]\n",
    "# y_test = diabetes.target[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5481227216244245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "score = mean_squared_error(y_test, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x17f81a0aeb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm.__call__(param_name=\"log_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import experiment_automation_script\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dataset_info = experiment_automation_script.get_dataset_info(\"diabetes\")\n",
    "url = \"../data/diabetes.csv\" if path.exists(\"../data/diabetes.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wrappers.sklearn.keras_classifier.SklearnKerasClassifier"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "wrapper_class = 'SklearnKerasClassifier'\n",
    "\n",
    "path = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', wrapper_class)\n",
    "module_path = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', path).lower()\n",
    "package_name = module_path.split('_')[0]\n",
    "wrapper_name = '_'.join(module_path.split('_')[1:3])\n",
    "\n",
    "module_path = 'wrappers.' + package_name + '.' + wrapper_name\n",
    "module_path\n",
    "wrapper_module = __import__(module_path, fromlist=[wrapper_class])\n",
    "function = getattr(wrapper_module, wrapper_class)\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = 'sklearn_keras_classifier'\n",
    "'_'.join(module_path.split('_')[1:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"../data/uci_carbon_nanotubes.csv\"\n",
    "data = pd.read_csv(url, delimiter=\";\")\n",
    "data\n",
    "# frames = [X, Y]\n",
    "# data = pd.concat(frames, axis=1)\n",
    "# data.to_csv('../data/uci_auto_mpg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier chosen\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-463eefa48d20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# score = m.score(x_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\wrappers\\sklearn\\keras_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m                                                                        \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                                                        \u001b[0mval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_metric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                                                                        metric=self.metric) \n\u001b[0m\u001b[0;32m     40\u001b[0m             self.model.fit(x_train, y_train, epochs=final_epoch,\n\u001b[0;32m     41\u001b[0m                            batch_size=final_batch_size, verbose=0)\n",
      "\u001b[1;32m~\\Desktop\\MLSquare\\cook-imly\\imly\\optimizers\\talos\\talos.py\u001b[0m in \u001b[0;36mget_best_model\u001b[1;34m(x_train, y_train, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mexperiment_no\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexperiment_no\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtalos_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 grid_downsample=0.5)\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, params, model, dataset_name, experiment_no, x_val, y_val, val_split, shuffle, round_limit, grid_downsample, random_method, seed, search_method, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, reduce_loss, last_epoch_value, clear_tf_session, disable_progress_bar, print_params, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;31m# input parameters section ends\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_null\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\talos\\scan\\Scan.py\u001b[0m in \u001b[0;36mruntime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mruntime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\talos\\scan\\scan_prepare.py\u001b[0m in \u001b[0;36mscan_prepare\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# create the data asset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[0;32m     26\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[0;32m     27\u001b[0m           initial=_NoValue):\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "### Testing concordance ###\n",
    "\n",
    "from automation_script import get_dataset_info\n",
    "from imly import dope\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_info = get_dataset_info(\"uci_iris_lda\")\n",
    "url = dataset_info['url']\n",
    "data = pd.read_csv(url, delimiter=\",\", header=None, index_col=False)\n",
    "# sc = StandardScaler()\n",
    "# data = sc.fit_transform(data)\n",
    "# data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "m = dope(model)\n",
    "\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "m.fit(x_train, y_train)\n",
    "\n",
    "# score = m.score(x_test, y_test)\n",
    "\n",
    "\n",
    "### Automation script ###\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 200\n",
    "# }\n",
    "\n",
    "# experiment_automation_script.dopify(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "sklearn_pred = model.predict(x_test)\n",
    "keras_pred = m.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9989899125789394"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.correlations import concordance_correlation_coefficient as ccc\n",
    "\n",
    "ccc(sklearn_pred, keras_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22704f6fb38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.scatter(sklearn_pred, keras_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x261561d9358>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Testing CCC ###\n",
    "\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imly import dope\n",
    "\n",
    "dataset_name = \"uci_auto_mpg\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "data = pd.read_csv(\"../data/uci_auto_mpg.csv\", delimiter=\",\", header=0, index_col='car name')\n",
    "data = data[data.horsepower != '?']\n",
    "sc = StandardScaler()\n",
    "data = sc.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "Y = data.iloc[:,1]\n",
    "X = data.iloc[:,2:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "m = dope(model)\n",
    "\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "\n",
    "m.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "sklearn_pred = model.predict(x_test)\n",
    "keras_pred = m.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9894374129958334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.correlations import concordance_correlation_coefficient as ccc\n",
    "\n",
    "ccc(sklearn_pred, keras_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x26156618e10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3X+Q3HWd5/HnezoN9KDHRBkVGnKJt1RYI5KYKURTtSXoEsQFRgIC69ZirVspbpe60vJSF8orApR1jJfaUvf0TqNnLXtwGgQdw8Je/BEsr6iLR+Ikhgi5RYSYDiVRGO4ks9CZvO+P7m+np+f77f5297d/fl+Pqqnpnv5Ofz/pTH3e3+/n8/68P+buiIhI+oz0ugEiItIbCgAiIimlACAiklIKACIiKaUAICKSUgoAIiIppQAgIpJSCgAiIimlACAiklJLet2Aes4++2xfvnx5r5shIjIw9u7d+1t3H49zbF8HgOXLl7Nnz55eN0NEZGCY2fNxj9UQkIhISikAiIiklAKAiEhKKQCIiKSUAoCISEopAIiIpFRfp4GKiKTF9EyBrTsPcXR2jnPHcmxav5LJNfmOnlMBQESkx6ZnCtz+nQPMFecBKMzOcft3DgB0NAhoCEhEpMe27jxU6fwDc8V5tu481NHz6g5ARKTDGg3vHJ2dC/29qJ8nRXcAIiIdFAzvFGbncE4N70zPFCrHnDuWC/3dqJ8nRXcAIiId1Gh4Z+vOQxRm5zDAq47JZTNsWr+yo21TABAR6aCoYZzgTiAIDg6VIJDvUhZQIkNAZvYNM3vRzJ6MeP39ZvaKme0rf92RxHlFRPpd1DBOxmzRnUHQ+T+++fKOd/6Q3BzA3wFXNjjmf7r76vLX3QmdV0Skr21av5JcNrPgZ7lshnn30OM7PfFbLZEA4O4/AV5K4r1ERAbF9EyBdVO7WLH5EdZN7VowsRuYXJPnnusuIj+Wwyhd4QfPw3R64rdaN+cA3mtm+4GjwL9194NdPLeISKLCFm99cvs+7nr4IFuuXrVgCGdyTT50SKf696E7E7/VzCNuQ5p+I7PlwD+4+ztDXvsXwEl3/72ZXQV80d0viHifjcBGgGXLlq19/vnYm9uIiHTNuqldFCKGa+JO5nai/IOZ7XX3iVjHdiMAhBz7HDDh7r+td9zExIRrS0gR6bWwjvpT2/cRp/fMZoyt11/clUldaC4AdGUhmJm9zcys/PiS8nl/141zi4i0I2oh11m5bKzfL847dz3cnyPeicwBmNk3gfcDZ5vZEWALkAVw968A1wP/2sxOAHPATZ7UrYeISAdFLeQ6IztCLptZ9FqYl48XO9W8tiQSANz95gavfwn4UhLnEhHppqi0zNnjRT5/42ru3HGQ2bnmOvhelH4Oo1pAIiJ1RKVljpRGtdm35Qq+cOPqyLROgLGq4aI4tYG6RQFARFKtUS7/pvUryWZs0e/Nu1c67sk1eR7ffDlfuHE12ZGFx2ZHjDuvWVV53qvSz2FUC0hEUissl3/Tt/dz18MHmT1e5NyxHJddOE5Uuk/QcQfDN8H3fiz9HEYBQERSK+xqvHjSK5O2hdk57tt9uO571HbcUYu+AueO5ULXD3RzBXBAQ0AiklpJXHU323FH1Qbq5grggAKAiKRWu1fdrXTcUbWBepEFpCEgEUmdIA0zbCOWuJaOZhfV/Imr0TBRtygAiEiq1E78Vm/EMpbL8urrJyjONw4JM3dc0dF2doMCgIgMrbAFV2ETv07pin7mjiuYnilw18MH667erZfzP0gUAERkKIWleNaWX6728vFiJad/ck2e6ZlC6CrfXk3YdoICgIgMvLhX+o3q9tTm9AeBoB/KNnSCAoCIDLRmr/TrCUsL7ZcJ205QGqiIDLSoK/2MLS7f0EgvFmP1kgKAiAy0qMVcUZuuR8lmbGjG9uPSEJCIDJyoCdpWtZPTP8gUAERkoEzPFNj07f0UT7a/p1R+LMfjmy9PoFWDSQFARAbG9EyBTz+wv+nhnTDDlM7ZKs0BiMhACLJ9kuj8DXpWf6ef6A5ARHqqui5Pxox5d/Ih+fZh2T7tSHvnD8ltCv8N4E+AF939nSGvG/BF4CrgOPBxd/9ZEucWkcFVm8MfXN0Hufx7nn+Jx54+xtHy9olJSVu6Z5SkhoD+DriyzusfAi4of20E/ktC5xWRAVbvqn6uOM99uw9X9s6NI06NHo39n5JIAHD3nwAv1TnkWuDvvWQ3MGZm5yRxbhEZXElug7h0NFvZl7d2w5VgSVgva+/3o27NAeSBX1c9P1L+2Qu1B5rZRkp3CSxbtqwrjROR3ojaHrFZ2Yyx5erSxutx9uWVkm4FgLA12aF3de6+DdgGMDExkeSwn4j0mU3rVzZdt2csl+XOa1Yt6OAvu3CcrTsP8ant+yodfprz++PqVgA4Apxf9fw84GiXzi0ifar6aj3uncCrr58AqHTwUcXgqt9fwnVrHcAO4M+t5FLgFXdfNPwjIsNveqbAuqldrNj8COumdgGlzvy5qQ/zhRtXN/z94ryzdeehyvOoYnDVx0i4RAKAmX0T+F/ASjM7YmafMLNbzezW8iGPAs8CzwBfA/4qifOKyGAJrtaDzJ7gan16pgDEv2KvnjyOmkhOcoJ5WCUyBOTuNzd43YG/TuJcIjK4oq7WP7l9H59+YD83v+f8ymKweqrz+KMmkpXr35hKQYhI19S7Kp93577dh3n7+Gjd96jN49+0fuWitE/l+sejACAiLakdyw+GceqJc1X+7LHj/NmlyyobupjBaHYEIzyPf3JNnnuuu4j8WC7yGAmnWkAi0rRWMm+mZwocL2fw1DPvzmcnL+KzkxfFbs8wb9vYSboDEJGmNZt5EwSMl4833sClla0cpTUKACLStKic/cLsXOhQUDOVPG9+z/mND5JEaAhIRJpWL1Pnk9v38akH9uFeqs/jTqytGzNm3Pye85sa+pH2KACISNMapWkGL8cZ8gFtzdgrCgAiskCwQUu9Qmr5hIq4gVI2e0kBQEQqorJ7qjdmCYqvPbS30PYOXWE7f0n3KACISEVUds99uw9Xnhdm53hob4ENa/Pc/9PDtLpFr4Z9ek8BQCTFaod74g7rzBXneezpYxFF3eNRrZ7eUxqoSEqFFWZrJgM/CBqtUq2e3lMAEBli9co1hA33OOG7N4UJJohboYnf/qAAIDKkGpVejhqCiTuqc9mF40yuyTOWyzbVrjNPy6hWT59QABAZUo3KNbQ7BPPY08cAuPOaVYuqcYbJmPFnly7j4N1XqvPvE5oEFhlSjTZK2bR+JZse3E9xvrWZ3OB9ard1NBbeReSyuuLvVwoAIkNqbDQbuhJ3wZV/G1k8547lFmURBVs6NlpIJv1BAUBkwMRZqTs9U+D3/xxeevn46ycq71E82VoEyGUzXHbhOJu+vb/yHoXZOTZ9ez9bb7hY+f0DwrzVVRzVb2J2JfBFIAN83d2nal7/OLAVCFIQvuTuX2/0vhMTE75nz5622ycyLGpX6sKpIRY4deU90mBbxVw20/Qq3mBoJygEVzvUExjLZdm35Yqm3luSY2Z73X0izrFt3wGYWQb4MvDHwBHgCTPb4e6/qDl0u7vf1u75RNIsamL3zh0Hee3EycprjYq1zRXnY+29W23JCGBWmTOI+s04lT+lPySRBXQJ8Iy7P+vurwPfAq5N4H1FpEbUxO7sXLHpK/rgKj6u4klanjCW/pREAMgDv656fqT8s1obzOznZvagmWnHB5EmBAu6kux+l45med+/elOC73jqfWUwJBEAwi4iav9OHwaWu/u7gB8C90a+mdlGM9tjZnuOHTuWQPNEBlv1gq4oIy3sovjy8SI/OzzbRssWy2aMLVevSvQ9pXOSCABHgOor+vOAo9UHuPvv3P218tOvAWuj3szdt7n7hLtPjI+PJ9A8kcEWZzvFFpN5mCuebO0Xy7IjxtLRLEapuufW6y9WyucASSIN9AngAjNbQSnL5ybgT6sPMLNz3P2F8tNrgKcSOK9IKvRT1czsiPGGM5Ywe7yoHP8h0HYAcPcTZnYbsJNSGug33P2gmd0N7HH3HcC/MbNrgBPAS8DH2z2vSFo0U6a5E4J0T23eMnwSWQfQKVoHIBKe+5+kpaNZRk9bsmC3r+rdv9TpD5aurgMQkc6aXJNnz/MvLdiVKym5bIYtV69SB59SCgAifSSqzENQeTNpKtKWbhoCEukTYUM9waRrWFG3dmlP3uHUzBCQ9gMQ6RNh6Z7Fk55I51+7TEA7cgloCEikb3Qq3bO2iJuyeSSgACDSI7Xj/WflsokVUgur2DnvXrnyV+cvoCEgkZ6Ynimw6cH9C/br/X+vnSDbSk2HKvmxHM9NfZhf3nMV+bHcopos1VtCiigAiPTAXQ8fXFRZc/6kM9JGAKge15+eKUQuHuunlcXSWxoCEumBqInd1060Vpunelw/yCaK0u5m8DI8FABEuigY90/Sc1MfXvC8XvE4Zf9INQ0BiXRJnLLOrQwArZvaxfRMofK83hCPFn5JNQUAkS6JU9bZWRwEGgWFwuwct3/nQCUIRA3x5Mdy6vxlAQUAkQQFO3et2PxIU1fm1aqDQH4sx+dvXE2+wbh9dXbPpvUryWUzC17X0I+E0RyASBuqc/nPymV59fUTleye4MocSgXdminrHJRfri7V0KgiaBBggqv8sJpCItVUC0ikRXHLNAcdeStlnQ0qHTjAJ7fva3geSTfVAhLpgjhj+rDwynzD2nxTE73BIrHgTiJqKMhAQzzSNAUAkRbFHdOvnpR97Olji1bnxhGM8YeN7xvwsUuXaYhHmqYAINKiuAuqXn3tRGUyuF7QyFj9e4Ojs3NMrslzz3UXkR/LVTZi//yNq/ns5EWx2y0S0CSwSIs2rV8ZWr//tCUjvPr6qZ/NzhUrQzhRE8HV4/frpnaFHhMEnMk1eV3tSyISuQMwsyvN7JCZPWNmm0NeP93Mtpdf/6mZLU/ivCLdVp3muXXnITaszS+4Gt96w8WMjZ626PfmivPcueNgrBRNpXFKt7QdAMwsA3wZ+BDwDuBmM3tHzWGfAF529z8APg98rt3zinRb9UreYHL2/t2HuezCcX419WEe33w5k2vykameQannDWvzleGejBkb1i68og8mi+sdI5KEJO4ALgGecfdn3f114FvAtTXHXAvcW378IPABswYDniJ9Jizrx4H7dx9esOCr3lj+px7Yx327DzNfTr+ed+ehvYUFvz89U+ChvYW6x4gkIYkAkAd+XfX8SPlnoce4+wngFeDNYW9mZhvNbI+Z7Tl2rDMbYYvEVT3kE3Vl78CnH9hf6aDn66ytCXuptkZ/WKBRHX/phCQCQNjlTu2feZxjSj903+buE+4+MT4+3nbjRFpVO+RTz7w7mx7cz+q7vt/Suaqzg6IyhVTHX5KWRAA4Apxf9fw84GjUMWa2BDgLeCmBc4t0TNyFXoHivLe8pWN1SmlUeqnq+EvSkggATwAXmNkKMzsNuAnYUXPMDuCW8uPrgV3ezzUoROjeFbeygKRX2l4H4O4nzOw2YCeQAb7h7gfN7G5gj7vvAP4r8N/M7BlKV/43tXtekVbVbsYeVSitmeJtjWQzBg7Fkwuve5aOZtly9apFWUCgYm7SeSoGJ6kSVpAtl82wYW2ex54+tqDDhcYVOOPImPE3H70YUKcunddMMTgFAEmVqFW2xsKshFw2wz3Xlcor1KvAGYcBv6rZtlGkU1QNVCRC1Lh+7WVQkHY5uSbfcDOWRjR5K/1KAUBSY3qmwEgT6w+DYHHZheMt7dULmryV/qZicJIKwdh/vUVaYZZvfmTR8FBcGTNtwi59TXcAkgrN5vTDqU4/qvMP7gqWjmbJjiy8R8hlM/zNRy9W5y99TQFAUqETOf3Bvr0zd1zB1hsuXlAVVFf+Mgg0BCSpkGROf7Xq7R7V4cugUQCQoVK7yOuyC8d5aO8R5oon6/7eaHaE4w2OCaMMHxlkCgAy8KZnCtz18EFePr6wDk9hdo77dh+O9R7/4bp3Nb3oSxk+MugUAGSgTc8U2PTgforz7S1orC2/MDaaxR1emSsuWBmslbwyTBQAZGCE1fDZuvNQ253/0tEsEG8cXx2+DBOVgpCBEFXDp906PQBjueyCK3118jLImikFoTsAGQhRu2QlIajhX5id4/bvHAB0pS/poHUAMhBazeO/4C1nNnW8tl6UNFEAkIHQarrls8eOA9BECSBtvSipoQAgAyFsl6w4gto/7vH/2JXbL2mhACADYXJNng1r2xuXj7PMS7n9kiaaBJa+FL6it9Cx8xkoC0hSp60AYGZvArYDy4HngI+6+8shx80DB8pPD7v7Ne2cV4ZbbcpnYXaO+3cfbqkkcxz5sRyPb768Q+8u0r/aHQLaDPzI3S8AflR+HmbO3VeXv9T5S113PXxwUYpns51/doRFcwbZESttzl5FQz6SZu0GgGuBe8uP7wUm23w/SbnpmcKimj6teMMZWe657qIFJZq33nAxW69X2WaRQLtzAG919xcA3P0FM3tLxHFnmNke4AQw5e7TbZ5XhlRSOfizx4uRpR3U4YuUNAwAZvZD4G0hL32mifMsc/ejZvZ2YJeZHXD3X0acbyOwEWDZsmVNnEKGQVI5+ErlFGmsYQBw9w9GvWZmvzGzc8pX/+cAL0a8x9Hy92fN7MfAGiA0ALj7NmAblGoBNfwXyFCJ2rgllx0BLHb5B43rizTW7hzADuCW8uNbgO/VHmBmS83s9PLjs4F1wC/aPK/0memZAuumdrFi8yOsm9rF9ExrKZthC75y2Qz3XPeuRWP6Y7ls6HssHc1qmEckhnbnAKaAB8zsE8Bh4AYAM5sAbnX3vwT+EPiqmZ2kFHCm3F0BYIiEpW02W1StOu9/bDTL6UtGQit0Vr9fVIXQLVevSuqfJjLUVA5a2rbm7u+HZu5kzDjp3nCBVVRHHidDJ2yPAF39S5qpHLR0Tb20zaAOT9QdQdB5h435B1U542zQog5fpDWqBSRtiZu2OVec566HD1aeB1f9YZ1/QFU5RTpLAUBaNj1TqNuB13r5eLEyORy2wUstpXKKdJaGgGSBuGPqwRV8s4JhnUZX9yrRINJ5CgBS0Uw2z507FtfriSPo+KPy/aGU4qnJXJHOUwCQiqh9d2snY6dnCpV9dJs1YsaKzY8wNpolO2IUT57KQoub+SMiydAcgFREDcvU/rx6MrdZ8+44pfkADMZyWRVmE+kR3QFIRdSwTO1kbBLVOgGK886Zpy9h35YrEnk/EWmO7gCkYtP6lYvq5Wcz1vZkbL29fJXqKdI7CgCyUO3C8JCF4lE1eALZEWPpaOmYjJUKuGXMQo9VqqdI7ygASMXWnYcWTMoCFE/6osVed16zipHw/ryy8cqWq1eRy2Yqq4HnQ0qOKNVTpLc0ByAVUcMxhdk51k3tWrBBe1DnJ5AdMbbecHFlEnfd1K7QNNG49YFEpPMUAKSy+CuqLKBBZXI4aoP24E4h6NCjgslJd3419eFkGi4ibdEQUMo1qsljxJoWABZ2+lFj+xrzF+kfCgApV68mT34sF9nZh6nu3KM2dtGYv0j/UABIuaihGgMe33w5+Ygr9to54NrOfXJNftEOXlroJdJfNAeQco0Wf21avzJ0s5YNa/M89vSxukXjVKtfpL9pR7CUC9uNKxj3D4qyAdp1S2RAaEcwqat6J66MGfPule/Vk75BNdB7rruIxzdf3ssmi0gHtDUHYGY3mNlBMztZ3gg+6rgrzeyQmT1jZpvbOadEm54psG5qFys2P8K6qV2VzVdqj6nO+qleqBWW8RNUAxWR4dPuJPCTwHXAT6IOMLMM8GXgQ8A7gJvN7B1tnldqVHfszqmr9+ogMD1T4NMP7I/M+omT3ikiw6OtAODuT7l7o8vDS4Bn3P1Zd38d+BZwbTvnlcXq1fKHUwEirCRDI8rdFxlO3UgDzQO/rnp+pPyzUGa20cz2mNmeY8eOdbxxw6JRLf84e/BC4/ROERkeDQOAmf3QzJ4M+Yp7FR9WNizyMtTdt7n7hLtPjI+PxzyFRF2lj5gxPVOINYyTy2b42KXLlLsvkhINs4Dc/YNtnuMIcH7V8/OAo22+p9QIy9eH0uTu7d85wNhotu5GLtqHVyR9upEG+gRwgZmtAArATcCfduG8qRJ03J9+YP+icf654jynLxkhl80syvf/2KXL+OzkRd1sqoj0iXbTQD9iZkeA9wKPmNnO8s/PNbNHAdz9BHAbsBN4CnjA3VvfVHbIxUnljDK5Jr+gRHO1V+aKbFibXzAe58BDewuxz9FO20Sk/2glcB8JW5Wby2aaGodfN7UrtLRDUNMn6rVGC72SaNsgChbNaRW0DIpmVgKrGFwfaZTKGUe9KpyNMoU63bZBE2dthcggUwDoI8100FHDMfWqcLZTo7+d4DGo0hj0JF1UC6iPnJXLMju3OFMnSOUMhh5qh2OCK1M4VYEzbJgiqrJnnDz/RlVDh1Eag56ki+4A+sT0TIFXXz8R+lqQyhlc5bd6ZdpOjf40bvCiXc1k2OkOoA8ENXrqlWkIOvjJNfnI7RvjXJk2qtEfNekZ/E6aJkTbuWMSGQQKAD3WTI2eo7NzTM8UQqt2wsIr01ayV1odWhpWaQx6ki4KAB0StwOOW6MHSh381p2HQjt/g8qVaaOOPEq9oaW0dnppC3qSLpoD6IBm0gfjTig2SuV0Fl6xtjJHoElPkXTRHUAHNOqAq+8MRk/L8Orri+8ADBgbzTJ7vLjgDiLYyatWfiy3YKevMI068jRm+oikmQJAB0R1tMGdQPXQTJSzclm2XL1q0fBD1MTkZReOhxaDq9aoI9ekp0i6KAC0KGxf3aCiZtSVNBB7vH92rhg6bh81MdloLiFOR65JT5F0US2gFoTVxQnkshk2rM3z0N5C7M6+njh1egBWbH4kcpMFlXoWSQ/VAuqwelfbc8V5Hnv6WGXBVbviTsBGDe8EAUSdv4jUUgBoQaNOuTA7x9adh9i0fmXodmjNiDsBm8aVuiLSHs0B0PyiqaiaPdWCCd9Gx45mR1h65ukUZucWLfBqpgPX+L2INCv1AaCVRVMW87J+rjjPGdnFO3EtPOYkvyiP8bdbe16LlkSkGakPAK2sfp2ts7du2LGfv3F1ZK2f6iEedeAi0k2pDwD1Vr9GXZHXS/Osde5YrtKpK8deRPpJu3sC32BmB83spJlFph2Z2XNmdsDM9plZX+V1Rk2yjo1mI8s5hE24ZkeMbGbh2FB1B99OKWYRkU5o9w7gSeA64Ksxjr3M3X/b5vkSF7X61X3xoq1gaCjIyw/uDsZGs7iXFm/VLgqrXcSlDl9E+kVbdwDu/pS7D/T+eFFX5q9EZO4EQ0aTa/I8vvlyPn/jav65eLKS6TPvXrnyV2cvIv2sW3MADnzfzBz4qrtv69J5Ywm7Mo8qqla7PaNKKIvIoGp4B2BmPzSzJ0O+rm3iPOvc/d3Ah4C/NrM/qnO+jWa2x8z2HDt2rIlTJCtsnB8Wb8+oEsoiMqgaBgB3/6C7vzPk63txT+LuR8vfXwS+C1xS59ht7j7h7hPj4+NxT5G4YGgoE5L0X13aWfvGisig6ngpCDM708zeGDwGrqA0edz3JtfkORlRLC+4wlcJBhEZVG3NAZjZR4D/BIwDj5jZPndfb2bnAl9396uAtwLftdKV9BLgv7v7/2iz3R0RlvffaJMUlWAQkUGlctBlYSWeo0o757IZ5fCLSF9SOegWRGXzVJd21gIuERkmqS8FEaiXzaMFXCIyjHQHUKZsHhFJm6EMANMzBdZN7WLF5kdYN7WrkrNfj7J5RCRthm4SOGwyNztivOGMJcweL9bN0mm3Hr+ISK81Mwk8dHMAYZO5xZPOy+Ua/vU2fNFYv4ikydANAcUpwVC9kldEJK2G7g4g7mYtnarVo2EkERkUQ3cHEFXErVYnsnv+/fQBPrV9X+gmMiIi/Wbo7gBqSzOclcvy6usnKM6fmuxOKrun+mr/rFy2sidANZWGFpF+NXQBABZP5nZiWKY22yis8w8UyvsLKwiISD8ZygBQqxPZPWHZRvVEZR6JiPTK0M0BxNHKQrFazU4izxXn+eT2fS2fT0Qkaam4A4BTw0CF2TmM0h6VUH9dQD1xs41qtXo+EZGkpeIOIBivDzrs2rXPrawLCMs2yo4YS0ezDX9X6xBEpB+kIgDcueNgw/H6Zod0gi0jq8tEb73hYmbuuIIv3Li6YSqq9gwWkV4b+iGg6ZlC3QydQCvrAqImlyfX5Nnz/Evcv/vworuNds4nIpKkob8DiDPU0omqn489fSyy81eVURHpB20FADPbamZPm9nPzey7ZjYWcdyVZnbIzJ4xs83tnLNZjYZaOrXDV73zakcxEekH7Q4B/QC43d1PmNnngNuBf1d9gJllgC8DfwwcAZ4wsx3u/os2zx1LVLbO0tEsM3dc0fXz5sdy6vxFpC+0dQfg7t939xPlp7uB80IOuwR4xt2fdffXgW8B17Zz3mZEbfSy5epVPTmvhn5EpF8kOQn8F8D2kJ/ngV9XPT8CvCfB89ZVWxuoWxU6e3VeEZG4GgYAM/sh8LaQlz7j7t8rH/MZ4ARwf9hbhPwschsyM9sIbARYtmxZo+bF0quNXrTBjIj0s4YBwN0/WO91M7sF+BPgAx6+v+QR4Pyq5+cBR+ucbxuwDUpbQjZqn4iItKbdLKArKU36XuPuxyMOewK4wMxWmNlpwE3AjnbOKyIi7Wt3HcCXgDcCPzCzfWb2FQAzO9fMHgUoTxLfBuwEngIecPeDbZ5XRETa1NYksLv/QcTPjwJXVT1/FHi0nXOJiEiyhn4lsIiIhFMAEBFJKQtP3OkPZnYMeL7mx2cDv+1Bc1oxKG0dlHbC4LR1UNoJamsn9LKd/9Ldx+Mc2NcBIIyZ7XH3iV63I45BaeugtBMGp62D0k5QWzthUNqpISARkZRSABARSalBDADbet2AJgxKWwelnTA4bR2UdoLa2gkD0c6BmwMQEZFkDOIdgIiIJKDvA8Ag7DpWPv8NZnbQzE6aWeTsv5k9Z2YHyqUz9nSzjVVtiNvWnn6m5Ta8ycx+YGb/VP6+NOK4+fJnus/MulZrqtFnZGanm9n28us/NbPl3WpbSFsMVRQIAAADk0lEQVQatfXjZnas6nP8yx618xtm9qKZPRnxupnZ35b/HT83s3d3u43ldjRq5/vN7JWqz/OObrexIXfv6y/gCmBJ+fHngM+FHJMBfgm8HTgN2A+8o8vt/ENgJfBjYKLOcc8BZ/f4M23Y1n74TMvt+I/A5vLjzWH//+XXft+DtjX8jIC/Ar5SfnwTsL1H/+dx2vpx4Eu9aF9NO/4IeDfwZMTrVwH/SKnU/KXAT/u0ne8H/qHXn2e9r76/A/AB2HUMwN2fcvfGO9D3gZht7flnWnYtcG/58b3AZA/aECXOZ1Td/geBD5hZ2B4ZndYv/58NuftPgJfqHHIt8PdeshsYM7NzutO6U2K0s+/1fQCo8ReUIn+tsF3H+nUnFge+b2Z7y5vf9Kt++Uzf6u4vAJS/vyXiuDPMbI+Z7TazbgWJOJ9R5ZjyhcwrwJu70rqIdpRF/X9uKA+rPGhm54e83g/65W8zjvea2X4z+0cz6+w+tC1IckvIlnV717FWxWlnDOvc/aiZvYVSGe2ny1cSiUqgrV35TKF+W5t4m2Xlz/XtwC4zO+Duv0ymhZHifEZd+xwbiNOOh4FvuvtrZnYrpTuXyzvesub1y2fayM8olWX4vZldBUwDF/S4TQv0RQDwLu861qpG7Yz5HkfL3180s+9SujVPPAAk0NaufKZQv61m9hszO8fdXyjf5r8Y8R7B5/qsmf0YWENpzLuT4nxGwTFHzGwJcBa9GTZo2FZ3/13V069RmnPrR13722yHu//fqsePmtl/NrOz3b1vahn1/RDQMO06ZmZnmtkbg8eUJrhDMwj6QL98pjuAW8qPbwEW3b2Y2VIzO738+GxgHfCLLrQtzmdU3f7rgV0RFzGd1rCtNePo11DawKkf7QD+vJwNdCnwSjBM2E/M7G3BfI+ZXUKpv/1d/d/qsl7PQjf6Ap6hNN63r/wVZFScCzxaddxVwP+hdNX3mR608yOUrkxeA34D7KxtJ6UMjP3lr4O9aGfctvbDZ1puw5uBHwH/VP7+pvLPJ4Cvlx+/DzhQ/lwPAJ/oYvsWfUbA3ZQuWADOAL5d/jv+38Dbe/E5xmzrPeW/y/3AY8CFPWrnN4EXgGL57/QTwK3AreXXDfhy+d9xgDpZdz1u521Vn+du4H29+r+P+tJKYBGRlOr7ISAREekMBQARkZRSABARSSkFABGRlFIAEBFJKQUAEZGUUgAQEUkpBQARkZT6/yMMciLdP5xFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(sklearn_pred, keras_pred)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"../data/test2_pdf.pdf\", bbox_inches='tight') # write pdf to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../data/test_pdf.pdf to Amazon S3 bucket mlsquare-datasets\n",
      "..."
     ]
    }
   ],
   "source": [
    "### Test upload to AWS S3 ###\n",
    "import boto\n",
    "import sys\n",
    "from boto.s3.key import Key\n",
    "# from boto.s3.key import Key\n",
    "bucket_name = 'mlsquare-datasets'\n",
    "AWS_ACCESS_KEY_ID = 'AKIAJXRNK62PGFLPIJTA'\n",
    "AWS_SECRET_ACCESS_KEY = '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'\n",
    "REGION_HOST = 's3.ap-south-1.amazonaws.com'\n",
    "\n",
    "# bucket_name = AWS_ACCESS_KEY_ID.lower() + '-dump'\n",
    "conn = boto.connect_s3(AWS_ACCESS_KEY_ID,\n",
    "        AWS_SECRET_ACCESS_KEY, host=REGION_HOST)\n",
    "bucket = conn.get_bucket('mlsquare-pdf', validate=False)\n",
    "\n",
    "# bucket = conn.create_bucket(bucket_name,\n",
    "#     location=boto.s3.connection.Location.DEFAULT)\n",
    "\n",
    "testfile = \"../data/test_pdf.pdf\"\n",
    "print ('Uploading %s to Amazon S3 bucket %s' % (testfile, bucket_name))\n",
    "\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "k = Key(bucket)\n",
    "k.key = 'my test file'\n",
    "k.set_contents_from_filename(testfile,\n",
    "    cb=percent_cb, num_cb=10) # upload file\n",
    "url = k.generate_url(expires_in=0, query_auth=False) # get url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/uci_abalone_logistic.pdf'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'uci_abalone'\n",
    "algo = 'logistic'\n",
    "'../data/' + ('_').join([name,algo]) + '.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets #\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "df = pd.concat([X, y], axis=1)\n",
    "# testData3 = np.concatenate((X,y), axis=1)\n",
    "# testData3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1001)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/testData4.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y type --  <class 'theano.tensor.var.TensorVariable'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - 0s 449us/step - loss: -6.6398\n",
      "60/60 [==============================] - 0s 50us/step\n",
      "-5.562779839833578\n"
     ]
    }
   ],
   "source": [
    "### Testing Deep LDA implementation with Theano loss ###\n",
    "\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 170\n",
    "# }\n",
    "\n",
    "# automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "\n",
    "## Create model ##\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model(input_dim, reg_par):\n",
    "    \"\"\"\n",
    "    Builds the model\n",
    "    The structure of the model can get easily substituted with a more efficient and powerful network like CNN\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1, input_shape=(input_dim,), activation='sigmoid', kernel_regularizer=l2(reg_par)))\n",
    "#     model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_par)))\n",
    "#     model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_par)))\n",
    "#     model.add(Dense(2, activation='linear', kernel_regularizer=l2(reg_par))) \n",
    "#     outdim_size is passed via arguments\n",
    "\n",
    "    return model\n",
    "\n",
    "## Define loss function ##\n",
    "\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from theano.compile.ops import as_op\n",
    "\n",
    "\n",
    "@as_op(itypes=[theano.tensor.ivector],  # Why? What is the need for such an op?\n",
    "       otypes=[theano.tensor.ivector])\n",
    "def numpy_unique(a):\n",
    "    return np.unique(a)\n",
    "\n",
    "\n",
    "def lda_loss(n_components, margin):\n",
    "    \"\"\"\n",
    "    The main loss function (inner_lda_objective) is wrapped in this function due to\n",
    "    the constraints imposed by Keras on objective functions\n",
    "    \"\"\"\n",
    "    def inner_lda_objective(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        It is the loss function of LDA as introduced in the original paper. \n",
    "        It is adopted from the the original implementation in the following link:\n",
    "        https://github.com/CPJKU/deep_lda\n",
    "        Note: it is implemented by Theano tensor operations, and does not work on Tensorflow backend\n",
    "        \"\"\"\n",
    "        r = 1e-4\n",
    "\n",
    "        # init groups\n",
    "        print('y type -- ',type(y_true))\n",
    "        yt = T.cast(y_true.flatten(), \"int32\")\n",
    "        groups = numpy_unique(yt)\n",
    "\n",
    "        def compute_cov(group, Xt, yt):\n",
    "            Xgt = Xt[T.eq(yt, group).nonzero()[0], :]\n",
    "            Xgt_bar = Xgt - T.mean(Xgt, axis=0)\n",
    "            m = T.cast(Xgt_bar.shape[0], 'float32')\n",
    "            return (1.0 / (m - 1)) * T.dot(Xgt_bar.T, Xgt_bar)\n",
    "\n",
    "        # scan over groups\n",
    "        covs_t, updates = theano.scan(fn=compute_cov, outputs_info=None,\n",
    "                                      sequences=[groups], non_sequences=[y_pred, yt])\n",
    "\n",
    "        # compute average covariance matrix (within scatter)\n",
    "        Sw_t = T.mean(covs_t, axis=0)\n",
    "\n",
    "        # compute total scatter\n",
    "        Xt_bar = y_pred - T.mean(y_pred, axis=0)\n",
    "        m = T.cast(Xt_bar.shape[0], 'float32')\n",
    "        St_t = (1.0 / (m - 1)) * T.dot(Xt_bar.T, Xt_bar)\n",
    "\n",
    "        # compute between scatter\n",
    "        Sb_t = St_t - Sw_t\n",
    "\n",
    "        # cope for numerical instability (regularize)\n",
    "        Sw_t += T.identity_like(Sw_t) * r\n",
    "\n",
    "        # return T.cast(T.neq(yt[0], -1), 'float32')*T.nlinalg.trace(T.dot(T.nlinalg.matrix_inverse(St_t), Sb_t))\n",
    "\n",
    "        # compute eigenvalues\n",
    "        evals_t = T.slinalg.eigvalsh(Sb_t, Sw_t)\n",
    "\n",
    "        # get eigenvalues\n",
    "        top_k_evals = evals_t[-n_components:]\n",
    "\n",
    "        # maximize variance between classes\n",
    "        # (k smallest eigenvalues below threshold)\n",
    "        thresh = T.min(top_k_evals) + margin\n",
    "        top_k_evals = top_k_evals[(top_k_evals <= thresh).nonzero()]\n",
    "        costs = T.mean(top_k_evals)\n",
    "\n",
    "        return -costs\n",
    "\n",
    "    return inner_lda_objective\n",
    "\n",
    "## Fit the model ##\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = create_model(x_train.shape[-1], reg_par=1e-5)\n",
    "\n",
    "model_optimizer = Adam()\n",
    "model.compile(loss=lda_loss(n_components=1, margin=1), optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lda_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c4fa72ce1ce2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlda_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_loss' is not defined"
     ]
    }
   ],
   "source": [
    "### Testing Keras with Theano backend ###\n",
    "\n",
    "import automation_script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_name = \"uci_iris\"\n",
    "dataset_info = automation_script.get_dataset_info(dataset_name)\n",
    "\n",
    "url = \"../data/iris.csv\" if path.exists(\"../data/iris.csv\") else dataset_info['url']\n",
    "data = pd.read_csv(url , delimiter=\",\", header=None, index_col=False)\n",
    "class_name,index = np.unique(data.iloc[:,-1],return_inverse=True)\n",
    "data.iloc[:,-1] = index\n",
    "data = data.loc[data[4] != 2]\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "# params = {\n",
    "#     'epochs': 170\n",
    "# }\n",
    "\n",
    "# automation_script.run_imly(dataset_info, 'logistic_regression', X, Y, 0.60, params=params)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.60, random_state=0)\n",
    "\n",
    "\n",
    "## Create model ##\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Builds the model\n",
    "    The structure of the model can get easily substituted with a more efficient and powerful network like CNN\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(1, input_shape=(4,), activation='sigmoid', kernel_regularizer=l2(1e-5))) \n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.compile(loss=lda_loss(n_components=1, margin=1), optimizer='adam')\n",
    "model.fit(x_train, y_train)\n",
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3719358722368876"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
